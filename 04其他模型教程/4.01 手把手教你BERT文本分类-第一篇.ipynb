{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ‰‹æŠŠæ‰‹æ•™ä½ BERTä¸­æ–‡æ–‡æœ¬åˆ†ç±»-ç¬¬ä¸€ç¯‡\n",
    "\n",
    "ä½œè€…ï¼šä½ ä»¬å¤§å«\n",
    "\n",
    "æ—¶é—´ï¼š2020å¹´4æœˆ\n",
    "\n",
    "githubï¼šhttps://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "å¼€æºåè®®ï¼š[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "## å†™åœ¨å¼€å¤´\n",
    "\n",
    "### æ„Ÿè¨€\n",
    "\n",
    "BERT æ¨¡å‹è‡ª2018å¹´å‘å¸ƒä»¥æ¥ï¼Œå®ƒå’Œå®ƒçš„è¡ç”Ÿå“å‡ ä¹åœ¨NLPåœˆä¸€ç»Ÿå¤©ä¸‹ã€‚å…³äºBERTæ¨¡å‹çš„åŸç†æˆ‘å°±ä¸ç³»ç»Ÿä»‹ç»äº†ï¼Œå„ä½è‡ªè¡Œå»è¯»è®ºæ–‡å’Œçœ‹åšå®¢åˆ†æã€‚\n",
    "\n",
    "è¿™ç¯‡Notebookçš„èµ·å› ï¼Œæ˜¯æˆ‘åœ¨ä½¿ç”¨ PyTorch å’Œ Keras å­¦ä¹ å’Œå®éªŒ BERT ç³»åˆ—æ¨¡å‹ï¼ˆåŒ…æ‹¬RoBERTaã€ALBERTç­‰ï¼‰çš„æ—¶å€™ï¼Œçœ‹ç€ CSDNã€Githubã€çŸ¥ä¹ ç­‰è®ºå›ä¸Šäº”èŠ±å…«é—¨çš„å°è£…ä»£ç å¤´ç–¼â€”â€”å°¤å…¶æ˜¯ä¸­æ–‡ä»»åŠ¡çš„ä»£ç ï¼Œå¤§å¤šæ•°ä»£ç ä½œè€…ä¼°è®¡éƒ½æ˜¯çŸ¥å…¶ç„¶è€Œä¸çŸ¥å…¶æ‰€ä»¥ç„¶ï¼Œç”¨å¾—ä¸Šç”¨ä¸ä¸Šçš„ç»Ÿç»Ÿå°è£…ï¼Œåˆè‡­åˆé•¿ã€ç¼ºå¤±æ³¨é‡Šã€ç»“æ„æ··ä¹±ï¼Œå¯¹åˆå­¦è€…æå…¶ä¸å‹å¥½ï¼Œçœ‹å¾—æƒ³åã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œè¿™äº›ä»£ç è¿˜æœ‰ä¸€äº›ç‰ˆæœ¬é—®é¢˜ï¼Œå°±æ˜¯åŸºäº PyTorch çš„ BERT æ¡†æ¶å·²ç»è¿›åŒ–ä¸ºäº† [transformers](https://github.com/huggingface/transformers)ï¼Œè€ŒCSDNã€çŸ¥ä¹ã€Githubä¸Šå¾ˆå¤šé¡¹ç›®è¿˜æ˜¯åŸºäº**æ—§ç‰ˆ** `pytorch-pretrained-bert` æ¡†æ¶ï¼Œå‚è€ƒä»·å€¼æœ‰é™ã€‚\n",
    "\n",
    "æ–°æ¡†æ¶æ”¯æŒ ALBERTã€RoBERTa ç­‰æ–°æ¨¡å‹çš„è°ƒç”¨ï¼Œæ¨¡å‹åŠŸèƒ½ä¸Šä¹Ÿæœ‰æ‰€æ›´æ–°ï¼Œæ›´ä½•å†µæˆ‘æœ¬äººåœ¨æŠ€æœ¯ä¸Šçˆ±ç”¨æ–°ä¸çˆ±ç”¨æ—§ï¼Œæ‰€ä»¥å¯¹å¾ˆå¤šåŸºäºæ—§ç‰ˆæœ¬æ¡†æ¶çš„ä¸­æ–‡ä»»åŠ¡å‚è€ƒä»£ç å¾ˆæ˜¯å¤´ç–¼ã€‚\n",
    "\n",
    "åŸºäºè¿™äº›åŸå› ï¼Œæˆ‘åœ¨è¿›è¡Œäº†ä¸€æ®µæ—¶é—´çš„å®éªŒä»¥åï¼Œå†³å®šåŸºäº PyTorch å’Œ [transformers](https://github.com/huggingface/transformers) æ¡†æ¶å†™ä¸€ç¯‡**æ–°æ‰‹å‹å¥½**çš„ BERT å®æˆ˜æ•™ç¨‹ã€‚\n",
    "\n",
    "> æ³¨æ„ï¼Œè¿™ç¯‡æ•™ç¨‹å¯ä»¥è¯´æ˜¯ Baselineï¼Œé‡Œé¢ç¼ºå°‘äº†å¾ˆå¤šå·¥ç¨‹å®ç”¨å¤„ç†ï¼Œæ¯”å¦‚ï¼š\n",
    ">\n",
    ">- BERT.Configå°è£…ï¼›\n",
    ">- æ–‡æœ¬é¢„å¤„ç†ï¼›\n",
    ">- åŠç²¾åº¦å¤„ç†ï¼›\n",
    ">- æ•°æ®è¯»å†™æ”¹è¿›ï¼›\n",
    ">- æ˜¾å­˜ç›‘æ§ï¼›\n",
    ">- TorchSnooperï¼›\n",
    ">- è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼›\n",
    ">- è‡ªå®šä¹‰ç½‘ç»œç»“æ„ï¼›\n",
    ">\n",
    ">è®¡åˆ’æ˜¯ä»¥ååœ¨å¦ä¸€ç¯‡å®Œå–„ï¼Œæœ¬ç¯‡ä»»åŠ¡ä»…ä»…æ˜¯ç®€æ´æ˜“æ‡‚åœ°è¯´æ˜ BERT å’Œ [transformers](https://github.com/huggingface/transformers) æ¡†æ¶çš„ä½¿ç”¨ã€‚\n",
    "\n",
    "### åŸºç¡€è¦æ±‚\n",
    "\n",
    "- PythonåŸºç¡€ï¼›\n",
    "- ä½ æœ‰ä¸€å®šçš„ PyTorch ä½¿ç”¨ç»éªŒï¼›\n",
    "- ä½ å¯¹ NLP æœ‰ä¸€äº›ç»éªŒï¼›\n",
    "- ä½ å¯¹ BERT å’Œ Tranformer çš„ç†è®ºå’Œç»“æ„æœ‰ä¸€å®šçš„äº†è§£ï¼›\n",
    "- ä½ æƒ³ä½¿ç”¨ BERT ç³»åˆ—æ¨¡å‹æ‰§è¡Œä¸€äº› NLP ä»»åŠ¡ï¼Œæ¯”å¦‚ NERã€æ–‡æœ¬åˆ†ç±»ç­‰ã€‚\n",
    "\n",
    "### Transformersæ¡†æ¶ç®€ä»‹\n",
    "\n",
    "ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "\n",
    "Githubåœ°å€ï¼šhttps://github.com/huggingface/transformers\n",
    "\n",
    "æ–‡æ¡£åœ°å€ï¼šhttps://huggingface.co/transformers/index.html\n",
    "\n",
    "transformers æ¡†æ¶æ¨ªè·¨ TF2.0 å’Œ PyTorch ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å¥½ç”¨çš„é«˜çº§è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚\n",
    "\n",
    "### ä¸»è¦è½¯ä»¶å‡†å¤‡\n",
    "\n",
    "- **transformers**æ¡†æ¶ï¼š\n",
    "\n",
    "    pipå®‰è£…ï¼š\n",
    "\n",
    "    ```shell\n",
    "    pip install transformers\n",
    "    ```\n",
    "    \n",
    "- **PyTorch**ï¼šè§ [PyTorchå®˜ç½‘](https://pytorch.org)\n",
    "\n",
    "- **Keras**ï¼šè§ [Keraså®˜ç½‘](https://keras.io)\n",
    "\n",
    "### Pytorch-BERTä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½\n",
    "\n",
    "åŒ…å«ä¸‰ä¸ªæ–‡ä»¶ï¼š\n",
    "\n",
    "|name | size \n",
    "|:------|:------\n",
    "|config.json | 1KB |\n",
    "|pytorch_model.bin | 392MB |\n",
    "|bocab.txt | 107KB |\n",
    "\n",
    ">å…¶ä¸­ `pytorch_model.bin` å°±æ˜¯ `bert-base-chinese`:\n",
    ">\n",
    ">12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    ">Trained on cased Chinese Simplified and Traditional text.\n",
    ">\n",
    ">å»ºè®®ä¸è¦ç”¨transformersè‡ªå¸¦çš„å‘½ä»¤ä¸‹è½½ï¼Œç”±äºä¼—æ‰€å‘¨çŸ¥çš„åŸå› æ˜¯å¥‡æ…¢æ— æ¯”ï¼Œè€Œä¸”å®¹æ˜“æ–­çº¿ã€‚\n",
    ">\n",
    ">æˆ‘ä¼ åˆ°äº†åº¦ç›˜ä¸Šé¢ï¼Œå¦‚æœè¿˜å«Œæ…¢å¯ä»¥è‡ªå·±æ‰¾åœ°æ–¹ä¸‹è½½ã€‚\n",
    ">\n",
    ">åº¦ç›˜ä¸‹è½½åœ°å€ï¼šhttps://pan.baidu.com/s/1CCylS1nkL4ut8T3nr9cUNA æå–ç ï¼š3ypf\n",
    "\n",
    "### ç¡¬ä»¶å‡†å¤‡\n",
    "\n",
    "- æ”¯æŒCUDAè¿ç®—çš„æ˜¾å¡ï¼ˆè®¡ç®—å¡ï¼‰ï¼Œæœ€å¥½ç»™ç‚¹åŠ›ï¼Œè¦ä¸ç„¶è®­ç»ƒå¾ˆä¹…ã€‚\n",
    "\n",
    "- æ˜¾å­˜å°½é‡å¤§ç‚¹ï¼Œä¸ç„¶ batch size å°äº†è®­ç»ƒå¾ˆæ†‹å±ˆã€‚\n",
    "\n",
    "### æ•°æ®å‡†å¤‡\n",
    "\n",
    "æˆ‘é‡‡ç”¨çš„æ˜¯ THUCNews æ•°æ®é›†çš„**å­é›†**ï¼Œç”±æ¸…åNLPç»„æä¾›ã€‚\n",
    "\n",
    "è¿™ä¸ªæ•°æ®é›†æ˜¯é’ˆå¯¹æ–°é—»æ ‡é¢˜è¿›è¡Œåˆ†ç±»çš„æ•°æ®é›†ã€‚å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/DrDavidS/Pytorch_Basic/tree/master/datasets/THUCNews)ä¸‹è½½ã€‚æ–‡ä»¶ä»¥txtæ ¼å¼ä¿å­˜ï¼Œæ‰“å¼€ä»¥åå¯ä»¥çœ‹çœ‹å†…å®¹ã€‚\n",
    "\n",
    "ç®€å•ä»‹ç»ä¸€ä¸‹æ•°æ®é›†ï¼ŒTHUCNewsæ˜¯æ ¹æ®æ–°æµªæ–°é—»RSSè®¢é˜…é¢‘é“2005~2011å¹´é—´çš„å†å²æ•°æ®ç­›é€‰è¿‡æ»¤ç”Ÿæˆï¼ŒåŒ…å«74ä¸‡ç¯‡æ–°é—»æ–‡æ¡£ï¼Œåˆ’åˆ†å‡º 14 ä¸ªå€™é€‰åˆ†ç±»ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªé‡‡ç”¨äº†å…¶ä¸­ 10 ä¸ªå­ç±»ï¼ŒåŒ…æ‹¬\n",
    "\n",
    "```\n",
    "finance\n",
    "realty\n",
    "stocks\n",
    "education\n",
    "science\n",
    "society\n",
    "politics\n",
    "sports\n",
    "game\n",
    "entertainment\n",
    "```\n",
    "\n",
    "è®­ç»ƒæ•°æ®å…± 180000 æ¡ï¼Œä¿å­˜åœ¨ `train.txt` ä¸­ã€‚æµ‹è¯•æ•°æ®ä¿å­˜åœ¨ `dev.txt` å’Œ `text.txt` ä¸­ï¼Œæ¯ä¸ªæ–‡ä»¶10000æ¡ã€‚\n",
    "\n",
    "æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "```\n",
    "ã€Šéè¯šå‹¿æ‰°ã€‹â€œå†¯å¥³éƒâ€è½¦æ™“å¸¦å¦ˆå¦ˆé—¯ä¸–ç•Œ(å›¾)\t9\n",
    "ç¾å¼—å‰å°¼äºšå¤§å­¦è®¿åå¤ªè®¾è®¡ç­¾å®ä¹ åŸºåœ°åè®®ï¼ˆç»„å›¾ï¼‰\t1\n",
    "åä¸­ç§‘æŠ€å¤§å­¦2010å¹´è€ƒç ”æˆç»©æŸ¥è¯¢å¼€é€š\t3\n",
    "é™ˆå°è‰ºâ€œæ¿€å»ç…§â€ç–‘ä¼¼ç‚’ä½œ\t9\n",
    "90å²è€å¤ªåŠä¸–çºªæ’®åˆ200å¤šå¯¹æ–°äºº(å›¾)\t5\n",
    "è¢ç«‹æŒ‘é€‰é’»æˆ’è¢«ç–‘å©šæœŸå°†è¿‘ ç”·ä¼´é…·ä¼¼æ¢æœä¼Ÿ(å›¾)\t9\n",
    "å›½åŠ¡é™¢ï¼šä¸¥æ‰“æ‹å–æ“æ§æœªæˆå¹´äººè¿æ³•çŠ¯ç½ª\t6\n",
    "éƒå¹³ä¸æƒ§åœŸè€³å…¶åŠ²æ—…çª˜å¢ƒï¼šæˆ‘å°±å–œæ¬¢æ¥è¿™ç§çƒ‚æ‘Šå­\t7\n",
    "...\n",
    "```\n",
    "\n",
    "å…¶ä¸­æœ«å°¾æ•°å­—ä»£è¡¨æ ‡ç­¾ç±»å‹ï¼Œæ•°å­—å’Œç§ç±»å¯¹ç…§å‚è§ `class.txt`ã€‚æ•°å­—æ ‡ç­¾å’Œæ–‡æœ¬ä¸­é—´ç”¨åˆ¶è¡¨ç¬¦éš”å¼€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é¢„å…ˆå‡†å¤‡\n",
    "\n",
    "### å¯¼å…¥å¿…è¦åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "An error occured.\n",
      "ValueError: \"@jupyter-widgets/jupyterlab-manager@2\" is not a valid npm package\n",
      "See the log file for details:  /tmp/jupyterlab-debug-tj_n2lgq.log\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences  # paddingå¥å­ç”¨\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "\n",
    "# å¦‚æœä½¿ç”¨jupyter labï¼Œé‚£ä¹ˆæ‰“å¼€ä»¥ä¸‹æ’ä»¶ï¼Œå¹¶ä¸”å®‰è£…äº†node.js\n",
    "# å‚è€ƒï¼šhttps://stackoverflow.com/questions/57343134/jupyter-notebooks-not-displaying-progress-bars\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager@2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ç‰ˆæœ¬ï¼š 1.6.0\n",
      "Transformers ç‰ˆæœ¬ï¼š 3.0.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch ç‰ˆæœ¬ï¼š {torch.__version__}\")\n",
    "print(f\"Transformers ç‰ˆæœ¬ï¼š {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„æˆ‘ç”¨çš„ PyTorch ç‰ˆæœ¬æ˜¯ 1.6.0ï¼Œç”¨æ–°ä¸ç”¨æ—§ã€‚\n",
    "\n",
    "ç†è®ºä¸Šè¿™äº›ä»£ç åœ¨ PyTorch 1.3.1 ä»¥ä¸Šéƒ½å¯ä»¥è¿è¡Œï¼Œå¦‚æœæœ‰é—®é¢˜è¯·åœ¨ Github ä¸Šå‘æˆ‘æé—®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "æ£€æŸ¥GPUçŠ¶æ€ï¼Œæˆ‘ä¹‹å‰ç”¨çš„æ˜¯ Tesla M40ï¼Œä¸è¿‡ç°åœ¨åªæœ‰ä¸€å—å¯æ€œçš„ GTX1070 äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "GPU numbers:  1\n",
      "device_name:  GeForce GTX 1070\n",
      "å½“å‰è®¾å¤‡ç¼–å·ï¼š0\n"
     ]
    }
   ],
   "source": [
    "# GPUcheck\n",
    "\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU numbers: \", n_gpu)\n",
    "    print(\"device_name: \", torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\")  # æ³¨æ„é€‰æ‹©\n",
    "    torch.cuda.set_device(0) \n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "print(f\"å½“å‰è®¾å¤‡ç¼–å·ï¼š{torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTç›¸å…³è®¾ç½®\n",
    "\n",
    "ä¸ºäº†è§„èŒƒå’Œå·¥ç¨‹åŒ–ï¼Œæˆ‘ä»¬æœ€å¥½ä¸è¦æŠŠä¸€äº›è¶…å‚æ•°é›¶é›¶æ•£æ•£åœ°åˆ†æ•£åœ¨æ•´ä¸ªä»£ç ä¸­ã€‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€ä¸ª`Config()`ç±»æ¥ç»Ÿä¸€ç®¡ç†è¿™äº›å‚æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"é…ç½®å‚æ•°\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Bert_NEWS_CLF.bin'\n",
    "        self.bert_path = './bert-chinese/'\n",
    "        self.train_file = '../datasets/THUCNews/train.txt'\n",
    "        \n",
    "        self.num_classes = 10                    # ç±»åˆ«æ•°(æŒ‰éœ€ä¿®æ”¹)\n",
    "        self.hidden_size = 768                   # éšè—å±‚è¾“å‡ºç»´åº¦\n",
    "        self.hidden_dropout_prob = 0.1           # dropoutæ¯”ä¾‹\n",
    "        self.batch_size = 128                    # mini-batchå¤§å°\n",
    "        self.max_len = 32                        # å¥å­çš„æœ€é•¿paddingé•¿åº¦\n",
    "        \n",
    "        self.epochs = 3                          # epochæ•°\n",
    "        self.learning_rate = 2e-5                # å­¦ä¹ ç‡        \n",
    "\n",
    "        self.save_path = './saved_model/'        # æ¨¡å‹è®­ç»ƒç»“æœä¿å­˜è·¯å¾„\n",
    "        \n",
    "        # self.fp16 = False\n",
    "        # self.fp16_opt_level = 'O1'\n",
    "        # self.gradient_accumulation_steps = 1\n",
    "        # self.warmup_ratio = 0.06\n",
    "        # self.warmup_steps = 0\n",
    "        # self.max_grad_norm = 1.0\n",
    "        # self.adam_epsilon = 1e-8\n",
    "        # self.class_list = class_list                              # ç±»åˆ«åå•\n",
    "        # self.require_improvement = 1000                                # è‹¥è¶…è¿‡1000batchæ•ˆæœè¿˜æ²¡æå‡ï¼Œåˆ™æå‰ç»“æŸè®­ç»ƒ\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®å¤„ç†\n",
    "\n",
    "### è¯»å–æ•°æ®\n",
    "\n",
    "æ”¾åœ¨ `./data/train.txt`ä¸­ï¼Œæœ‰éœ€è¦è¯·è‡ªå·±æ”¹è·¯å¾„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = config.train_file\n",
    "\n",
    "with open(file, encoding=\"utf-8\") as f:\n",
    "    sentences_and_labels = [line for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ\\t3\\n',\n",
       " 'ä¸¤å¤©ä»·ç½‘ç«™èƒŒåé‡é‡è¿·é›¾ï¼šåšä¸ªç½‘ç«™ç©¶ç«Ÿè¦å¤šå°‘é’±\\t4\\n',\n",
       " 'ä¸œ5ç¯æµ·æ£ å…¬ç¤¾230-290å¹³2å±…å‡†ç°æˆ¿98æŠ˜ä¼˜æƒ \\t1\\n',\n",
       " 'å¡ä½©ç½—ï¼šå‘Šè¯‰ä½ å¾·å›½è„šç”ŸçŒ›çš„åŸå›  ä¸å¸Œæœ›è‹±å¾·æˆ˜è¸¢ç‚¹çƒ\\t7\\n',\n",
       " '82å²è€å¤ªä¸ºå­¦ç”Ÿåšé¥­æ‰«åœ°44å¹´è·æˆæ¸¯å¤§è£èª‰é™¢å£«\\t5\\n',\n",
       " 'è®°è€…å›è®¿åœ°éœ‡ä¸­å¯ä¹ç”·å­©ï¼šå°†å—é‚€èµ´ç¾å›½å‚è§‚\\t5\\n',\n",
       " 'å†¯å¾·ä¼¦å¾è‹¥ç‘„éš”ç©ºä¼ æƒ… é»˜è®¤å…¶æ˜¯å¥³å‹\\t9\\n',\n",
       " 'ä¼ éƒ­æ™¶æ™¶æ¬²è½æˆ·é¦™æ¸¯æˆ˜ä¼¦æ•¦å¥¥è¿ è£…ä¿®åˆ«å¢…å½“å©šæˆ¿\\t1\\n',\n",
       " 'ã€Šèµ¤å£OLã€‹æ”»åŸæˆ˜è¯¸ä¾¯æˆ˜ç¡çƒŸåˆèµ·\\t8\\n',\n",
       " 'â€œæ‰‹æœºé’±åŒ…â€äº®ç›¸ç§‘åšä¼š\\t4\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å‰å‡ å¥\n",
    "sentences_and_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°æ®ä»¥ `table` åˆ†å‰²ï¼Œæ‰€ä»¥ç”¨ `split('\\t')`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸œ5ç¯æµ·æ£ å…¬ç¤¾230-290å¹³2å±…å‡†ç°æˆ¿98æŠ˜ä¼˜æƒ \n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq, label = sentences_and_labels[2].split('\\t')\n",
    "print(seq)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sentence_with_label in sentences_and_labels:\n",
    "    sentence, label = sentence_with_label.split('\\t')\n",
    "    sentences.append(sentence)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ', 'ä¸¤å¤©ä»·ç½‘ç«™èƒŒåé‡é‡è¿·é›¾ï¼šåšä¸ªç½‘ç«™ç©¶ç«Ÿè¦å¤šå°‘é’±', 'ä¸œ5ç¯æµ·æ£ å…¬ç¤¾230-290å¹³2å±…å‡†ç°æˆ¿98æŠ˜ä¼˜æƒ ', 'å¡ä½©ç½—ï¼šå‘Šè¯‰ä½ å¾·å›½è„šç”ŸçŒ›çš„åŸå›  ä¸å¸Œæœ›è‹±å¾·æˆ˜è¸¢ç‚¹çƒ', '82å²è€å¤ªä¸ºå­¦ç”Ÿåšé¥­æ‰«åœ°44å¹´è·æˆæ¸¯å¤§è£èª‰é™¢å£«']\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æŒ‰å­—æ‹†åˆ†ï¼š\n",
    "    \n",
    "    \n",
    "    \n",
    "æŒ‰ç…§BERTçš„è¦æ±‚ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è¾“å…¥ä¸ºï¼š\n",
    "```\n",
    "[CLS]<å¥å­A>[SEP]<å¥å­B>[SEP]\n",
    "```\n",
    "è¿™æ ·çš„å½¢å¼ï¼Œä½†æ˜¯å¾ˆæ˜æ˜¾æˆ‘ä»¬çš„æ–°é—»æ ‡é¢˜æ˜¯ä¸é€‚åˆè¿™æ ·åˆ†å¼€çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è¾“å…¥å½¢å¼æ˜¯ï¼š\n",
    "```\n",
    "[CLS]<å¥å­A>[SEP]\n",
    "```\n",
    "\n",
    "BERTä¸éœ€è¦åˆ†è¯ï¼Œæˆ‘ä»¬åªè¦ç›´æ¥å°†ä»–ä»¬è½¬æ¢ä¸º `vocab.txt` å­—å…¸ä¸­å¯¹åº”çš„å­—ç´¢å¼•å³å¯ï¼ŒåŒ…æ‹¬`[CLS]`å’Œ`[SEP]`ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x7fba7c1e3990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path, \n",
    "                                          do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶ä¸­ï¼Œ`BertTokenizer.from_pretrained()` æ˜¯ä»æœ¬åœ°ï¼ˆæˆ–è€…åœ¨çº¿ä¸‹è½½ï¼‰ç›¸å…³çš„BertTokenæ–‡ä»¶ã€‚\n",
    "\n",
    "æˆ‘æœ¬åœ°å·²ç»æœ‰æ­¤æ–‡ä»¶äº†ï¼Œæ‰€ä»¥åœ°å€å°±å­˜åœ¨äº† `config.bert_path` ä¸­ï¼Œç›´æ¥æŒ‡å®šå³å¯å¯¼å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—§ç‰ˆä»£ç ï¼š\n",
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "# æ–°ç‰ˆä»£ç ï¼š\n",
    "# text_batch = sentences\n",
    "# encoding = tokenizer(text_batch, \n",
    "#                     return_tensors='pt',  # pt æŒ‡ pytorchï¼Œtf å°±æ˜¯ tensorflow \n",
    "#                     padding=True,  # paddingåˆ°æœ€é•¿çš„é‚£å¥è¯\n",
    "#                     truncation=True,  # æ¿€æ´»å¹¶æ§åˆ¶æˆªæ–­\n",
    "#                     max_length=50)\n",
    "#input_ids = encoding['input_ids']\n",
    "#input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_special_tokens=True` æŒ‡çš„æ˜¯æ˜¯å¦è¦åœ¨å¥é¦–å’Œå¥å°¾è‡ªåŠ¨æ·»åŠ  `[CLS]` å’Œ `[SEP]` tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\n",
      "ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ\n",
      "\n",
      "Tokenize åçš„ç¬¬ä¸€å¥è¯: \n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n"
     ]
    }
   ],
   "source": [
    "# è¿™å¥è¯çš„input_ids\n",
    "print(f\"Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\\n{sentences[0]}\\n\")\n",
    "print(f\"Tokenize åçš„ç¬¬ä¸€å¥è¯: \\n{tokenized_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šé¢çš„ä¾‹å­è¯´æ˜ï¼š\n",
    "\n",
    "é¦–å…ˆencodeåŒ…å«äº†ä¸¤ä¸ªåŠ¨ä½œï¼Œ\n",
    "\n",
    "**ç¬¬ä¸€**ï¼Œå¯¹å¥å­\n",
    "\n",
    "    ```ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ```\n",
    "\n",
    "çš„å‰åæ·»åŠ æ ‡ç­¾ï¼Œå³ï¼š\n",
    "\n",
    "    ```[CLS]ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ[SEP]```\n",
    "\n",
    "**ç¬¬äºŒ**ï¼Œå°†æ·»åŠ æ ‡ç­¾åçš„å¥å­æŒ‰å­—ç¬¦ï¼ˆæ ‡ç‚¹ç¬¦å·ä¹Ÿå•ç‹¬ç®—ä¸€ä¸ªå­—ç¬¦ï¼‰åˆ†å¼€ï¼Œç„¶åè½¬æ¢ä¸º `input_ids`ï¼š\n",
    "\n",
    "    ```[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]```\n",
    "\n",
    "> éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå…¶ä¸­ `101` æ˜¯ `[CLS]` çš„ç´¢å¼•ï¼Œ`102` æ˜¯ `[SEP]` çš„ç´¢å¼•ã€‚\n",
    "\n",
    "ä¸Šè¿°è¿‡ç¨‹ç§°ä¸º `tokenized`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "print (len(tokenized_texts))  # 180000å¥è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "ä¸ºäº†ä¿è¯è¾“å…¥é•¿åº¦çš„ç»Ÿä¸€ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å¥å­è¿›è¡Œpaddingã€‚\n",
    "\n",
    "æœ¬ä¾‹ä¸­é‡‡ç”¨çš„æ˜¯æ–°é—»æ ‡é¢˜ï¼Œæ‰€ä»¥æ ‡é¢˜ä¸ä¼šå¤ªé•¿ï¼Œæˆ‘ä»¬é™å®šä¸º `32` ä¸ªå­—ç¬¦ï¼Œæ­¤å‚æ•°åœ¨ `config.max_len` ä¸­ã€‚\n",
    "\n",
    "ä¸€æ—¦æ ‡é¢˜é•¿åº¦è¶…è¿‡32ä¸ªå­—ç¬¦ï¼Œåˆ™ä¼šæˆªæ–­è¶…è¿‡éƒ¨åˆ†ä¸ç”¨ï¼›å¦‚æœä¸è¶³ `32` ä¸ªå­—ç¬¦ï¼Œåˆ™æ‰§è¡Œ `pad_sequences` ï¼ˆå³`padding`ï¼‰æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¾“å…¥padding\n",
    "# æ­¤å‡½æ•°åœ¨kerasé‡Œé¢\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=config.max_len, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\n",
      "\n",
      "ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ\n",
      "\n",
      "\n",
      "Tokenize åçš„ç¬¬ä¸€å¥è¯: \n",
      "\n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n",
      "\n",
      "\n",
      "Padding åçš„ç¬¬ä¸€å¥è¯ï¼š \n",
      "\n",
      "[ 101  704 1290 1957 2094 2110 7368 8038 3315 4906 2231 3613  788  122\n",
      "  683  689 2875 4511 4495  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\\n\\n{sentences[0]}\\n\\n\")\n",
    "print(f\"Tokenize åçš„ç¬¬ä¸€å¥è¯: \\n\\n{tokenized_texts[0]}\\n\\n\")\n",
    "print(f\"Padding åçš„ç¬¬ä¸€å¥è¯ï¼š \\n\\n{input_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶å® `padding` ä¹‹åè¿˜å¯è½¬æ¢å›æ¥ï¼Œ\n",
    "\n",
    "å¾ˆå®¹æ˜“çœ‹å‡ºæ¯ä¸ªå­—ï¼ŒåŒ…æ‹¬`[PAD]`ã€`[CLS]`ã€`[SEP]`æ‰€åœ¨çš„ä½ç½®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] ä¸­ å å¥³ å­ å­¦ é™¢ ï¼š æœ¬ ç§‘ å±‚ æ¬¡ ä»… 1 ä¸“ ä¸š æ‹› ç”· ç”Ÿ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# è½¬æ¢å›æ¥\n",
    "raw_texts = [tokenizer.decode(input_ids[0])]\n",
    "print(raw_texts)\n",
    "print(len(raw_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTçš„è¾“å…¥å‡†å¤‡\n",
    "\n",
    "### æ³¨æ„åŠ›maskï¼ˆattention masksï¼‰ï¼š\n",
    "\n",
    "BERT æ¨¡å‹çš„æ ¸å¿ƒæ˜¯ Transformer ç»“æ„ï¼Œå…¶ä¸­å¾ˆé‡è¦çš„ä¸€ç‚¹å°±æ˜¯ self-attention ç»“æ„ã€‚\n",
    "\n",
    "BERT-Chinese æ¨¡å‹åŒ BERT-base æ¨¡å‹ç»“æ„ä¸€è‡´ï¼Œæ¯å±‚æœ‰12ä¸ªè‡ªæ³¨æ„å¤´ï¼Œä¸ºäº†ä¸è®©è¿™äº› self-attention ç»“æ„æ³¨æ„åˆ°è¡¥å…¨çš„`[PAD]`éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦è¾“å…¥ä¸€ä¸ª attention_masks æ ‡ç­¾ï¼Œå‘Šè¯‰æ¨¡å‹å“ªäº›å†…å®¹æ˜¯çœŸå®å†…å®¹ï¼Œå“ªäº›æ˜¯æ— æ„ä¹‰çš„`[PAD]`ã€‚\n",
    "\n",
    "åˆšåˆšè¯´åˆ°è¢« `padding` éƒ¨åˆ†æ˜¯ä¸éœ€è¦è¢« attention åˆ°çš„ã€‚ç›¸å½“äºè¿™éƒ¨åˆ†åœ¨  attention_masks ä¸­çš„æ ‡ç­¾å°±æ˜¯çœŸå®å¥å­ä¸º 1 ï¼Œpadding éƒ¨åˆ†ä¸º 0 ã€‚æ‰€ä»¥æˆ‘ä»¬å¾—åˆ° attention masksï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºattention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸€å¥è¯çš„ attention_masks\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  å‡†å¤‡Labels\n",
    "\n",
    "é¦–å…ˆå‡†å¤‡Labelsã€‚è¿™äº›æ ‡é¢˜çš„ Labels åœ¨ä¸€å¼€å§‹å°±å·²ç»åˆ†ç¦»å¼€æ¥ï¼Œä¿å­˜åœ¨äº† `labels` é‡Œé¢\n",
    " \n",
    "è¿™é‡Œå¯ä»¥ç”¨ `train_test_split` æ¥åˆ†ã€‚æ³¨æ„ï¼Œç”±äºå¤šäº†ä¸€ä¸ª `attention_masks` æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨ä¸¤æ¬¡ `train_test_split`ï¼Œå¹¶ä¸”é‡‡ç”¨ç›¸åŒçš„éšæœºç§å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n', '5\\n', '9\\n', '1\\n', '8\\n', '4\\n']\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºç°åœ¨çš„labelsé‡Œé¢å¹¶ä¸æ˜¯æ•°å­—ï¼Œè€Œä¸”æœ‰æ¢è¡Œç¬¦`\\n`ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 1, 7, 5, 5, 9, 1, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "clean_labels = []\n",
    "for label in labels:\n",
    "    clean_labels.append(int(label.strip('\\n')))\n",
    "\n",
    "print(clean_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, clean_labels, \n",
    "                                                            random_state=2019, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2019, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 4, 3, 8, 8, 1, 8, 7, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      æ ‡ç­¾æ€»æ•°ï¼š 180000\n",
      "è®­ç»ƒé›†æ ‡ç­¾æ€»æ•°ï¼š 162000\n",
      "éªŒè¯é›†æ ‡ç­¾æ€»æ•°ï¼š 18000\n"
     ]
    }
   ],
   "source": [
    "print(f\"      æ ‡ç­¾æ€»æ•°ï¼š\", len(labels))\n",
    "print(f\"è®­ç»ƒé›†æ ‡ç­¾æ€»æ•°ï¼š\", len(train_labels))\n",
    "print(f\"éªŒè¯é›†æ ‡ç­¾æ€»æ•°ï¼š\", len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å’Œ TF2.0 å¯ä»¥ç›´æ¥æ¥å—`ndarray`ä¸åŒï¼ŒPyTorchè¦æ±‚æˆ‘ä»¬å…ˆå°†æ•°æ®è½¬åŒ–ä¸º `Tensor` å½¢å¼å†è¾“å…¥æ¨¡å‹ã€‚\n",
    "\n",
    "ç°æ‰€ä»¥æˆ‘ä»¬å‡†å¤‡ Tensor åŒ–ã€‚\n",
    "\n",
    "> ç”±äºç›´æ¥ä½¿ç”¨ `torch.tensor()` ä¼šå¼¹å‡ºè­¦å‘Šï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨æ›´å®‰å…¨çš„ä»£ç ã€‚\n",
    "\n",
    "**æ—§**ï¼š\n",
    "```python\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "```\n",
    "\n",
    "**æ–°**:\n",
    "```python\n",
    "train_inputs = torch.tensor(train_inputs).clone().detach()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensoråŒ–\n",
    "train_inputs = torch.tensor(train_inputs).clone().detach()\n",
    "validation_inputs = torch.tensor(validation_inputs).clone().detach()\n",
    "train_labels = torch.tensor(train_labels).clone().detach()\n",
    "validation_labels = torch.tensor(validation_labels).clone().detach()\n",
    "train_masks = torch.tensor(train_masks).clone().detach()\n",
    "validation_masks = torch.tensor(validation_masks).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_inputs))\n",
    "print(len(validation_labels))\n",
    "print(len(validation_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ›å»ºè¿­ä»£å™¨\n",
    "\n",
    "æˆ‘ä»¬é‡‡ç”¨\n",
    "\n",
    "`torch.utils.data.TensorDataset` å°†ä»–ä»¬å°è£…ä¸º `TensorDataset` çš„å½¢å¼ï¼Œ\n",
    "\n",
    "`torch.utils.data.RandomSampler` é‡‡ç”¨éšæœºé‡‡æ ·çš„æ–¹æ³•ä»ä¸­é‡‡æ ·ï¼Œ\n",
    "\n",
    "`torch.utils.data.DataLoader` è‡ªåŠ¨å½¢æˆè¿­ä»£å™¨ã€‚\n",
    "\n",
    "> æ³¨æ„ batch_size çš„è®¾ç½®å¤§å°å’Œæ˜¾å­˜å¤§å°å¯†åˆ‡ç›¸å…³ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "print(config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½¢æˆè®­ç»ƒæ•°æ®é›†\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# éšæœºé‡‡æ ·\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# è¯»å–æ•°æ®\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=config.batch_size)\n",
    "\n",
    "\n",
    "# å½¢æˆéªŒè¯æ•°æ®é›†\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# éšæœºé‡‡æ ·\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# è¯»å–æ•°æ®\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTçš„å¾®è°ƒ\n",
    "\n",
    "åœ¨å‡†å¤‡å¥½è¾“å…¥ä»¥åï¼Œç°åœ¨æˆ‘ä»¬å¼€å§‹å¾®è°ƒBERTæ¨¡å‹ã€‚\n",
    "\n",
    "ä½¿ç”¨ `BertForSequenceClassification`ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªæ™®é€šBERTæ¨¡å‹ï¼Œåœ¨æœ€åé¢åŠ äº†ä¸€ä¸ªçº¿å½¢å±‚ç”¨äºåˆ†ç±»ã€‚\n",
    "\n",
    "### å¯¼å…¥æ¨¡å‹\n",
    "\n",
    "ç›´æ¥ä½¿ç”¨ `from_pretrained` å¯¼å…¥é¢„è®­ç»ƒå¥½çš„ä¸­æ–‡ BERT æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# ç»Ÿè®¡æ ‡ç­¾ç§ç±»\n",
    "label_count = len(set(labels))\n",
    "print(label_count)\n",
    "\n",
    "config.num_classes = label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-chinese/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-chinese/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å– BertForSequenceClassification æ¨¡å‹ï¼Œ\n",
    "# æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„BERTæ¨¡å‹ï¼Œåœ¨æœ€åé¢åŠ äº†ä¸€ä¸ªçº¿å½¢å±‚ç”¨äºåˆ†ç±»ã€‚\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(config.bert_path, \n",
    "                                                      num_labels=config.num_classes)\n",
    "model.cuda()\n",
    "\n",
    "# æ³¨æ„ï¼š\n",
    "# åœ¨æ–°ç‰ˆçš„ Transformers ä¸­ä¼šç»™å‡ºè­¦å‘Š\n",
    "# åŸå› æ˜¯æˆ‘ä»¬å¯¼å…¥çš„é¢„è®­ç»ƒå‚æ•°æƒé‡æ˜¯ä¸åŒ…å«æ¨¡å‹æœ€ç»ˆçš„çº¿æ€§å±‚æƒé‡çš„\n",
    "# ä¸è¿‡æˆ‘ä»¬æœ¬æ¥å°±æ˜¯è¦â€œå¾®è°ƒâ€å®ƒï¼Œæ‰€ä»¥è¿™ä¸ªæƒ…å†µæ˜¯ç¬¦åˆæœŸæœ›çš„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å®Œæˆ `model.cuda()` è¿™ä¸€æ­¥ä»¥åï¼Œæ¨¡å‹å°±æ”¾åœ¨æ˜¾å­˜ä¸­äº†ã€‚åŒæ—¶ä¹Ÿæ‰“å°å‡ºäº†æ¨¡å‹çš„å…·ä½“ç»“æ„ã€‚\n",
    "\n",
    "> éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬ç›´æ¥é‡‡ç”¨äº† `BertForSequenceClassification.from_pretrained()`ï¼Œå®é™…ä¸Šå¦‚æœæœ‰æ›´çµæ´»çš„éœ€æ±‚æ—¶ï¼Œåº”è¯¥ä½¿ç”¨ `BertModel.from_pretrained()` æ­å»ºæ¨¡å‹ã€‚ä¸¤è€…çš„åŒºåˆ«å¯ä»¥å‚è€ƒæºä»£ç ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œæ¥ä¸‹æ¥ç»™å‡ºä¸€æ®µé­”æ”¹æ¨¡å‹å‚è€ƒï¼Œæˆ‘ä»¬é‡‡ç”¨äº† `BERT` è¾“å‡ºåæ‹¼æ¥å…¶ä»–ç‰¹å¾å†è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼š\n",
    "\n",
    "```python\n",
    "class ModelBert(nn.Module):\n",
    "    \"\"\"\n",
    "    æ–°å¢ è¡Œä¸šID ç‰¹å¾æ‹¼æ¥ã€‚\n",
    "    \n",
    "    æˆ‘ä»¬æƒ³è¦å¯¹å•†å“åç§°è¿›è¡Œåˆ†ç±»ï¼Œå•†å“åç§°æ˜¯æ–‡æœ¬å½¢å¼ï¼Œä¼šè¿›å…¥BERTæ¨¡å‹ã€‚\n",
    "    è¡Œä¸šIDï¼Œå³ä»£ç ä¸­çš„â€œHYIDâ€æœ¬èº«å°±æ˜¯æ•°å­—å½¢å¼ï¼Œæ²¡å¿…è¦æ”¾å…¥BERTæ¨¡å‹ä¸­ï¼Œäºæ˜¯æˆ‘ä»¬å°† BERT è¾“å‡ºåçš„ 768 ç»´å‘é‡æ‹¼æ¥\n",
    "    tensor(HYID)ï¼Œä¹Ÿå°±æ˜¯å˜æˆäº† 769 ç»´ï¼Œå†è¿‡ä¸€ä¸ª çº¿å½¢å±‚ + softmax è¾“å‡ºåˆ†ç±»ç»“æœã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(ModelBert, self).__init__()\n",
    "        self.num_labels = config.num_classes\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size + 1, config.num_classes)  # è¿™é‡Œç»´åº¦ +1 æ”¾å…¥HYID\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        HYID=None):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds,)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        HYID_f = HYID.float()\n",
    "        pooled_output = torch.cat((pooled_output, HYID_f), dim=1)  # æ‹¼æ¥åœ¨è¿™é‡Œ\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs        \n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) \n",
    "    \n",
    "    model = ModelBert(config)\n",
    "    model.cuda()\n",
    "```\n",
    "\n",
    "### å‡†å¤‡å¾®è°ƒ\n",
    "\n",
    "ä¼—æ‰€å‘¨çŸ¥ï¼Œæˆ‘ä»¬æ‰€ä½¿ç”¨çš„ BERT æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®ä¸åŒçš„ä»»åŠ¡ï¼Œæ¯”å¦‚è¿™é‡Œçš„ æ–‡æœ¬åˆ†ç±»ï¼Œæˆ–è€… NER ä¹‹ç±»çš„ä»»åŠ¡ï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬è®¾ç½®å¥½ç›¸å…³çš„ä¼˜åŒ–å™¨å’Œè¯„ä»·å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# æƒé‡è¡°å‡\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶ä¸­ï¼Œ`no_decay`è§[issue#492](https://github.com/huggingface/transformers/issues/492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¼˜åŒ–å™¨\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†ç¡®ç‡è®¡ç®—å‡½æ•°\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒ Epochsï¼š 3\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜loss\n",
    "train_loss_set = []\n",
    "# epochs \n",
    "print(f\"è®­ç»ƒ Epochsï¼š {config.epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹å¾®è°ƒ\n",
    "\n",
    "3ä¸ªepochï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰epochï¼š 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cd23ba68d044f5ab42abc9ad89bd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å½“å‰ epoch çš„ Train loss: 0.29743796679386975\n",
      "å½“å‰epochï¼š 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47efde0a4629427d8e24b21b3bee70c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å½“å‰ epoch çš„ Train loss: 0.1526083102741089\n",
      "å½“å‰epochï¼š 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d2388ebec544ef8465f0668d9e2c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å½“å‰ epoch çš„ Train loss: 0.10776557406413517\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in range(config.epochs): \n",
    "    ## è®­ç»ƒ\n",
    "    print(f\"å½“å‰epochï¼š {_}\")\n",
    "    # å¼€å¯è®­ç»ƒæ¨¡å¼\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # æŠŠbatchæ”¾å…¥GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # è§£åŒ…batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # æ¢¯åº¦å½’é›¶\n",
    "        optimizer.zero_grad()\n",
    "        # å‰å‘ä¼ æ’­lossè®¡ç®—\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  # æœ‰labelsçš„æ—¶å€™ï¼Œä¸”labels>1å°±ç›´æ¥è¿”å›Cross-Entropy\n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"å½“å‰ epoch çš„ Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰¹é‡éªŒè¯\n",
    "\n",
    "æˆ‘ä»¬ä¹‹å‰å·²ç»ä½¿ç”¨ `train_test_split()` ä¸ºäº¤å‰éªŒè¯åšå¥½äº†å‡†å¤‡ã€‚å› æ­¤å¯ä»¥ç›´æ¥è¿›å…¥éªŒè¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯çŠ¶æ€\n",
    "model.eval()\n",
    "\n",
    "# å»ºç«‹å˜é‡\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdf6d12a4ea451aac64966aff6ce17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.9465093085106382\n"
     ]
    }
   ],
   "source": [
    "# éªŒè¯é›†çš„è¯»å–ä¹Ÿè¦batch\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # å…ƒç»„æ‰“åŒ…æ”¾è¿›GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # è§£å¼€å…ƒç»„\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # é¢„æµ‹\n",
    "    with torch.no_grad():\n",
    "        # segment embeddingsï¼Œå¦‚æœæ²¡æœ‰å°±æ˜¯å…¨0ï¼Œè¡¨ç¤ºå•å¥\n",
    "        # position embeddingsï¼Œ[0,å¥å­é•¿åº¦-1]\n",
    "        logits = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()  # æ³¨æ„è¿™é‡Œçš„logitsæ˜¯åœ¨softmaxä¹‹å‰ï¼Œæ‰€ä»¥å’Œä¸ä¸º1\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    # print(logits, label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)  # è®¡ç®—å‡†ç¡®ç‡\n",
    "    eval_accuracy += tmp_eval_accuracy  # å‡†ç¡®ç‡ç§¯ç´¯\n",
    "    nb_eval_steps += 1  # æ­¥æ•°ç§¯ç´¯\n",
    "print(f\"Validation Accuracy: {eval_accuracy/nb_eval_steps}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å•æ¡é¢„æµ‹\n",
    "\n",
    "ä¸ºäº†è¯¦ç»†è¯´æ˜æˆ‘ä»¬å¾®è°ƒåçš„ BERT é¢„æµ‹è¿‡ç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å•æ¡é¢„æµ‹çš„æ–¹å¼å±•ç¤ºä¸€ä¸‹ç›¸å…³ç»“æœã€‚\n",
    "\n",
    "æœ‰è¿™æ ·ä¸€æ¡æ–°é—»æ ‡é¢˜ï¼š\n",
    "\n",
    "```è¯æ±‡é˜…è¯»æ˜¯å…³é”® 08å¹´è€ƒç ”æš‘æœŸè‹±è¯­å¤ä¹ å…¨æŒ‡å—```\n",
    "\n",
    "å…¶ç±»åˆ«ä¸º\n",
    "\n",
    "```3. education```\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬é‡‡ç”¨å•æ¡é¢„æµ‹çš„å½¢å¼çœ‹çœ‹æ¨¡å‹å¯¹å®ƒçš„é¢„æµ‹ã€‚\n",
    "\n",
    "æµç¨‹å’Œä¹‹å‰ä¸€æ ·ï¼Œä¹Ÿæ˜¯å…ˆ tokenize å†åš padding å’Œ maskã€‚å…·ä½“å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = \"è¯æ±‡é˜…è¯»æ˜¯å…³é”® 08å¹´è€ƒç ”æš‘æœŸè‹±è¯­å¤ä¹ å…¨æŒ‡å—\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_texts = [tokenizer.encode(test_sent, add_special_tokens=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = pad_sequences([txt for txt in test_tokenized_texts],\n",
    "                          maxlen=config.max_len, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\n",
      "è¯æ±‡é˜…è¯»æ˜¯å…³é”® 08å¹´è€ƒç ”æš‘æœŸè‹±è¯­å¤ä¹ å…¨æŒ‡å—\n",
      "\n",
      "\n",
      "Tokenize åçš„ç¬¬ä¸€å¥è¯: \n",
      "[[101, 6404, 3726, 7325, 6438, 3221, 1068, 7241, 8142, 2399, 5440, 4777, 3264, 3309, 5739, 6427, 1908, 739, 1059, 2900, 1298, 102]]\n",
      "\n",
      "\n",
      "Padding åçš„ç¬¬ä¸€å¥è¯ï¼š \n",
      "[[ 101 6404 3726 7325 6438 3221 1068 7241 8142 2399 5440 4777 3264 3309\n",
      "  5739 6427 1908  739 1059 2900 1298  102    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\\n{test_sent}\\n\\n\")\n",
    "print(f\"Tokenize åçš„ç¬¬ä¸€å¥è¯: \\n{test_tokenized_texts}\\n\\n\")\n",
    "print(f\"Padding åçš„ç¬¬ä¸€å¥è¯ï¼š \\n{test_input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºattention masks\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensoråŒ–\n",
    "test_inputs = torch.tensor(test_input_ids).clone().detach()\n",
    "test_masks = torch.tensor(test_attention_masks).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½¢æˆéªŒè¯æ•°æ®é›†\n",
    "# ä¸ºäº†é€šç”¨ï¼Œè¿™é‡Œè¿˜æ˜¯ç”¨äº† DataLoader çš„å½¢å¼\n",
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "# éšæœºé‡‡æ ·\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# è¯»å–æ•°æ®\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f838970b54114e1ca7ab7b7a032ea11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# éªŒè¯é›†çš„è¯»å–ä¹Ÿè¦batch\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # å…ƒç»„æ‰“åŒ…æ”¾è¿›GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # è§£å¼€å…ƒç»„\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # é¢„æµ‹\n",
    "    outputs = model(input_ids=b_input_ids, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    token_type_ids=None,\n",
    "                    position_ids=None)\n",
    "                       \n",
    "    # Move logits and labels to CPU\n",
    "    scores = outputs[0].detach().cpu().numpy()  # æ¯ä¸ªå­—çš„æ ‡ç­¾çš„æ¦‚ç‡\n",
    "    pred_flat = np.argmax(scores, axis=1).flatten()\n",
    "    # label_ids = b_labels.to('cpu').numpy()  # çœŸå®labels\n",
    "    print(pred_flat)  # é¢„æµ‹å€¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹å‡ºï¼Œé¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚è¿™æ ·çš„å•å¥é¢„æµ‹å¸¸å¸¸ç”¨åœ¨åœ¨éœ€è¦æ¼”ç¤ºçš„æƒ…å†µä¸‹ï¼Œè¿™æ—¶å€™ç›´æ¥æ›¿æ¢ `test_sent` å°±è¡Œäº†ã€‚\n",
    "\n",
    "å¦‚æœä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œè¿˜å¯ä»¥æ‰“åŒ…ä¸ºä¸€ä¸ªå‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¿å­˜å’Œè¯»å–æ¨¡å‹\n",
    "\n",
    "æ¨¡å‹è®­ç»ƒå®Œæ¯•åï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¿å­˜ã€‚ PyTorch çš„ä¿å­˜å’Œ Transformers æ˜¯ä¸å¤ªä¸€æ ·çš„ï¼Œè¿™é‡Œéœ€è¦**ç‰¹åˆ«è¯´æ˜**ã€‚\n",
    "\n",
    "### åŸºäº PyTorch ä¿å­˜ / è¯»å–\n",
    "\n",
    "ä¸€èˆ¬æˆ‘ä»¬é‡‡ç”¨ PyTorch è‡ªå¸¦çš„æ¨¡å‹ä¿å­˜æ–¹æ³•ã€‚æ­¤æ–¹æ³•å¯ä»¥ä¿å­˜è¯»å–å„ç§ç½‘ç»œç»“æ„çš„æ¨¡å‹ã€‚\n",
    "\n",
    "ä½¿ç”¨ `torch.save()` æ–¹æ³•å°†æ¨¡å‹å‚æ•°ä¿å­˜ä¸º `state_dict`ï¼Œè¯»å–æ—¶å€™ä¹Ÿæ˜¯ç›¸åŒçš„é“ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–‡ä»¶å¤¹\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºæ–‡ä»¶å¤¹\n",
    "if not os.path.exists(config.save_path):\n",
    "    os.makedirs(config.save_path)\n",
    "    print(\"æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–‡ä»¶å¤¹!\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "model_to_save = model.module if hasattr(model, 'module') else model                          # ç”¨äºå¤šå¡è®­ç»ƒçš„æƒ…å†µ\n",
    "torch.save(model_to_save.state_dict(), os.path.join(config.save_path, config.model_name))    # æ¨¡å‹ä¿å­˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¿å­˜å®Œæ¯•åæˆ‘ä»¬è¯»å–æ¨¡å‹ã€‚\n",
    "\n",
    "> æ³¨æ„ï¼Œè¿™é‡Œæˆ‘åˆ†åˆ«è¯»å–äº† **æ¨¡å‹ç»“æ„** å’Œ **Tokenizer**ï¼Œä¸¤è€…è·¯å¾„è¦æŒ‰å®é™…è°ƒæ•´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å–æ¨¡å‹\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path)\n",
    "model = BertForSequenceClassification.from_pretrained(config.bert_path, num_labels=config.num_classes)\n",
    "model.load_state_dict(torch.load(os.path.join(config.save_path, config.model_name)))\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒæ ·çš„ï¼Œå¦‚æœæˆ‘è¦è¯»å–é­”æ”¹çš„æ¨¡å‹ï¼Œæ¯”å¦‚å‰é¢æåˆ°çš„ `ModelBert`ï¼Œä¸€æ ·å¯é‡‡ç”¨ `model.load_state_dict()` æ¥è¯»å–ï¼Œä½†æ˜¯å…ˆè¦å®šä¹‰å¥½æ¨¡å‹ç»“æ„æ‰è¡Œã€‚\n",
    "\n",
    "ç¤ºä¾‹å¦‚ä¸‹ï¼Œå‡è®¾å·²ç»æ­å»ºå¥½äº†æ¨¡å‹ç»“æ„äº†ï¼Œç°åœ¨éœ€è¦è¯»å–ä¹‹å‰è®­ç»ƒå¥½ä¸”ä¿å­˜å¥½çš„æ¨¡å‹å‚æ•°ï¼š\n",
    "\n",
    "```python\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path)\n",
    "model = ModelBert(config)\n",
    "model.load_state_dict(torch.load(os.path.join(config.save_path, config.model_name)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŸºäº Transformers ä¿å­˜ / è¯»å–ï¼ˆä¸æ¨èï¼‰\n",
    "\n",
    "å¦‚æœç›´æ¥é‡‡ç”¨çš„æ˜¯ Transformers è‡ªå¸¦çš„æ¨¡å‹ç»“æ„ï¼Œæ¯”å¦‚ä¸Šæ–‡çš„ `BertForSequenceClassification`ï¼ŒTransformers è‡ªå¸¦äº†ä¸€ç§ `save_pretrained` ä¿å­˜æ–¹æ³•ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†æ¨¡å‹ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶ã€‚\n",
    "\n",
    "> ä¸ºä»€ä¹ˆä¸æ¨èè¿™ä¸ªæ–¹æ³•å‘¢ï¼Ÿ\n",
    ">\n",
    "> å› ä¸ºå¦‚æœä½ éœ€è¦ä»¥ BERT ä¸ºæ ¸å¿ƒï¼Œå¯¹æ¨¡å‹è¿›è¡Œé­”æ”¹çš„è¯ï¼Œè¿™ä¸ªåŠŸèƒ½å¾ˆå¯èƒ½ç”±äºæ¨¡å‹ç»“æ„çš„ä¸åŒè€ŒæŠ¥é”™æˆ–è€…å¯¼è‡´æ•ˆæœä¸€å¡Œç³Šæ¶‚ã€‚\n",
    ">\n",
    "> é­”æ”¹æ¨¡å‹å‚è€ƒä¸Šæ–‡ä¸­çš„ `ModelBert` éƒ¨åˆ†ï¼Œæ¢å¥è¯è¯´ï¼Œæ¨¡å‹ç»“æ„ç”±è‡ªå·±å®šä¹‰ï¼Œå·²ç»å’Œ Transformers è‡ªå¸¦çš„ç»“æ„ä¸ä¸€è‡´äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ–‡ä»¶å¤¹\n",
    "if not os.path.exists(config.save_path):\n",
    "    os.makedirs(config.save_path)\n",
    "    print(\"æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–‡ä»¶å¤¹!\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# ä¿å­˜\n",
    "model_to_save2 = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save2.save_pretrained(config.save_path)\n",
    "tokenizer.save_pretrained(config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–æ¨¡å‹\n",
    "model = BertModel.from_pretrained(config.save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(config.save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°ç»“\n",
    "\n",
    "å½“å‰æˆ‘é‡‡ç”¨çš„å¡æ˜¯ Tesla M40ï¼Œå•å¡24GBæ˜¾å­˜ã€‚\n",
    "\n",
    "æ€»å…±è·‘äº†3ä¸ª epochï¼Œbatch_size æ˜¯ 512ã€‚\n",
    "\n",
    "`tqdm_notebook` çš„è¾“å‡ºæ—¶é—´æœ‰ç‚¹è¿·ï¼Œè¿™é‡Œå°±ä¸è®°å½•äº†ã€‚ä¹‹å‰ç”¨ `tqdm` çš„æ—¶å€™å¤§æ¦‚ä¸€ä¸ª epoch åœ¨ 23 åˆ†é’Ÿå·¦å³ã€‚\n",
    "\n",
    "æµ‹è¯•é›†ä»…ä»…æ˜¯å‰å‘ä¼ æ’­ï¼Œæ€»å…±18000æ¡æ•°æ®åªèŠ±äº†ä¸åˆ°ä¸€åˆ†é’Ÿå°±å®Œæˆäº†ã€‚\n",
    "\n",
    "æœ€åå‡†ç¡®ç‡94.4%ã€‚\n",
    "\n",
    "å®é™…ä¸Šè¿™ä¸ªå‡†ç¡®ç‡æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼ŒåŒ…æ‹¬è°ƒå‚ï¼ŒåŒ…æ‹¬æ¸…æ´—ä»€ä¹ˆçš„éƒ½æ²¡æœ‰åšï¼ˆå®é™…ä¸Šæ•°æ®é›†å¾ˆå¹²å‡€äº†ä¹Ÿæ²¡å¿…è¦åšï¼‰ï¼Œç›´æ¥å°±æ˜¯BERTæ¢­å“ˆã€‚\n",
    "\n",
    "> æ€»ä½“æ¥è¯´ï¼Œæˆ‘å†™çš„è¿™ä¸ªæ•™ç¨‹è¿˜æ˜¯å¾ˆè¯¦ç»†å¾ˆå‹å¥½çš„ï¼Œè€Œä¸”æ²¡æœ‰ä½¿ç”¨è€æ—§çš„ `pytorch-pretrained-bert`ï¼Œè€Œæ˜¯å½“å‰æœ€æ–°ç‰ˆæœ¬çš„ `transformers`ã€‚\n",
    ">\n",
    "> å…¶æ¬¡ï¼Œæ¯ä¸€æ­¥å‡ ä¹éƒ½æœ‰æ¯”è¾ƒè¯¦ç»†çš„ä¸­é—´è¿‡ç¨‹è§£é‡Šï¼Œæˆ‘ç›¸ä¿¡ç¨å¾®ç†Ÿæ‚‰ PyTorch å’Œ BERT åŸºæœ¬ç»“æ„çš„äººéƒ½èƒ½å¿«é€Ÿä¸Šæ‰‹ã€‚\n",
    ">\n",
    "> æ€»çš„æ¥è¯´ï¼Œæˆ‘è§‰å¾—æ¯”ä»€ä¹ˆ CSDN æˆ–è€… çŸ¥ä¹ ä¸Šé¢é›¶é›¶æ•£æ•£çš„ä»£ç åŠè§£é‡Šå¼ºå¤šäº†...\n",
    ">\n",
    "> æœ‰ä»€ä¹ˆé—®é¢˜å¯ä»¥åœ¨æˆ‘çš„ Github ä¸Šé¢æå‡ºæ¥ï¼Œåœ°å€åœ¨è¿™é‡Œï¼š[DrDavidS](https://github.com/DrDavidS/basic_Machine_Learning) ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åç»­æ”¹è¿›æ–¹å‘\n",
    "\n",
    "è¿˜æœ‰å¾ˆå¤šå·¥ä½œå¾…è¡¥å……ï¼š\n",
    "\n",
    "- Apex.fp16æ”¹å†™\n",
    "- Schedulerè®¾ç½®\n",
    "- Early-Stopè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
