{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手把手教你BERT中文文本分类-第一篇\n",
    "\n",
    "作者：你们大卫\n",
    "\n",
    "时间：2020年4月\n",
    "\n",
    "github：https://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "开源协议：[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "## 写在开头\n",
    "\n",
    "### 感言\n",
    "\n",
    "BERT 模型自2018年发布以来，它和它的衍生品几乎在NLP圈一统天下。关于BERT模型的原理我就不系统介绍了，各位自行去读论文和看博客分析。\n",
    "\n",
    "这篇Notebook的起因，是我在使用 PyTorch 和 Keras 学习和实验 BERT 系列模型（包括RoBERTa、ALBERT等）的时候，看着 CSDN、Github、知乎 等论坛上五花八门的封装代码头疼——尤其是中文任务的代码，大多数代码作者估计都是知其然而不知其所以然，用得上用不上的统统封装，又臭又长、缺失注释、结构混乱，对初学者极其不友好，看得想吐。\n",
    "\n",
    "此外，这些代码还有一些版本问题，就是基于 PyTorch 的 BERT 框架已经进化为了 [transformers](https://github.com/huggingface/transformers)，而CSDN、知乎、Github上很多项目还是基于**旧版** `pytorch-pretrained-bert` 框架，参考价值有限。\n",
    "\n",
    "新框架支持 ALBERT、RoBERTa 等新模型的调用，模型功能上也有所更新，更何况我本人在技术上爱用新不爱用旧，所以对很多基于旧版本框架的中文任务参考代码很是头疼。\n",
    "\n",
    "基于这些原因，我在进行了一段时间的实验以后，决定基于 PyTorch 和 [transformers](https://github.com/huggingface/transformers) 框架写一篇**新手友好**的 BERT 实战教程。\n",
    "\n",
    "> 注意，这篇教程可以说是 Baseline，里面缺少了很多工程实用处理，比如：\n",
    ">\n",
    ">- BERT.Config封装；\n",
    ">- 文本预处理；\n",
    ">- 半精度处理；\n",
    ">- 数据读写改进；\n",
    ">- 显存监控；\n",
    ">- TorchSnooper；\n",
    ">- 自定义损失函数；\n",
    ">- 自定义网络结构；\n",
    ">\n",
    ">计划是以后在另一篇完善，本篇任务仅仅是简洁易懂地说明 BERT 和 [transformers](https://github.com/huggingface/transformers) 框架的使用。\n",
    "\n",
    "### 基础要求\n",
    "\n",
    "- Python基础；\n",
    "- 你有一定的 PyTorch 使用经验；\n",
    "- 你对 NLP 有一些经验；\n",
    "- 你对 BERT 和 Tranformer 的理论和结构有一定的了解；\n",
    "- 你想使用 BERT 系列模型执行一些 NLP 任务，比如 NER、文本分类等。\n",
    "\n",
    "### Transformers框架简介\n",
    "\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "\n",
    "Github地址：https://github.com/huggingface/transformers\n",
    "\n",
    "文档地址：https://huggingface.co/transformers/index.html\n",
    "\n",
    "transformers 框架横跨 TF2.0 和 PyTorch ，是一个非常好用的高级语言模型框架。\n",
    "\n",
    "### 主要软件准备\n",
    "\n",
    "- **transformers**框架：\n",
    "\n",
    "    pip安装：\n",
    "\n",
    "    ```shell\n",
    "    pip install transformers\n",
    "    ```\n",
    "    \n",
    "- **PyTorch**：见 [PyTorch官网](https://pytorch.org)\n",
    "\n",
    "- **Keras**：见 [Keras官网](https://keras.io)\n",
    "\n",
    "### Pytorch-BERT中文预训练模型下载\n",
    "\n",
    "包含三个文件：\n",
    "\n",
    "|name | size \n",
    "|:------|:------\n",
    "|config.json | 1KB |\n",
    "|pytorch_model.bin | 392MB |\n",
    "|bocab.txt | 107KB |\n",
    "\n",
    ">其中 `pytorch_model.bin` 就是 `bert-base-chinese`:\n",
    ">\n",
    ">12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    ">Trained on cased Chinese Simplified and Traditional text.\n",
    ">\n",
    ">建议不要用transformers自带的命令下载，由于众所周知的原因是奇慢无比，而且容易断线。\n",
    ">\n",
    ">我传到了度盘上面，如果还嫌慢可以自己找地方下载。\n",
    ">\n",
    ">度盘下载地址：https://pan.baidu.com/s/1CCylS1nkL4ut8T3nr9cUNA 提取码：3ypf\n",
    "\n",
    "### 硬件准备\n",
    "\n",
    "- 支持CUDA运算的显卡（计算卡），最好给点力，要不然训练很久。\n",
    "\n",
    "- 显存尽量大点，不然 batch size 小了训练很憋屈。\n",
    "\n",
    "### 数据准备\n",
    "\n",
    "我采用的是 THUCNews 数据集的**子集**，由清华NLP组提供。\n",
    "\n",
    "这个数据集是针对新闻标题进行分类的数据集。可以在[这里](https://github.com/DrDavidS/Pytorch_Basic/tree/master/datasets/THUCNews)下载。文件以txt格式保存，打开以后可以看看内容。\n",
    "\n",
    "简单介绍一下数据集，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档，划分出 14 个候选分类。\n",
    "\n",
    "我们只采用了其中 10 个子类，包括\n",
    "\n",
    "```\n",
    "finance\n",
    "realty\n",
    "stocks\n",
    "education\n",
    "science\n",
    "society\n",
    "politics\n",
    "sports\n",
    "game\n",
    "entertainment\n",
    "```\n",
    "\n",
    "训练数据共 180000 条，保存在 `train.txt` 中。测试数据保存在 `dev.txt` 和 `text.txt` 中，每个文件10000条。\n",
    "\n",
    "数据格式如下：\n",
    "\n",
    "```\n",
    "《非诚勿扰》“冯女郎”车晓带妈妈闯世界(图)\t9\n",
    "美弗吉尼亚大学访华太设计签实习基地协议（组图）\t1\n",
    "华中科技大学2010年考研成绩查询开通\t3\n",
    "陈小艺“激吻照”疑似炒作\t9\n",
    "90岁老太半世纪撮合200多对新人(图)\t5\n",
    "袁立挑选钻戒被疑婚期将近 男伴酷似梁朝伟(图)\t9\n",
    "国务院：严打拐卖操控未成年人违法犯罪\t6\n",
    "郎平不惧土耳其劲旅窘境：我就喜欢接这种烂摊子\t7\n",
    "...\n",
    "```\n",
    "\n",
    "其中末尾数字代表标签类型，数字和种类对照参见 `class.txt`。数字标签和文本中间用制表符隔开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预先准备\n",
    "\n",
    "### 导入必要包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "An error occured.\n",
      "ValueError: \"@jupyter-widgets/jupyterlab-manager@2\" is not a valid npm package\n",
      "See the log file for details:  /tmp/jupyterlab-debug-tj_n2lgq.log\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences  # padding句子用\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "\n",
    "# 如果使用jupyter lab，那么打开以下插件，并且安装了node.js\n",
    "# 参考：https://stackoverflow.com/questions/57343134/jupyter-notebooks-not-displaying-progress-bars\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager@2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.6.0\n",
      "Transformers 版本： 3.0.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch 版本： {torch.__version__}\")\n",
    "print(f\"Transformers 版本： {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意我用的 PyTorch 版本是 1.6.0，用新不用旧。\n",
    "\n",
    "理论上这些代码在 PyTorch 1.3.1 以上都可以运行，如果有问题请在 Github 上向我提问。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "检查GPU状态，我之前用的是 Tesla M40，不过现在只有一块可怜的 GTX1070 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "GPU numbers:  1\n",
      "device_name:  GeForce GTX 1070\n",
      "当前设备编号：0\n"
     ]
    }
   ],
   "source": [
    "# GPUcheck\n",
    "\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU numbers: \", n_gpu)\n",
    "    print(\"device_name: \", torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda:0\")  # 注意选择\n",
    "    torch.cuda.set_device(0) \n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "print(f\"当前设备编号：{torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT相关设置\n",
    "\n",
    "为了规范和工程化，我们最好不要把一些超参数零零散散地分散在整个代码中。我们需要使用一个`Config()`类来统一管理这些参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Bert_NEWS_CLF.bin'\n",
    "        self.bert_path = './bert-chinese/'\n",
    "        self.train_file = '../datasets/THUCNews/train.txt'\n",
    "        \n",
    "        self.num_classes = 10                    # 类别数(按需修改)\n",
    "        self.hidden_size = 768                   # 隐藏层输出维度\n",
    "        self.hidden_dropout_prob = 0.1           # dropout比例\n",
    "        self.batch_size = 128                    # mini-batch大小\n",
    "        self.max_len = 32                        # 句子的最长padding长度\n",
    "        \n",
    "        self.epochs = 3                          # epoch数\n",
    "        self.learning_rate = 2e-5                # 学习率        \n",
    "\n",
    "        self.save_path = './saved_model/'        # 模型训练结果保存路径\n",
    "        \n",
    "        # self.fp16 = False\n",
    "        # self.fp16_opt_level = 'O1'\n",
    "        # self.gradient_accumulation_steps = 1\n",
    "        # self.warmup_ratio = 0.06\n",
    "        # self.warmup_steps = 0\n",
    "        # self.max_grad_norm = 1.0\n",
    "        # self.adam_epsilon = 1e-8\n",
    "        # self.class_list = class_list                              # 类别名单\n",
    "        # self.require_improvement = 1000                                # 若超过1000batch效果还没提升，则提前结束训练\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "### 读取数据\n",
    "\n",
    "放在 `./data/train.txt`中，有需要请自己改路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = config.train_file\n",
    "\n",
    "with open(file, encoding=\"utf-8\") as f:\n",
    "    sentences_and_labels = [line for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中华女子学院：本科层次仅1专业招男生\\t3\\n',\n",
       " '两天价网站背后重重迷雾：做个网站究竟要多少钱\\t4\\n',\n",
       " '东5环海棠公社230-290平2居准现房98折优惠\\t1\\n',\n",
       " '卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球\\t7\\n',\n",
       " '82岁老太为学生做饭扫地44年获授港大荣誉院士\\t5\\n',\n",
       " '记者回访地震中可乐男孩：将受邀赴美国参观\\t5\\n',\n",
       " '冯德伦徐若瑄隔空传情 默认其是女友\\t9\\n',\n",
       " '传郭晶晶欲落户香港战伦敦奥运 装修别墅当婚房\\t1\\n',\n",
       " '《赤壁OL》攻城战诸侯战硝烟又起\\t8\\n',\n",
       " '“手机钱包”亮相科博会\\t4\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前几句\n",
    "sentences_and_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据以 `table` 分割，所以用 `split('\\t')`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "东5环海棠公社230-290平2居准现房98折优惠\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq, label = sentences_and_labels[2].split('\\t')\n",
    "print(seq)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sentence_with_label in sentences_and_labels:\n",
    "    sentence, label = sentence_with_label.split('\\t')\n",
    "    sentences.append(sentence)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中华女子学院：本科层次仅1专业招男生', '两天价网站背后重重迷雾：做个网站究竟要多少钱', '东5环海棠公社230-290平2居准现房98折优惠', '卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球', '82岁老太为学生做饭扫地44年获授港大荣誉院士']\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按字拆分：\n",
    "    \n",
    "    \n",
    "    \n",
    "按照BERT的要求，我们需要使用输入为：\n",
    "```\n",
    "[CLS]<句子A>[SEP]<句子B>[SEP]\n",
    "```\n",
    "这样的形式，但是很明显我们的新闻标题是不适合这样分开的，所以我们的输入形式是：\n",
    "```\n",
    "[CLS]<句子A>[SEP]\n",
    "```\n",
    "\n",
    "BERT不需要分词，我们只要直接将他们转换为 `vocab.txt` 字典中对应的字索引即可，包括`[CLS]`和`[SEP]`。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x7fba7c1e3990>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path, \n",
    "                                          do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，`BertTokenizer.from_pretrained()` 是从本地（或者在线下载）相关的BertToken文件。\n",
    "\n",
    "我本地已经有此文件了，所以地址就存在了 `config.bert_path` 中，直接指定即可导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧版代码：\n",
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "# 新版代码：\n",
    "# text_batch = sentences\n",
    "# encoding = tokenizer(text_batch, \n",
    "#                     return_tensors='pt',  # pt 指 pytorch，tf 就是 tensorflow \n",
    "#                     padding=True,  # padding到最长的那句话\n",
    "#                     truncation=True,  # 激活并控制截断\n",
    "#                     max_length=50)\n",
    "#input_ids = encoding['input_ids']\n",
    "#input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_special_tokens=True` 指的是是否要在句首和句尾自动添加 `[CLS]` 和 `[SEP]` token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "中华女子学院：本科层次仅1专业招男生\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n"
     ]
    }
   ],
   "source": [
    "# 这句话的input_ids\n",
    "print(f\"Tokenize 前的第一句话：\\n{sentences[0]}\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n{tokenized_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上面的例子说明：\n",
    "\n",
    "首先encode包含了两个动作，\n",
    "\n",
    "**第一**，对句子\n",
    "\n",
    "    ```中华女子学院：本科层次仅1专业招男生```\n",
    "\n",
    "的前后添加标签，即：\n",
    "\n",
    "    ```[CLS]中华女子学院：本科层次仅1专业招男生[SEP]```\n",
    "\n",
    "**第二**，将添加标签后的句子按字符（标点符号也单独算一个字符）分开，然后转换为 `input_ids`：\n",
    "\n",
    "    ```[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]```\n",
    "\n",
    "> 需要说明的是，其中 `101` 是 `[CLS]` 的索引，`102` 是 `[SEP]` 的索引。\n",
    "\n",
    "上述过程称为 `tokenized`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "print (len(tokenized_texts))  # 180000句话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "为了保证输入长度的统一，我们需要对句子进行padding。\n",
    "\n",
    "本例中采用的是新闻标题，所以标题不会太长，我们限定为 `32` 个字符，此参数在 `config.max_len` 中。\n",
    "\n",
    "一旦标题长度超过32个字符，则会截断超过部分不用；如果不足 `32` 个字符，则执行 `pad_sequences` （即`padding`）操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=config.max_len, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "\n",
      "中华女子学院：本科层次仅1专业招男生\n",
      "\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "\n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n",
      "\n",
      "\n",
      "Padding 后的第一句话： \n",
      "\n",
      "[ 101  704 1290 1957 2094 2110 7368 8038 3315 4906 2231 3613  788  122\n",
      "  683  689 2875 4511 4495  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize 前的第一句话：\\n\\n{sentences[0]}\\n\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n\\n{tokenized_texts[0]}\\n\\n\")\n",
    "print(f\"Padding 后的第一句话： \\n\\n{input_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实 `padding` 之后还可转换回来，\n",
    "\n",
    "很容易看出每个字，包括`[PAD]`、`[CLS]`、`[SEP]`所在的位置:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 中 华 女 子 学 院 ： 本 科 层 次 仅 1 专 业 招 男 生 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 转换回来\n",
    "raw_texts = [tokenizer.decode(input_ids[0])]\n",
    "print(raw_texts)\n",
    "print(len(raw_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT的输入准备\n",
    "\n",
    "### 注意力mask（attention masks）：\n",
    "\n",
    "BERT 模型的核心是 Transformer 结构，其中很重要的一点就是 self-attention 结构。\n",
    "\n",
    "BERT-Chinese 模型同 BERT-base 模型结构一致，每层有12个自注意头，为了不让这些 self-attention 结构注意到补全的`[PAD]`部分，我们需要输入一个 attention_masks 标签，告诉模型哪些内容是真实内容，哪些是无意义的`[PAD]`。\n",
    "\n",
    "刚刚说到被 `padding` 部分是不需要被 attention 到的。相当于这部分在  attention_masks 中的标签就是真实句子为 1 ，padding 部分为 0 。所以我们得到 attention masks："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 第一句话的 attention_masks\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  准备Labels\n",
    "\n",
    "首先准备Labels。这些标题的 Labels 在一开始就已经分离开来，保存在了 `labels` 里面\n",
    " \n",
    "这里可以用 `train_test_split` 来分。注意，由于多了一个 `attention_masks` 所以我们需要用两次 `train_test_split`，并且采用相同的随机种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n', '5\\n', '9\\n', '1\\n', '8\\n', '4\\n']\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于现在的labels里面并不是数字，而且有换行符`\\n`，我们需要一些处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 1, 7, 5, 5, 9, 1, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "clean_labels = []\n",
    "for label in labels:\n",
    "    clean_labels.append(int(label.strip('\\n')))\n",
    "\n",
    "print(clean_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, clean_labels, \n",
    "                                                            random_state=2019, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2019, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 4, 3, 8, 8, 1, 8, 7, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      标签总数： 180000\n",
      "训练集标签总数： 162000\n",
      "验证集标签总数： 18000\n"
     ]
    }
   ],
   "source": [
    "print(f\"      标签总数：\", len(labels))\n",
    "print(f\"训练集标签总数：\", len(train_labels))\n",
    "print(f\"验证集标签总数：\", len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和 TF2.0 可以直接接受`ndarray`不同，PyTorch要求我们先将数据转化为 `Tensor` 形式再输入模型。\n",
    "\n",
    "现所以我们准备 Tensor 化。\n",
    "\n",
    "> 由于直接使用 `torch.tensor()` 会弹出警告，所以我们使用更安全的代码。\n",
    "\n",
    "**旧**：\n",
    "```python\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "```\n",
    "\n",
    "**新**:\n",
    "```python\n",
    "train_inputs = torch.tensor(train_inputs).clone().detach()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "train_inputs = torch.tensor(train_inputs).clone().detach()\n",
    "validation_inputs = torch.tensor(validation_inputs).clone().detach()\n",
    "train_labels = torch.tensor(train_labels).clone().detach()\n",
    "validation_labels = torch.tensor(validation_labels).clone().detach()\n",
    "train_masks = torch.tensor(train_masks).clone().detach()\n",
    "validation_masks = torch.tensor(validation_masks).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_inputs))\n",
    "print(len(validation_labels))\n",
    "print(len(validation_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建迭代器\n",
    "\n",
    "我们采用\n",
    "\n",
    "`torch.utils.data.TensorDataset` 将他们封装为 `TensorDataset` 的形式，\n",
    "\n",
    "`torch.utils.data.RandomSampler` 采用随机采样的方法从中采样，\n",
    "\n",
    "`torch.utils.data.DataLoader` 自动形成迭代器。\n",
    "\n",
    "> 注意 batch_size 的设置大小和显存大小密切相关！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "print(config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成训练数据集\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# 随机采样\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# 读取数据\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=config.batch_size)\n",
    "\n",
    "\n",
    "# 形成验证数据集\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# 随机采样\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# 读取数据\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT的微调\n",
    "\n",
    "在准备好输入以后，现在我们开始微调BERT模型。\n",
    "\n",
    "使用 `BertForSequenceClassification`，它就是一个普通BERT模型，在最后面加了一个线形层用于分类。\n",
    "\n",
    "### 导入模型\n",
    "\n",
    "直接使用 `from_pretrained` 导入预训练好的中文 BERT 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 统计标签种类\n",
    "label_count = len(set(labels))\n",
    "print(label_count)\n",
    "\n",
    "config.num_classes = label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-chinese/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-chinese/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 BertForSequenceClassification 模型，\n",
    "# 是一个预训练的BERT模型，在最后面加了一个线形层用于分类。\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(config.bert_path, \n",
    "                                                      num_labels=config.num_classes)\n",
    "model.cuda()\n",
    "\n",
    "# 注意：\n",
    "# 在新版的 Transformers 中会给出警告\n",
    "# 原因是我们导入的预训练参数权重是不包含模型最终的线性层权重的\n",
    "# 不过我们本来就是要“微调”它，所以这个情况是符合期望的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在完成 `model.cuda()` 这一步以后，模型就放在显存中了。同时也打印出了模型的具体结构。\n",
    "\n",
    "> 需要说明的是，为了方便起见，我们直接采用了 `BertForSequenceClassification.from_pretrained()`，实际上如果有更灵活的需求时，应该使用 `BertModel.from_pretrained()` 搭建模型。两者的区别可以参考源代码。\n",
    "\n",
    "例如，接下来给出一段魔改模型参考，我们采用了 `BERT` 输出后拼接其他特征再过一个线性层：\n",
    "\n",
    "```python\n",
    "class ModelBert(nn.Module):\n",
    "    \"\"\"\n",
    "    新增 行业ID 特征拼接。\n",
    "    \n",
    "    我们想要对商品名称进行分类，商品名称是文本形式，会进入BERT模型。\n",
    "    行业ID，即代码中的“HYID”本身就是数字形式，没必要放入BERT模型中，于是我们将 BERT 输出后的 768 维向量拼接\n",
    "    tensor(HYID)，也就是变成了 769 维，再过一个 线形层 + softmax 输出分类结果。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(ModelBert, self).__init__()\n",
    "        self.num_labels = config.num_classes\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size + 1, config.num_classes)  # 这里维度 +1 放入HYID\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        HYID=None):\n",
    "        \n",
    "        outputs = self.bert(\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds,)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        HYID_f = HYID.float()\n",
    "        pooled_output = torch.cat((pooled_output, HYID_f), dim=1)  # 拼接在这里\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs        \n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) \n",
    "    \n",
    "    model = ModelBert(config)\n",
    "    model.cuda()\n",
    "```\n",
    "\n",
    "### 准备微调\n",
    "\n",
    "众所周知，我们所使用的 BERT 是一个预训练好的模型，我们需要根据不同的任务，比如这里的 文本分类，或者 NER 之类的任务，使用本地数据进行微调。\n",
    "\n",
    "首先我们设置好相关的优化器和评价函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 权重衰减\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，`no_decay`见[issue#492](https://github.com/huggingface/transformers/issues/492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率计算函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练 Epochs： 3\n"
     ]
    }
   ],
   "source": [
    "# 保存loss\n",
    "train_loss_set = []\n",
    "# epochs \n",
    "print(f\"训练 Epochs： {config.epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始微调\n",
    "\n",
    "3个epoch："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前epoch： 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cd23ba68d044f5ab42abc9ad89bd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前 epoch 的 Train loss: 0.29743796679386975\n",
      "当前epoch： 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47efde0a4629427d8e24b21b3bee70c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前 epoch 的 Train loss: 0.1526083102741089\n",
      "当前epoch： 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d2388ebec544ef8465f0668d9e2c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前 epoch 的 Train loss: 0.10776557406413517\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in range(config.epochs): \n",
    "    ## 训练\n",
    "    print(f\"当前epoch： {_}\")\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # 把batch放入GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # 解包batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播loss计算\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  # 有labels的时候，且labels>1就直接返回Cross-Entropy\n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"当前 epoch 的 Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量验证\n",
    "\n",
    "我们之前已经使用 `train_test_split()` 为交叉验证做好了准备。因此可以直接进入验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证状态\n",
    "model.eval()\n",
    "\n",
    "# 建立变量\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdf6d12a4ea451aac64966aff6ce17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.9465093085106382\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        logits = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()  # 注意这里的logits是在softmax之前，所以和不为1\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    # print(logits, label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)  # 计算准确率\n",
    "    eval_accuracy += tmp_eval_accuracy  # 准确率积累\n",
    "    nb_eval_steps += 1  # 步数积累\n",
    "print(f\"Validation Accuracy: {eval_accuracy/nb_eval_steps}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单条预测\n",
    "\n",
    "为了详细说明我们微调后的 BERT 预测过程，我们使用单条预测的方式展示一下相关结果。\n",
    "\n",
    "有这样一条新闻标题：\n",
    "\n",
    "```词汇阅读是关键 08年考研暑期英语复习全指南```\n",
    "\n",
    "其类别为\n",
    "\n",
    "```3. education```\n",
    "\n",
    "现在我们采用单条预测的形式看看模型对它的预测。\n",
    "\n",
    "流程和之前一样，也是先 tokenize 再做 padding 和 mask。具体如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = \"词汇阅读是关键 08年考研暑期英语复习全指南\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_texts = [tokenizer.encode(test_sent, add_special_tokens=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = pad_sequences([txt for txt in test_tokenized_texts],\n",
    "                          maxlen=config.max_len, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "词汇阅读是关键 08年考研暑期英语复习全指南\n",
      "\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "[[101, 6404, 3726, 7325, 6438, 3221, 1068, 7241, 8142, 2399, 5440, 4777, 3264, 3309, 5739, 6427, 1908, 739, 1059, 2900, 1298, 102]]\n",
      "\n",
      "\n",
      "Padding 后的第一句话： \n",
      "[[ 101 6404 3726 7325 6438 3221 1068 7241 8142 2399 5440 4777 3264 3309\n",
      "  5739 6427 1908  739 1059 2900 1298  102    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize 前的第一句话：\\n{test_sent}\\n\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n{test_tokenized_texts}\\n\\n\")\n",
    "print(f\"Padding 后的第一句话： \\n{test_input_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "test_inputs = torch.tensor(test_input_ids).clone().detach()\n",
    "test_masks = torch.tensor(test_attention_masks).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成验证数据集\n",
    "# 为了通用，这里还是用了 DataLoader 的形式\n",
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "# 随机采样\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# 读取数据\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f838970b54114e1ca7ab7b7a032ea11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # 预测\n",
    "    outputs = model(input_ids=b_input_ids, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    token_type_ids=None,\n",
    "                    position_ids=None)\n",
    "                       \n",
    "    # Move logits and labels to CPU\n",
    "    scores = outputs[0].detach().cpu().numpy()  # 每个字的标签的概率\n",
    "    pred_flat = np.argmax(scores, axis=1).flatten()\n",
    "    # label_ids = b_labels.to('cpu').numpy()  # 真实labels\n",
    "    print(pred_flat)  # 预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，预测是正确的。这样的单句预测常常用在在需要演示的情况下，这时候直接替换 `test_sent` 就行了。\n",
    "\n",
    "如果为了方便起见，还可以打包为一个函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和读取模型\n",
    "\n",
    "模型训练完毕后，我们需要对模型进行保存。 PyTorch 的保存和 Transformers 是不太一样的，这里需要**特别说明**。\n",
    "\n",
    "### 基于 PyTorch 保存 / 读取\n",
    "\n",
    "一般我们采用 PyTorch 自带的模型保存方法。此方法可以保存读取各种网络结构的模型。\n",
    "\n",
    "使用 `torch.save()` 方法将模型参数保存为 `state_dict`，读取时候也是相同的道理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹不存在，创建文件夹\n"
     ]
    }
   ],
   "source": [
    "# 创建文件夹\n",
    "if not os.path.exists(config.save_path):\n",
    "    os.makedirs(config.save_path)\n",
    "    print(\"文件夹不存在，创建文件夹!\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model_to_save = model.module if hasattr(model, 'module') else model                          # 用于多卡训练的情况\n",
    "torch.save(model_to_save.state_dict(), os.path.join(config.save_path, config.model_name))    # 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存完毕后我们读取模型。\n",
    "\n",
    "> 注意，这里我分别读取了 **模型结构** 和 **Tokenizer**，两者路径要按实际调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取模型\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path)\n",
    "model = BertForSequenceClassification.from_pretrained(config.bert_path, num_labels=config.num_classes)\n",
    "model.load_state_dict(torch.load(os.path.join(config.save_path, config.model_name)))\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的，如果我要读取魔改的模型，比如前面提到的 `ModelBert`，一样可采用 `model.load_state_dict()` 来读取，但是先要定义好模型结构才行。\n",
    "\n",
    "示例如下，假设已经搭建好了模型结构了，现在需要读取之前训练好且保存好的模型参数：\n",
    "\n",
    "```python\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path)\n",
    "model = ModelBert(config)\n",
    "model.load_state_dict(torch.load(os.path.join(config.save_path, config.model_name)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 Transformers 保存 / 读取（不推荐）\n",
    "\n",
    "如果直接采用的是 Transformers 自带的模型结构，比如上文的 `BertForSequenceClassification`，Transformers 自带了一种 `save_pretrained` 保存方法，可以方便地将模型保存为二进制文件。\n",
    "\n",
    "> 为什么不推荐这个方法呢？\n",
    ">\n",
    "> 因为如果你需要以 BERT 为核心，对模型进行魔改的话，这个功能很可能由于模型结构的不同而报错或者导致效果一塌糊涂。\n",
    ">\n",
    "> 魔改模型参考上文中的 `ModelBert` 部分，换句话说，模型结构由自己定义，已经和 Transformers 自带的结构不一致了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建文件夹\n",
    "if not os.path.exists(config.save_path):\n",
    "    os.makedirs(config.save_path)\n",
    "    print(\"文件夹不存在，创建文件夹!\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# 保存\n",
    "model_to_save2 = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save2.save_pretrained(config.save_path)\n",
    "tokenizer.save_pretrained(config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "model = BertModel.from_pretrained(config.save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(config.save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "\n",
    "当前我采用的卡是 Tesla M40，单卡24GB显存。\n",
    "\n",
    "总共跑了3个 epoch，batch_size 是 512。\n",
    "\n",
    "`tqdm_notebook` 的输出时间有点迷，这里就不记录了。之前用 `tqdm` 的时候大概一个 epoch 在 23 分钟左右。\n",
    "\n",
    "测试集仅仅是前向传播，总共18000条数据只花了不到一分钟就完成了。\n",
    "\n",
    "最后准确率94.4%。\n",
    "\n",
    "实际上这个准确率有很大的提升空间，包括调参，包括清洗什么的都没有做（实际上数据集很干净了也没必要做），直接就是BERT梭哈。\n",
    "\n",
    "> 总体来说，我写的这个教程还是很详细很友好的，而且没有使用老旧的 `pytorch-pretrained-bert`，而是当前最新版本的 `transformers`。\n",
    ">\n",
    "> 其次，每一步几乎都有比较详细的中间过程解释，我相信稍微熟悉 PyTorch 和 BERT 基本结构的人都能快速上手。\n",
    ">\n",
    "> 总的来说，我觉得比什么 CSDN 或者 知乎 上面零零散散的代码及解释强多了...\n",
    ">\n",
    "> 有什么问题可以在我的 Github 上面提出来，地址在这里：[DrDavidS](https://github.com/DrDavidS/basic_Machine_Learning) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续改进方向\n",
    "\n",
    "还有很多工作待补充：\n",
    "\n",
    "- Apex.fp16改写\n",
    "- Scheduler设置\n",
    "- Early-Stop设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
