{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences  # padding\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"dh_msra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "GPU numbers:  2\n",
      "device_name:  Tesla M40 24GB\n",
      "Current device: 1\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPU numbers: \", n_gpu)\n",
    "print(\"device_name: \", torch.cuda.get_device_name(0))\n",
    "torch.cuda.set_device(1)\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_separate = []\n",
    "all_letter_labels = []\n",
    "label_set = set()\n",
    "with open(file, encoding=\"utf-8\") as f:\n",
    "    single_sentence = []\n",
    "    single_sentence_labels = []\n",
    "    for s in f.readlines():\n",
    "        if s != \"\\n\":\n",
    "            word, label = s.split(\"\\t\")\n",
    "            label = label.strip(\"\\n\")\n",
    "            single_sentence.append(word)\n",
    "            single_sentence_labels.append(label)\n",
    "            label_set.add(label)\n",
    "        elif s == \"\\n\":\n",
    "            all_sentences_separate.append(single_sentence)\n",
    "            all_letter_labels.append(single_sentence_labels)\n",
    "            single_sentence = []\n",
    "            single_sentence_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！'], ['藏', '书', '本', '来', '就', '是', '所', '有', '传', '统', '收', '藏', '门', '类', '中', '的', '第', '一', '大', '户', '，', '只', '是', '我', '们', '结', '束', '温', '饱', '的', '时', '间', '太', '短', '而', '已', '。']]\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
      "\n",
      "所有的标签：{'I-LOC', 'B-ORG', 'I-PER', 'I-ORG', 'O', 'B-PER', 'B-LOC'}\n"
     ]
    }
   ],
   "source": [
    "print(all_sentences_separate[0:2])\n",
    "print(all_letter_labels[0:2])\n",
    "print(f\"\\n所有的标签：{label_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 tag 到 索引 的字典\n",
    "tag_to_ix = {\"B-LOC\": 0,\n",
    "             \"I-LOC\": 1, \n",
    "             \"B-ORG\": 2, \n",
    "             \"I-ORG\": 3,\n",
    "             \"B-PER\": 4,\n",
    "             \"I-PER\": 5,\n",
    "             \"O\": 6,\n",
    "             \"[CLS]\":7,\n",
    "             \"[SEP]\":8,\n",
    "             \"[PAD]\":9}\n",
    "\n",
    "ix_to_tag = {0:\"B-LOC\", \n",
    "             1:\"I-LOC\", \n",
    "             2:\"B-ORG\", \n",
    "             3:\"I-ORG\",\n",
    "             4:\"B-PER\",\n",
    "             5:\"I-PER\",\n",
    "             6:\"O\",\n",
    "             7:\"[CLS]\",\n",
    "             8:\"[SEP]\",\n",
    "             9:\"[PAD]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当希望工程救助的百万儿童成长起来，科教兴国蔚然成风时，今天有收藏价值的书你没买，明日就叫你悔不当初！', '藏书本来就是所有传统收藏门类中的第一大户，只是我们结束温饱的时间太短而已。']\n"
     ]
    }
   ],
   "source": [
    "all_sentences = []  # 句子\n",
    "\n",
    "for one_sentence in all_sentences_separate:\n",
    "    sentence = \"\".join(one_sentence)\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "print(all_sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]]\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "all_labels = []  # labels\n",
    "for letter_labels in all_letter_labels:\n",
    "    labels = [tag_to_ix[t] for t in letter_labels]\n",
    "    all_labels.append(labels)\n",
    "\n",
    "print(all_labels[0:2])\n",
    "print(len(all_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55289\n"
     ]
    }
   ],
   "source": [
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2token\n",
    "tokenizer = BertTokenizer.from_pretrained('./bert-chinese/', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636, 674, 1036, 4997, 2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768, 7599, 3198, 8024, 791, 1921, 3300, 3119, 5966, 817, 966, 4638, 741, 872, 3766, 743, 8024, 3209, 3189, 2218, 1373, 872, 2637, 679, 2496, 1159, 8013, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子padding\n",
    "# 句子最长长度\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[ 101 2496 2361 3307 2339 4923 3131 1221 4638 4636  674 1036 4997 2768\n",
      " 7270 6629 3341 8024 4906 3136 1069 1744 5917 4197 2768 7599 3198 8024\n",
      "  791 1921 3300 3119 5966  817  966 4638  741  872 3766  743 8024 3209\n",
      " 3189 2218 1373  872 2637  679 2496 1159 8013  102    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids[0]))\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 代表 O 实体\n",
    "for label in all_labels:\n",
    "    label.insert(len(label), 8)  # [SEP]\n",
    "    label.insert(0, 7) # [CLS]\n",
    "    if MAX_LEN > len(label) -1:\n",
    "        for i in range(MAX_LEN - len(label)):\n",
    "            label.append(9)  # [PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_labels[0]))\n",
    "print(all_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# 第一句话的 attention_masks\n",
    "print(np.array(attention_masks[0]))\n",
    "print(len(np.array(attention_masks[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, \n",
    "                                                                                    all_labels, \n",
    "                                                                                    random_state=2019, \n",
    "                                                                                    test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2019, \n",
    "                                                       test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49760\n",
      "5529\n",
      "[ 101 3616 3828 6598 3315 2356 1767 4638 2600  817  966 8024 1315 2828\n",
      " 3616 4673 8115 1744 2792 3300 4638 5500 4873  510 6395 1171 1469 7213\n",
      " 6121 2100 3621 1217 1762  671 6629 8024 2347 5307 6631 6814 5401 1744\n",
      "  511  102    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[ 101 3655 4904  510 7029 3941 1469 2166 3360 3918 3884 8024 3354 2768\n",
      "  749 4263 3173 6230 5384  185 6825 5307  704 1744 4514 4638  712 6206\n",
      " 7579 3332  511  102    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_inputs))\n",
    "print(len(validation_inputs))\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(validation_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# 形成训练数据集\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# 随机采样\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# 读取数据\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# 形成验证数据集\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# 随机采样\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# 读取数据\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"./bert-chinese/\", num_labels=10)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 权重衰减\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存loss\n",
    "train_loss_set = []\n",
    "# epochs \n",
    "epochs = 5\n",
    "# epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前epoch： 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [17:01<1:08:06, 1021.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.08692683487086722\n",
      "当前epoch： 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [34:03<51:05, 1021.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.028457486720403758\n",
      "当前epoch： 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [51:05<34:03, 1021.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.018437476925657233\n",
      "当前epoch： 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [1:08:07<17:01, 1021.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.014582799714816385\n",
      "当前epoch： 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [1:25:09<00:00, 1021.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.011921173068431413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in range(epochs): \n",
    "    ## 训练\n",
    "    print(f\"当前epoch： {_}\")\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # 把batch放入GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # 解包batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播loss计算\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  \n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"当前 epoch 的 Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证状态\n",
    "model.eval()\n",
    "\n",
    "# 建立变量\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/346 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/346 [00:00<00:43,  7.85it/s]\u001b[A\n",
      "  1%|          | 2/346 [00:00<00:44,  7.81it/s]\u001b[A\n",
      "  1%|          | 3/346 [00:00<00:42,  7.99it/s]\u001b[A\n",
      "  1%|          | 4/346 [00:00<00:41,  8.18it/s]\u001b[A\n",
      "  1%|▏         | 5/346 [00:00<00:40,  8.33it/s]\u001b[A\n",
      "  2%|▏         | 6/346 [00:00<00:40,  8.45it/s]\u001b[A\n",
      "  2%|▏         | 7/346 [00:00<00:39,  8.53it/s]\u001b[A\n",
      "  2%|▏         | 8/346 [00:00<00:39,  8.58it/s]\u001b[A\n",
      "  3%|▎         | 9/346 [00:01<00:39,  8.60it/s]\u001b[A\n",
      "  3%|▎         | 10/346 [00:01<00:38,  8.63it/s]\u001b[A\n",
      "  3%|▎         | 11/346 [00:01<00:38,  8.65it/s]\u001b[A\n",
      "  3%|▎         | 12/346 [00:01<00:38,  8.67it/s]\u001b[A\n",
      "  4%|▍         | 13/346 [00:01<00:38,  8.69it/s]\u001b[A\n",
      "  4%|▍         | 14/346 [00:01<00:38,  8.70it/s]\u001b[A\n",
      "  4%|▍         | 15/346 [00:01<00:38,  8.71it/s]\u001b[A\n",
      "  5%|▍         | 16/346 [00:01<00:37,  8.71it/s]\u001b[A\n",
      "  5%|▍         | 17/346 [00:01<00:37,  8.71it/s]\u001b[A\n",
      "  5%|▌         | 18/346 [00:02<00:37,  8.71it/s]\u001b[A\n",
      "  5%|▌         | 19/346 [00:02<00:37,  8.71it/s]\u001b[A\n",
      "  6%|▌         | 20/346 [00:02<00:37,  8.71it/s]\u001b[A\n",
      "  6%|▌         | 21/346 [00:02<00:37,  8.71it/s]\u001b[A\n",
      "  6%|▋         | 22/346 [00:02<00:37,  8.71it/s]\u001b[A\n",
      "  7%|▋         | 23/346 [00:02<00:37,  8.71it/s]\u001b[A\n",
      "  7%|▋         | 24/346 [00:02<00:36,  8.71it/s]\u001b[A\n",
      "  7%|▋         | 25/346 [00:02<00:36,  8.71it/s]\u001b[A\n",
      "  8%|▊         | 26/346 [00:03<00:36,  8.71it/s]\u001b[A\n",
      "  8%|▊         | 27/346 [00:03<00:36,  8.71it/s]\u001b[A\n",
      "  8%|▊         | 28/346 [00:03<00:36,  8.71it/s]\u001b[A\n",
      "  8%|▊         | 29/346 [00:03<00:36,  8.70it/s]\u001b[A\n",
      "  9%|▊         | 30/346 [00:03<00:36,  8.71it/s]\u001b[A\n",
      "  9%|▉         | 31/346 [00:03<00:36,  8.71it/s]\u001b[A\n",
      "  9%|▉         | 32/346 [00:03<00:36,  8.72it/s]\u001b[A\n",
      " 10%|▉         | 33/346 [00:03<00:35,  8.72it/s]\u001b[A\n",
      " 10%|▉         | 34/346 [00:03<00:35,  8.72it/s]\u001b[A\n",
      " 10%|█         | 35/346 [00:04<00:35,  8.71it/s]\u001b[A\n",
      " 10%|█         | 36/346 [00:04<00:35,  8.71it/s]\u001b[A\n",
      " 11%|█         | 37/346 [00:04<00:35,  8.72it/s]\u001b[A\n",
      " 11%|█         | 38/346 [00:04<00:35,  8.72it/s]\u001b[A\n",
      " 11%|█▏        | 39/346 [00:04<00:35,  8.71it/s]\u001b[A\n",
      " 12%|█▏        | 40/346 [00:04<00:35,  8.71it/s]\u001b[A\n",
      " 12%|█▏        | 41/346 [00:04<00:35,  8.71it/s]\u001b[A\n",
      " 12%|█▏        | 42/346 [00:04<00:34,  8.71it/s]\u001b[A\n",
      " 12%|█▏        | 43/346 [00:04<00:34,  8.71it/s]\u001b[A\n",
      " 13%|█▎        | 44/346 [00:05<00:34,  8.71it/s]\u001b[A\n",
      " 13%|█▎        | 45/346 [00:05<00:34,  8.71it/s]\u001b[A\n",
      " 13%|█▎        | 46/346 [00:05<00:34,  8.71it/s]\u001b[A\n",
      " 14%|█▎        | 47/346 [00:05<00:34,  8.72it/s]\u001b[A\n",
      " 14%|█▍        | 48/346 [00:05<00:34,  8.71it/s]\u001b[A\n",
      " 14%|█▍        | 49/346 [00:05<00:34,  8.71it/s]\u001b[A\n",
      " 14%|█▍        | 50/346 [00:05<00:33,  8.71it/s]\u001b[A\n",
      " 15%|█▍        | 51/346 [00:05<00:33,  8.71it/s]\u001b[A\n",
      " 15%|█▌        | 52/346 [00:06<00:33,  8.71it/s]\u001b[A\n",
      " 15%|█▌        | 53/346 [00:06<00:33,  8.70it/s]\u001b[A\n",
      " 16%|█▌        | 54/346 [00:06<00:33,  8.70it/s]\u001b[A\n",
      " 16%|█▌        | 55/346 [00:06<00:33,  8.70it/s]\u001b[A\n",
      " 16%|█▌        | 56/346 [00:06<00:33,  8.71it/s]\u001b[A\n",
      " 16%|█▋        | 57/346 [00:06<00:33,  8.71it/s]\u001b[A\n",
      " 17%|█▋        | 58/346 [00:06<00:33,  8.71it/s]\u001b[A\n",
      " 17%|█▋        | 59/346 [00:06<00:32,  8.71it/s]\u001b[A\n",
      " 17%|█▋        | 60/346 [00:06<00:32,  8.71it/s]\u001b[A\n",
      " 18%|█▊        | 61/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 18%|█▊        | 62/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 18%|█▊        | 63/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 18%|█▊        | 64/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 19%|█▉        | 65/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 19%|█▉        | 66/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 19%|█▉        | 67/346 [00:07<00:32,  8.71it/s]\u001b[A\n",
      " 20%|█▉        | 68/346 [00:07<00:31,  8.71it/s]\u001b[A\n",
      " 20%|█▉        | 69/346 [00:07<00:31,  8.71it/s]\u001b[A\n",
      " 20%|██        | 70/346 [00:08<00:31,  8.71it/s]\u001b[A\n",
      " 21%|██        | 71/346 [00:08<00:31,  8.71it/s]\u001b[A\n",
      " 21%|██        | 72/346 [00:08<00:31,  8.71it/s]\u001b[A\n",
      " 21%|██        | 73/346 [00:08<00:31,  8.71it/s]\u001b[A\n",
      " 21%|██▏       | 74/346 [00:08<00:31,  8.72it/s]\u001b[A\n",
      " 22%|██▏       | 75/346 [00:08<00:31,  8.72it/s]\u001b[A\n",
      " 22%|██▏       | 76/346 [00:08<00:31,  8.64it/s]\u001b[A\n",
      " 22%|██▏       | 77/346 [00:08<00:31,  8.65it/s]\u001b[A\n",
      " 23%|██▎       | 78/346 [00:08<00:30,  8.67it/s]\u001b[A\n",
      " 23%|██▎       | 79/346 [00:09<00:30,  8.68it/s]\u001b[A\n",
      " 23%|██▎       | 80/346 [00:09<00:30,  8.68it/s]\u001b[A\n",
      " 23%|██▎       | 81/346 [00:09<00:30,  8.69it/s]\u001b[A\n",
      " 24%|██▎       | 82/346 [00:09<00:30,  8.69it/s]\u001b[A\n",
      " 24%|██▍       | 83/346 [00:09<00:30,  8.70it/s]\u001b[A\n",
      " 24%|██▍       | 84/346 [00:09<00:30,  8.70it/s]\u001b[A\n",
      " 25%|██▍       | 85/346 [00:09<00:29,  8.71it/s]\u001b[A\n",
      " 25%|██▍       | 86/346 [00:09<00:29,  8.71it/s]\u001b[A\n",
      " 25%|██▌       | 87/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 25%|██▌       | 88/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 26%|██▌       | 89/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 26%|██▌       | 90/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 26%|██▋       | 91/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 27%|██▋       | 92/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 27%|██▋       | 93/346 [00:10<00:29,  8.71it/s]\u001b[A\n",
      " 27%|██▋       | 94/346 [00:10<00:28,  8.71it/s]\u001b[A\n",
      " 27%|██▋       | 95/346 [00:10<00:28,  8.72it/s]\u001b[A\n",
      " 28%|██▊       | 96/346 [00:11<00:28,  8.71it/s]\u001b[A\n",
      " 28%|██▊       | 97/346 [00:11<00:28,  8.72it/s]\u001b[A\n",
      " 28%|██▊       | 98/346 [00:11<00:28,  8.72it/s]\u001b[A\n",
      " 29%|██▊       | 99/346 [00:11<00:28,  8.72it/s]\u001b[A\n",
      " 29%|██▉       | 100/346 [00:11<00:28,  8.72it/s]\u001b[A\n",
      " 29%|██▉       | 101/346 [00:11<00:28,  8.72it/s]\u001b[A\n",
      " 29%|██▉       | 102/346 [00:11<00:27,  8.72it/s]\u001b[A\n",
      " 30%|██▉       | 103/346 [00:11<00:27,  8.72it/s]\u001b[A\n",
      " 30%|███       | 104/346 [00:11<00:27,  8.72it/s]\u001b[A\n",
      " 30%|███       | 105/346 [00:12<00:27,  8.71it/s]\u001b[A\n",
      " 31%|███       | 106/346 [00:12<00:27,  8.70it/s]\u001b[A\n",
      " 31%|███       | 107/346 [00:12<00:27,  8.69it/s]\u001b[A\n",
      " 31%|███       | 108/346 [00:12<00:27,  8.70it/s]\u001b[A\n",
      " 32%|███▏      | 109/346 [00:12<00:27,  8.70it/s]\u001b[A\n",
      " 32%|███▏      | 110/346 [00:12<00:27,  8.71it/s]\u001b[A\n",
      " 32%|███▏      | 111/346 [00:12<00:26,  8.71it/s]\u001b[A\n",
      " 32%|███▏      | 112/346 [00:12<00:26,  8.71it/s]\u001b[A\n",
      " 33%|███▎      | 113/346 [00:13<00:26,  8.71it/s]\u001b[A\n",
      " 33%|███▎      | 114/346 [00:13<00:26,  8.70it/s]\u001b[A\n",
      " 33%|███▎      | 115/346 [00:13<00:26,  8.71it/s]\u001b[A\n",
      " 34%|███▎      | 116/346 [00:13<00:26,  8.71it/s]\u001b[A\n",
      " 34%|███▍      | 117/346 [00:13<00:26,  8.71it/s]\u001b[A\n",
      " 34%|███▍      | 118/346 [00:13<00:26,  8.71it/s]\u001b[A\n",
      " 34%|███▍      | 119/346 [00:13<00:26,  8.71it/s]\u001b[A\n",
      " 35%|███▍      | 120/346 [00:13<00:25,  8.71it/s]\u001b[A\n",
      " 35%|███▍      | 121/346 [00:13<00:25,  8.71it/s]\u001b[A\n",
      " 35%|███▌      | 122/346 [00:14<00:25,  8.71it/s]\u001b[A\n",
      " 36%|███▌      | 123/346 [00:14<00:25,  8.69it/s]\u001b[A\n",
      " 36%|███▌      | 124/346 [00:14<00:25,  8.69it/s]\u001b[A\n",
      " 36%|███▌      | 125/346 [00:14<00:25,  8.70it/s]\u001b[A\n",
      " 36%|███▋      | 126/346 [00:14<00:25,  8.70it/s]\u001b[A\n",
      " 37%|███▋      | 127/346 [00:14<00:25,  8.71it/s]\u001b[A\n",
      " 37%|███▋      | 128/346 [00:14<00:25,  8.71it/s]\u001b[A\n",
      " 37%|███▋      | 129/346 [00:14<00:24,  8.71it/s]\u001b[A\n",
      " 38%|███▊      | 130/346 [00:14<00:24,  8.71it/s]\u001b[A\n",
      " 38%|███▊      | 131/346 [00:15<00:24,  8.71it/s]\u001b[A\n",
      " 38%|███▊      | 132/346 [00:15<00:24,  8.70it/s]\u001b[A\n",
      " 38%|███▊      | 133/346 [00:15<00:24,  8.70it/s]\u001b[A\n",
      " 39%|███▊      | 134/346 [00:15<00:24,  8.70it/s]\u001b[A\n",
      " 39%|███▉      | 135/346 [00:15<00:24,  8.70it/s]\u001b[A\n",
      " 39%|███▉      | 136/346 [00:15<00:24,  8.70it/s]\u001b[A\n",
      " 40%|███▉      | 137/346 [00:15<00:24,  8.71it/s]\u001b[A\n",
      " 40%|███▉      | 138/346 [00:15<00:23,  8.71it/s]\u001b[A\n",
      " 40%|████      | 139/346 [00:15<00:23,  8.71it/s]\u001b[A\n",
      " 40%|████      | 140/346 [00:16<00:23,  8.71it/s]\u001b[A\n",
      " 41%|████      | 141/346 [00:16<00:23,  8.71it/s]\u001b[A\n",
      " 41%|████      | 142/346 [00:16<00:23,  8.71it/s]\u001b[A\n",
      " 41%|████▏     | 143/346 [00:16<00:23,  8.71it/s]\u001b[A\n",
      " 42%|████▏     | 144/346 [00:16<00:23,  8.71it/s]\u001b[A\n",
      " 42%|████▏     | 145/346 [00:16<00:23,  8.71it/s]\u001b[A\n",
      " 42%|████▏     | 146/346 [00:16<00:22,  8.71it/s]\u001b[A\n",
      " 42%|████▏     | 147/346 [00:16<00:22,  8.71it/s]\u001b[A\n",
      " 43%|████▎     | 148/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 43%|████▎     | 149/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 43%|████▎     | 150/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 44%|████▎     | 151/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 44%|████▍     | 152/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 44%|████▍     | 153/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 45%|████▍     | 154/346 [00:17<00:22,  8.71it/s]\u001b[A\n",
      " 45%|████▍     | 155/346 [00:17<00:21,  8.71it/s]\u001b[A\n",
      " 45%|████▌     | 156/346 [00:17<00:21,  8.71it/s]\u001b[A\n",
      " 45%|████▌     | 157/346 [00:18<00:21,  8.71it/s]\u001b[A\n",
      " 46%|████▌     | 158/346 [00:18<00:21,  8.70it/s]\u001b[A\n",
      " 46%|████▌     | 159/346 [00:18<00:21,  8.70it/s]\u001b[A\n",
      " 46%|████▌     | 160/346 [00:18<00:21,  8.69it/s]\u001b[A\n",
      " 47%|████▋     | 161/346 [00:18<00:21,  8.69it/s]\u001b[A\n",
      " 47%|████▋     | 162/346 [00:18<00:21,  8.69it/s]\u001b[A\n",
      " 47%|████▋     | 163/346 [00:18<00:21,  8.70it/s]\u001b[A\n",
      " 47%|████▋     | 164/346 [00:18<00:20,  8.70it/s]\u001b[A\n",
      " 48%|████▊     | 165/346 [00:18<00:20,  8.70it/s]\u001b[A\n",
      " 48%|████▊     | 166/346 [00:19<00:20,  8.70it/s]\u001b[A\n",
      " 48%|████▊     | 167/346 [00:19<00:20,  8.70it/s]\u001b[A\n",
      " 49%|████▊     | 168/346 [00:19<00:20,  8.70it/s]\u001b[A\n",
      " 49%|████▉     | 169/346 [00:19<00:20,  8.68it/s]\u001b[A\n",
      " 49%|████▉     | 170/346 [00:19<00:20,  8.69it/s]\u001b[A\n",
      " 49%|████▉     | 171/346 [00:19<00:20,  8.69it/s]\u001b[A\n",
      " 50%|████▉     | 172/346 [00:19<00:20,  8.70it/s]\u001b[A\n",
      " 50%|█████     | 173/346 [00:19<00:19,  8.70it/s]\u001b[A\n",
      " 50%|█████     | 174/346 [00:20<00:19,  8.70it/s]\u001b[A\n",
      " 51%|█████     | 175/346 [00:20<00:19,  8.70it/s]\u001b[A\n",
      " 51%|█████     | 176/346 [00:20<00:19,  8.70it/s]\u001b[A\n",
      " 51%|█████     | 177/346 [00:20<00:19,  8.70it/s]\u001b[A\n",
      " 51%|█████▏    | 178/346 [00:20<00:19,  8.70it/s]\u001b[A\n",
      " 52%|█████▏    | 179/346 [00:20<00:19,  8.70it/s]\u001b[A\n",
      " 52%|█████▏    | 180/346 [00:20<00:19,  8.71it/s]\u001b[A\n",
      " 52%|█████▏    | 181/346 [00:20<00:18,  8.71it/s]\u001b[A\n",
      " 53%|█████▎    | 182/346 [00:20<00:18,  8.71it/s]\u001b[A\n",
      " 53%|█████▎    | 183/346 [00:21<00:18,  8.71it/s]\u001b[A\n",
      " 53%|█████▎    | 184/346 [00:21<00:18,  8.71it/s]\u001b[A\n",
      " 53%|█████▎    | 185/346 [00:21<00:18,  8.71it/s]\u001b[A\n",
      " 54%|█████▍    | 186/346 [00:21<00:18,  8.70it/s]\u001b[A\n",
      " 54%|█████▍    | 187/346 [00:21<00:18,  8.70it/s]\u001b[A\n",
      " 54%|█████▍    | 188/346 [00:21<00:18,  8.70it/s]\u001b[A\n",
      " 55%|█████▍    | 189/346 [00:21<00:18,  8.70it/s]\u001b[A\n",
      " 55%|█████▍    | 190/346 [00:21<00:17,  8.70it/s]\u001b[A\n",
      " 55%|█████▌    | 191/346 [00:21<00:17,  8.70it/s]\u001b[A\n",
      " 55%|█████▌    | 192/346 [00:22<00:17,  8.70it/s]\u001b[A\n",
      " 56%|█████▌    | 193/346 [00:22<00:17,  8.70it/s]\u001b[A\n",
      " 56%|█████▌    | 194/346 [00:22<00:17,  8.70it/s]\u001b[A\n",
      " 56%|█████▋    | 195/346 [00:22<00:17,  8.70it/s]\u001b[A\n",
      " 57%|█████▋    | 196/346 [00:22<00:17,  8.70it/s]\u001b[A\n",
      " 57%|█████▋    | 197/346 [00:22<00:17,  8.71it/s]\u001b[A\n",
      " 57%|█████▋    | 198/346 [00:22<00:17,  8.71it/s]\u001b[A\n",
      " 58%|█████▊    | 199/346 [00:22<00:16,  8.71it/s]\u001b[A\n",
      " 58%|█████▊    | 200/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 58%|█████▊    | 201/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 58%|█████▊    | 202/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 59%|█████▊    | 203/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 59%|█████▉    | 204/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 59%|█████▉    | 205/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 60%|█████▉    | 206/346 [00:23<00:16,  8.71it/s]\u001b[A\n",
      " 60%|█████▉    | 207/346 [00:23<00:15,  8.71it/s]\u001b[A\n",
      " 60%|██████    | 208/346 [00:23<00:15,  8.71it/s]\u001b[A\n",
      " 60%|██████    | 209/346 [00:24<00:15,  8.70it/s]\u001b[A\n",
      " 61%|██████    | 210/346 [00:24<00:15,  8.70it/s]\u001b[A\n",
      " 61%|██████    | 211/346 [00:24<00:15,  8.69it/s]\u001b[A\n",
      " 61%|██████▏   | 212/346 [00:24<00:15,  8.69it/s]\u001b[A\n",
      " 62%|██████▏   | 213/346 [00:24<00:15,  8.69it/s]\u001b[A\n",
      " 62%|██████▏   | 214/346 [00:24<00:15,  8.69it/s]\u001b[A\n",
      " 62%|██████▏   | 215/346 [00:24<00:15,  8.69it/s]\u001b[A\n",
      " 62%|██████▏   | 216/346 [00:24<00:14,  8.70it/s]\u001b[A\n",
      " 63%|██████▎   | 217/346 [00:24<00:14,  8.69it/s]\u001b[A\n",
      " 63%|██████▎   | 218/346 [00:25<00:14,  8.69it/s]\u001b[A\n",
      " 63%|██████▎   | 219/346 [00:25<00:14,  8.70it/s]\u001b[A\n",
      " 64%|██████▎   | 220/346 [00:25<00:14,  8.70it/s]\u001b[A\n",
      " 64%|██████▍   | 221/346 [00:25<00:14,  8.70it/s]\u001b[A\n",
      " 64%|██████▍   | 222/346 [00:25<00:14,  8.70it/s]\u001b[A\n",
      " 64%|██████▍   | 223/346 [00:25<00:14,  8.70it/s]\u001b[A\n",
      " 65%|██████▍   | 224/346 [00:25<00:14,  8.71it/s]\u001b[A\n",
      " 65%|██████▌   | 225/346 [00:25<00:13,  8.71it/s]\u001b[A\n",
      " 65%|██████▌   | 226/346 [00:25<00:13,  8.71it/s]\u001b[A\n",
      " 66%|██████▌   | 227/346 [00:26<00:13,  8.71it/s]\u001b[A\n",
      " 66%|██████▌   | 228/346 [00:26<00:13,  8.71it/s]\u001b[A\n",
      " 66%|██████▌   | 229/346 [00:26<00:13,  8.71it/s]\u001b[A\n",
      " 66%|██████▋   | 230/346 [00:26<00:13,  8.71it/s]\u001b[A\n",
      " 67%|██████▋   | 231/346 [00:26<00:13,  8.71it/s]\u001b[A\n",
      " 67%|██████▋   | 232/346 [00:26<00:13,  8.71it/s]\u001b[A\n",
      " 67%|██████▋   | 233/346 [00:26<00:12,  8.70it/s]\u001b[A\n",
      " 68%|██████▊   | 234/346 [00:26<00:12,  8.71it/s]\u001b[A\n",
      " 68%|██████▊   | 235/346 [00:27<00:12,  8.71it/s]\u001b[A\n",
      " 68%|██████▊   | 236/346 [00:27<00:12,  8.71it/s]\u001b[A\n",
      " 68%|██████▊   | 237/346 [00:27<00:12,  8.70it/s]\u001b[A\n",
      " 69%|██████▉   | 238/346 [00:27<00:12,  8.70it/s]\u001b[A\n",
      " 69%|██████▉   | 239/346 [00:27<00:12,  8.70it/s]\u001b[A\n",
      " 69%|██████▉   | 240/346 [00:27<00:12,  8.70it/s]\u001b[A\n",
      " 70%|██████▉   | 241/346 [00:27<00:12,  8.70it/s]\u001b[A\n",
      " 70%|██████▉   | 242/346 [00:27<00:11,  8.70it/s]\u001b[A\n",
      " 70%|███████   | 243/346 [00:27<00:11,  8.70it/s]\u001b[A\n",
      " 71%|███████   | 244/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 71%|███████   | 245/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 71%|███████   | 246/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 71%|███████▏  | 247/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 72%|███████▏  | 248/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 72%|███████▏  | 249/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 72%|███████▏  | 250/346 [00:28<00:11,  8.70it/s]\u001b[A\n",
      " 73%|███████▎  | 251/346 [00:28<00:10,  8.70it/s]\u001b[A\n",
      " 73%|███████▎  | 252/346 [00:28<00:10,  8.70it/s]\u001b[A\n",
      " 73%|███████▎  | 253/346 [00:29<00:10,  8.70it/s]\u001b[A\n",
      " 73%|███████▎  | 254/346 [00:29<00:10,  8.70it/s]\u001b[A\n",
      " 74%|███████▎  | 255/346 [00:29<00:10,  8.70it/s]\u001b[A\n",
      " 74%|███████▍  | 256/346 [00:29<00:10,  8.70it/s]\u001b[A\n",
      " 74%|███████▍  | 257/346 [00:29<00:10,  8.71it/s]\u001b[A\n",
      " 75%|███████▍  | 258/346 [00:29<00:10,  8.71it/s]\u001b[A\n",
      " 75%|███████▍  | 259/346 [00:29<00:09,  8.71it/s]\u001b[A\n",
      " 75%|███████▌  | 260/346 [00:29<00:09,  8.70it/s]\u001b[A\n",
      " 75%|███████▌  | 261/346 [00:30<00:09,  8.70it/s]\u001b[A\n",
      " 76%|███████▌  | 262/346 [00:30<00:09,  8.70it/s]\u001b[A\n",
      " 76%|███████▌  | 263/346 [00:30<00:09,  8.71it/s]\u001b[A\n",
      " 76%|███████▋  | 264/346 [00:30<00:09,  8.71it/s]\u001b[A\n",
      " 77%|███████▋  | 265/346 [00:30<00:09,  8.70it/s]\u001b[A\n",
      " 77%|███████▋  | 266/346 [00:30<00:09,  8.69it/s]\u001b[A\n",
      " 77%|███████▋  | 267/346 [00:30<00:09,  8.70it/s]\u001b[A\n",
      " 77%|███████▋  | 268/346 [00:30<00:08,  8.70it/s]\u001b[A\n",
      " 78%|███████▊  | 269/346 [00:30<00:08,  8.70it/s]\u001b[A\n",
      " 78%|███████▊  | 270/346 [00:31<00:08,  8.70it/s]\u001b[A\n",
      " 78%|███████▊  | 271/346 [00:31<00:08,  8.70it/s]\u001b[A\n",
      " 79%|███████▊  | 272/346 [00:31<00:08,  8.69it/s]\u001b[A\n",
      " 79%|███████▉  | 273/346 [00:31<00:08,  8.67it/s]\u001b[A\n",
      " 79%|███████▉  | 274/346 [00:31<00:08,  8.68it/s]\u001b[A\n",
      " 79%|███████▉  | 275/346 [00:31<00:08,  8.69it/s]\u001b[A\n",
      " 80%|███████▉  | 276/346 [00:31<00:08,  8.69it/s]\u001b[A\n",
      " 80%|████████  | 277/346 [00:31<00:07,  8.70it/s]\u001b[A\n",
      " 80%|████████  | 278/346 [00:31<00:07,  8.70it/s]\u001b[A\n",
      " 81%|████████  | 279/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 81%|████████  | 280/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 81%|████████  | 281/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 82%|████████▏ | 282/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 82%|████████▏ | 283/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 82%|████████▏ | 284/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 82%|████████▏ | 285/346 [00:32<00:07,  8.70it/s]\u001b[A\n",
      " 83%|████████▎ | 286/346 [00:32<00:06,  8.71it/s]\u001b[A\n",
      " 83%|████████▎ | 287/346 [00:33<00:06,  8.70it/s]\u001b[A\n",
      " 83%|████████▎ | 288/346 [00:33<00:06,  8.71it/s]\u001b[A\n",
      " 84%|████████▎ | 289/346 [00:33<00:06,  8.70it/s]\u001b[A\n",
      " 84%|████████▍ | 290/346 [00:33<00:06,  8.71it/s]\u001b[A\n",
      " 84%|████████▍ | 291/346 [00:33<00:06,  8.71it/s]\u001b[A\n",
      " 84%|████████▍ | 292/346 [00:33<00:06,  8.67it/s]\u001b[A\n",
      " 85%|████████▍ | 293/346 [00:33<00:06,  8.67it/s]\u001b[A\n",
      " 85%|████████▍ | 294/346 [00:33<00:05,  8.68it/s]\u001b[A\n",
      " 85%|████████▌ | 295/346 [00:33<00:05,  8.69it/s]\u001b[A\n",
      " 86%|████████▌ | 296/346 [00:34<00:05,  8.69it/s]\u001b[A\n",
      " 86%|████████▌ | 297/346 [00:34<00:05,  8.69it/s]\u001b[A\n",
      " 86%|████████▌ | 298/346 [00:34<00:05,  8.70it/s]\u001b[A\n",
      " 86%|████████▋ | 299/346 [00:34<00:05,  8.70it/s]\u001b[A\n",
      " 87%|████████▋ | 300/346 [00:34<00:05,  8.70it/s]\u001b[A\n",
      " 87%|████████▋ | 301/346 [00:34<00:05,  8.70it/s]\u001b[A\n",
      " 87%|████████▋ | 302/346 [00:34<00:05,  8.70it/s]\u001b[A\n",
      " 88%|████████▊ | 303/346 [00:34<00:04,  8.70it/s]\u001b[A\n",
      " 88%|████████▊ | 304/346 [00:34<00:04,  8.70it/s]\u001b[A\n",
      " 88%|████████▊ | 305/346 [00:35<00:04,  8.70it/s]\u001b[A\n",
      " 88%|████████▊ | 306/346 [00:35<00:04,  8.70it/s]\u001b[A\n",
      " 89%|████████▊ | 307/346 [00:35<00:04,  8.70it/s]\u001b[A\n",
      " 89%|████████▉ | 308/346 [00:35<00:04,  8.70it/s]\u001b[A\n",
      " 89%|████████▉ | 309/346 [00:35<00:04,  8.70it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 310/346 [00:35<00:04,  8.70it/s]\u001b[A\n",
      " 90%|████████▉ | 311/346 [00:35<00:04,  8.70it/s]\u001b[A\n",
      " 90%|█████████ | 312/346 [00:35<00:03,  8.70it/s]\u001b[A\n",
      " 90%|█████████ | 313/346 [00:35<00:03,  8.70it/s]\u001b[A\n",
      " 91%|█████████ | 314/346 [00:36<00:03,  8.70it/s]\u001b[A\n",
      " 91%|█████████ | 315/346 [00:36<00:03,  8.70it/s]\u001b[A\n",
      " 91%|█████████▏| 316/346 [00:36<00:03,  8.70it/s]\u001b[A\n",
      " 92%|█████████▏| 317/346 [00:36<00:03,  8.70it/s]\u001b[A\n",
      " 92%|█████████▏| 318/346 [00:36<00:03,  8.69it/s]\u001b[A\n",
      " 92%|█████████▏| 319/346 [00:36<00:03,  8.69it/s]\u001b[A\n",
      " 92%|█████████▏| 320/346 [00:36<00:02,  8.69it/s]\u001b[A\n",
      " 93%|█████████▎| 321/346 [00:36<00:02,  8.70it/s]\u001b[A\n",
      " 93%|█████████▎| 322/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 93%|█████████▎| 323/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 94%|█████████▎| 324/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 94%|█████████▍| 325/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 94%|█████████▍| 326/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 95%|█████████▍| 327/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 95%|█████████▍| 328/346 [00:37<00:02,  8.70it/s]\u001b[A\n",
      " 95%|█████████▌| 329/346 [00:37<00:01,  8.70it/s]\u001b[A\n",
      " 95%|█████████▌| 330/346 [00:37<00:01,  8.70it/s]\u001b[A\n",
      " 96%|█████████▌| 331/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 96%|█████████▌| 332/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 96%|█████████▌| 333/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 97%|█████████▋| 334/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 97%|█████████▋| 335/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 97%|█████████▋| 336/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 97%|█████████▋| 337/346 [00:38<00:01,  8.70it/s]\u001b[A\n",
      " 98%|█████████▊| 338/346 [00:38<00:00,  8.70it/s]\u001b[A\n",
      " 98%|█████████▊| 339/346 [00:38<00:00,  8.70it/s]\u001b[A\n",
      " 98%|█████████▊| 340/346 [00:39<00:00,  8.70it/s]\u001b[A\n",
      " 99%|█████████▊| 341/346 [00:39<00:00,  8.70it/s]\u001b[A\n",
      " 99%|█████████▉| 342/346 [00:39<00:00,  8.70it/s]\u001b[A\n",
      " 99%|█████████▉| 343/346 [00:39<00:00,  8.69it/s]\u001b[A\n",
      " 99%|█████████▉| 344/346 [00:39<00:00,  8.70it/s]\u001b[A\n",
      "100%|██████████| 346/346 [00:39<00:00,  8.70it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        outputs = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    scores = outputs[0].detach().cpu().numpy()  # 每个字的标签的概率\n",
    "    pred_flat = np.argmax(scores[0], axis=1).flatten()\n",
    "    label_ids = b_labels.to('cpu').numpy()  # 真实labels\n",
    "    # print(logits, label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "\n",
    "\n",
    "\n",
    "output_dir = \"./model_save\"\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "torch.save(model_to_save.state_dict(), os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取模型\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "output_dir = \"./model_save\"\n",
    "model = BertForTokenClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单句测试\n",
    "\n",
    "# test_sententce = \"在北京市朝阳区的一家网吧，我亲眼看见卢本伟和孙笑川一起开挂。\"\n",
    "test_sententce = \"史源源的房子租在滨江区南环路税友大厦附近。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 tag 到 索引 的字典\n",
    "tag_to_ix = {\"B-LOC\": 0,\n",
    "             \"I-LOC\": 1, \n",
    "             \"B-ORG\": 2, \n",
    "             \"I-ORG\": 3,\n",
    "             \"B-PER\": 4,\n",
    "             \"I-PER\": 5,\n",
    "             \"O\": 6,\n",
    "             \"[CLS]\":7,\n",
    "             \"[SEP]\":8,\n",
    "             \"[PAD]\":9}\n",
    "\n",
    "ix_to_tag = {0:\"B-LOC\", \n",
    "             1:\"I-LOC\", \n",
    "             2:\"B-ORG\", \n",
    "             3:\"I-ORG\",\n",
    "             4:\"B-PER\",\n",
    "             5:\"I-PER\",\n",
    "             6:\"O\",\n",
    "             7:\"[CLS]\",\n",
    "             8:\"[SEP]\",\n",
    "             9:\"[PAD]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 1380, 3975, 3975, 4638, 2791, 2094, 4909, 1762, 4012, 3736, 1277, 1298, 4384, 6662, 4925, 1351, 1920, 1336, 7353, 6818, 511, 102]]\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# word2token\n",
    "tokenized_texts = [tokenizer.encode(test_sententce, add_special_tokens=True)]\n",
    "print(tokenized_texts)\n",
    "print(len(tokenized_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 101 1380 3975 3975 4638 2791 2094 4909 1762 4012 3736 1277 1298 4384\n",
      "  6662 4925 1351 1920 1336 7353 6818  511  102    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "# 句子padding\n",
    "# 句子最长长度\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "test_input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")\n",
    "print(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 创建attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(np.array(attention_masks))\n",
    "\n",
    "# 0-51 为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1380, 3975, 3975, 4638, 2791, 2094, 4909, 1762, 4012, 3736, 1277,\n",
      "         1298, 4384, 6662, 4925, 1351, 1920, 1336, 7353, 6818,  511,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(test_inputs)\n",
    "print(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成验证数据集\n",
    "batch_size = 64\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "# 随机采样\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "# 读取数据\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证状态\n",
    "model.eval()\n",
    "\n",
    "# 建立变量\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 4 5 5 6 6 6 6 6 0 1 1 0 1 1 0 1 1 1 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        outputs = model(input_ids=b_input_ids, \n",
    "                       attention_mask=None,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # Move logits and labels to CPU\n",
    "    scores = outputs[0].detach().cpu().numpy()  # 每个字的标签的概率\n",
    "    pred_flat = np.argmax(scores[0], axis=1).flatten()\n",
    "    # label_ids = b_labels.to('cpu').numpy()  # 真实labels\n",
    "    print(pred_flat)  # 预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试句子: 史源源的房子租在滨江区南环路税友大厦附近。\n",
      "21\n",
      "['[CLS]', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "pre_labels = [ix_to_tag[n] for n in pred_flat]\n",
    "print(f\"测试句子: {test_sententce}\")\n",
    "print(len(test_sententce))\n",
    "print(pre_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_labels_cut = pre_labels[0:len(test_sententce)+2]\n",
    "pre_labels_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3]]\n",
      "[[9, 10, 11], [12, 13, 14], [15, 16, 17, 18]]\n"
     ]
    }
   ],
   "source": [
    "person = []  # 临时栈\n",
    "persons = []\n",
    "\n",
    "location = []\n",
    "locations = []\n",
    "\n",
    "\n",
    "for i in range(len(pre_labels_cut) - 1):\n",
    "    # Person\n",
    "    # 单字情况\n",
    "    if pre_labels[i] == 'B-PER' and pre_labels[i+1] != 'I-PER' and len(location) == 0:\n",
    "        person.append(i)  \n",
    "        persons.append(person)\n",
    "        person = []  # 清空\n",
    "        continue    \n",
    "        \n",
    "    # 非单字\n",
    "    # 如果前面有连着的 PER 实体    \n",
    "    if pre_labels[i] == 'B-PER'and pre_labels[i+1] == 'I-PER' and len(person) != 0:\n",
    "        person.append(i)\n",
    "        \n",
    "    # 如果前面没有连着的 B-PER 实体\n",
    "    elif pre_labels[i] == 'B-PER'and pre_labels[i+1] == 'I-PER' and len(location) == 0:\n",
    "        person.append(i)  # 加入新的 B-PER\n",
    "    elif pre_labels[i] != 'I-PER' and len(person) != 0:\n",
    "        persons.append(person)  # 临时栈内容放入正式栈\n",
    "        person = []  # 清空临时栈\n",
    "    elif pre_labels[i] == 'I-PER' and len(person) != 0:\n",
    "        person.append(i)\n",
    "    else:  # 极少数情况会有 I-PER 开头的，不理\n",
    "        pass\n",
    "\n",
    "    # Location\n",
    "    # 单字情况\n",
    "    if pre_labels[i] == 'B-LOC' and pre_labels[i+1] != 'I-LOC' and len(location) == 0:\n",
    "        location.append(i)  \n",
    "        locations.append(location)\n",
    "        location = []  # 清空\n",
    "        continue\n",
    "        \n",
    "    # 非单字\n",
    "    # 如果前面有连着的 LOC 实体\n",
    "    \n",
    "    if pre_labels[i] == 'B-LOC' and pre_labels[i+1] == 'I-LOC' and len(location) != 0:\n",
    "        locations.append(location)\n",
    "        location = []  # 清空栈\n",
    "        location.append(i)  # 加入新的 B-LOC\n",
    "        \n",
    "    # 如果前面没有连着的 B-LOC 实体\n",
    "    elif pre_labels[i] == 'B-LOC' and pre_labels[i+1] == 'I-LOC' and len(location) == 0:\n",
    "        location.append(i)  # 加入新的 B-LOC\n",
    "    elif pre_labels[i] == 'I-LOC' and len(location) != 0:\n",
    "        location.append(i)\n",
    "    # 结尾\n",
    "    elif pre_labels[i] != 'I-LOC' and len(location) != 0:\n",
    "        locations.append(location)  # 临时栈内容放入正式栈\n",
    "        location = []  # 清空临时栈\n",
    "    else:  # 极少数情况会有 I-LOC 开头的，不理\n",
    "        pass\n",
    "    \n",
    "print(persons)\n",
    "print(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文字中提取\n",
    "# 人物\n",
    "NER_PER = []\n",
    "for word_idx in persons:\n",
    "    ONE_PER = []\n",
    "    for letter_idx in word_idx: \n",
    "        ONE_PER.append(test_sententce[letter_idx - 1])\n",
    "    NER_PER.append(ONE_PER)\n",
    "\n",
    "NER_PER_COMBINE = []\n",
    "for w in NER_PER:\n",
    "    PER = \"\".join(w)\n",
    "    NER_PER_COMBINE.append(PER)\n",
    "    \n",
    "# 地点\n",
    "\n",
    "NER_LOC = []\n",
    "for word_idx in locations:\n",
    "    ONE_LOC = []\n",
    "    for letter_idx in word_idx: \n",
    "        # print(letter_idx)\n",
    "        # print(test_sententce[letter_idx])\n",
    "        ONE_LOC.append(test_sententce[letter_idx - 1])\n",
    "    NER_LOC.append(ONE_LOC)\n",
    "\n",
    "NER_LOC_COMBINE = []\n",
    "for w in NER_LOC:\n",
    "    LOC = \"\".join(w)\n",
    "    NER_LOC_COMBINE.append(LOC)\n",
    "\n",
    "# 组织"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前句子：史源源的房子租在滨江区南环路税友大厦附近。\n",
      "\n",
      "    人物：['史源源']\n",
      "\n",
      "    地点：['滨江区', '南环路', '税友大厦']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"当前句子：{test_sententce}\\n\")\n",
    "print(f\"    人物：{NER_PER_COMBINE}\\n\")\n",
    "print(f\"    地点：{NER_LOC_COMBINE}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}