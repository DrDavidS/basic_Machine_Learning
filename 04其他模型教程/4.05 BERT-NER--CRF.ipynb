{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch import Tensor\n",
    "from transformers import AlbertModel, BertTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # padding\n",
    "from pytorchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"dh_msra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"配置参数\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        current_path = os.getcwd()\n",
    "        self.model_name = \"pytorch_model.bin\"\n",
    "        self.bert_path = os.path.join(current_path + \"/albert_chinese_tiny\")\n",
    "        # self.train_file = '../datas/THUCNews/train.txt'\n",
    "        self.num_classes = 10  # NER 的 token 类别\n",
    "        self.hidden_size = 312  # 隐藏层输出维度\n",
    "        self.hidden_dropout_prob = 0.1  # dropout比例\n",
    "        self.batch_size = 64  # mini-batch大小\n",
    "        self.max_len = 128  # 句子的最长padding长度\n",
    "        self.epochs = 3  # epoch数\n",
    "        self.learning_rate = 2e-5  # 学习率\n",
    "        self.save_path = os.path.join(current_path + \"/finetuned_albert\")  # 模型训练结果保存路径\n",
    "        self.use_cuda = True\n",
    "        self.device_id = 5\n",
    "\n",
    "\n",
    "config = Config()\n",
    "print(config.bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUcheck\n",
    "\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "if torch.cuda.is_available() and config.use_cuda:\n",
    "    print(\"GPU numbers: \", n_gpu)\n",
    "    print(\"device_name: \", torch.cuda.get_device_name(0))\n",
    "    device_id = config.device_id  # 注意选择\n",
    "    torch.cuda.set_device(device_id)\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    print(f\"当前设备：{torch.cuda.current_device()}\")\n",
    "else :\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"当前设备：{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_separate = []\n",
    "all_letter_labels = []\n",
    "label_set = set()\n",
    "with open(file, encoding=\"utf-8\") as f:\n",
    "    single_sentence = []\n",
    "    single_sentence_labels = []\n",
    "    for s in f.readlines():\n",
    "        if s != \"\\n\":\n",
    "            word, label = s.split(\"\\t\")\n",
    "            label = label.strip(\"\\n\")\n",
    "            single_sentence.append(word)\n",
    "            single_sentence_labels.append(label)\n",
    "            label_set.add(label)\n",
    "        elif s == \"\\n\":\n",
    "            all_sentences_separate.append(single_sentence)\n",
    "            all_letter_labels.append(single_sentence_labels)\n",
    "            single_sentence = []\n",
    "            single_sentence_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_sentences_separate[0:2])\n",
    "print(all_letter_labels[0:2])\n",
    "print(f\"\\n所有的标签：{label_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 tag 到 索引 的字典\n",
    "tag_to_ix = {\"B-LOC\": 0,\n",
    "             \"I-LOC\": 1, \n",
    "             \"B-ORG\": 2, \n",
    "             \"I-ORG\": 3,\n",
    "             \"B-PER\": 4,\n",
    "             \"I-PER\": 5,\n",
    "             \"O\": 6,\n",
    "             \"[CLS]\":7,\n",
    "             \"[SEP]\":8,\n",
    "             \"[PAD]\":9}\n",
    "\n",
    "ix_to_tag = {0:\"B-LOC\", \n",
    "             1:\"I-LOC\", \n",
    "             2:\"B-ORG\", \n",
    "             3:\"I-ORG\",\n",
    "             4:\"B-PER\",\n",
    "             5:\"I-PER\",\n",
    "             6:\"O\",\n",
    "             7:\"[CLS]\",\n",
    "             8:\"[SEP]\",\n",
    "             9:\"[PAD]\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []  # 句子\n",
    "\n",
    "for one_sentence in all_sentences_separate:\n",
    "    sentence = \"\".join(one_sentence)\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "pprint(all_sentences[15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []  # labels\n",
    "for letter_labels in all_letter_labels:\n",
    "    labels = [tag_to_ix[t] for t in letter_labels]\n",
    "    all_labels.append(labels)\n",
    "\n",
    "print(all_labels[0:2])\n",
    "print(len(all_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2token\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path, do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子padding\n",
    "\n",
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=config.max_len, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 代表 Other 实体\n",
    "for label in all_labels:\n",
    "    label.insert(len(label), 8)  # [SEP] 加在末尾\n",
    "    label.insert(0, 7) # [CLS] 加在开头\n",
    "    if config.max_len > len(label) -1:\n",
    "        for i in range(config.max_len - len(label)):\n",
    "            label.append(9)  # [PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "# train-test-split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, \n",
    "                                                                                    all_labels, \n",
    "                                                                                    random_state=2019, \n",
    "                                                                                    test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2019, \n",
    "                                                       test_size=0.1)\n",
    "\n",
    "# tensor化\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# dataloader\n",
    "\n",
    "# 形成训练数据集\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# 随机采样\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# 读取数据\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=config.batch_size)\n",
    "\n",
    "\n",
    "# 形成验证数据集\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# 随机采样\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# 读取数据\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAlBert(nn.Module):\n",
    "    \"\"\"\n",
    "    新增 句子位置ID 特征拼接。\n",
    "    为了保证速度，选择基于预训练的 Albert-tiny 微调\n",
    "\n",
    "    我们想要对裁判文书进行分类，原文的文本形式，会进入BERT模型。\n",
    "    代码中的“HYID”本身就是数字形式，没必要放入BERT模型中，于是我们将 BERT 输出后的 768 维向量拼接\n",
    "    tensor(HYID)，也就是变成了 769 维，再过一个 线形层 + softmax 输出分类结果。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ModelAlBert, self).__init__()\n",
    "        self.num_labels = config.num_classes\n",
    "        self.albert = AlbertModel.from_pretrained(config.bert_path)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor = None,\n",
    "        attention_mask: Tensor = None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels: Tensor = None,\n",
    "    ) -> set:\n",
    "        \"\"\"\n",
    "        模型前向传播结构\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor[Tensor], optional): Token 化的句子. Defaults to None.\n",
    "            attention_mask (Tensor[Tensor], optional): Attention Mask，配合Padding使用. Defaults to None.\n",
    "            token_type_ids ([type], optional): 上下句 id 标记，这里不涉及. Defaults to None.\n",
    "            position_ids ([type], optional): token 位置 id. Defaults to None.\n",
    "            head_mask ([type], optional): [description]. Defaults to None.\n",
    "            inputs_embeds ([type], optional): 不需要. Defaults to None.\n",
    "            labels (Tensor, optional): 标签. Defaults to None.\n",
    "            HYID (Tensor, optional): 这里指的是句子的位置ID，也可以是其他特征. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            (set): 模型的返回值, (loss), logits, (hidden_states), (attentions)\n",
    "        \"\"\"\n",
    "        outputs = self.albert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "                active_labels = labels.view(-1)[active_loss]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        output = (logits,) + outputs[2:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "                \n",
    "\n",
    "model = ModelAlBert(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAlBertCRF(nn.Module):\n",
    "    \"\"\"\n",
    "    新增 句子位置ID 特征拼接。\n",
    "    为了保证速度，选择基于预训练的 Albert-tiny 微调\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(ModelAlBertCRF, self).__init__()\n",
    "        self.num_labels = config.num_classes\n",
    "        self.albert = AlbertModel.from_pretrained(config.bert_path)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_classes)\n",
    "        self.crf = CRF(num_tags=config.num_classes, batch_first=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Tensor = None,\n",
    "        attention_mask: Tensor = None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels: Tensor = None,\n",
    "    ) -> set:\n",
    "        \"\"\"\n",
    "        模型前向传播结构。\n",
    "        注意loss采用的是CRF的 log likelihood\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor[Tensor], optional): Token 化的句子. Defaults to None.\n",
    "            attention_mask (Tensor[Tensor], optional): Attention Mask，配合Padding使用. Defaults to None.\n",
    "            token_type_ids ([type], optional): 上下句 id 标记，这里不涉及. Defaults to None.\n",
    "            position_ids ([type], optional): token 位置 id. Defaults to None.\n",
    "            head_mask ([type], optional): [description]. Defaults to None.\n",
    "            inputs_embeds ([type], optional): 不需要. Defaults to None.\n",
    "            labels (Tensor, optional): 标签. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            (set): 模型的返回值, (loss), logits, (hidden_states), (attentions)\n",
    "        \"\"\"\n",
    "        outputs = self.albert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        loss = None\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss = self.crf(emissions = logits, tags=labels, mask=attention_mask)\n",
    "            # Note that the returned value is the log likelihood \n",
    "            # so you’ll need to make this value negative as your loss. \n",
    "            outputs =(-1 * loss,) + outputs\n",
    "        return outputs # (loss), scores\n",
    "                \n",
    "\n",
    "model = ModelAlBertCRF(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "bert_param_optimizer = list(model.albert.named_parameters())\n",
    "crf_param_optimizer = list(model.crf.named_parameters())\n",
    "linear_param_optimizer = list(model.classifier.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 权重衰减\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01,\n",
    "     'lr': config.learning_rate},\n",
    "    {'params': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0,\n",
    "     'lr': config.learning_rate},\n",
    "    \n",
    "    {'params': [p for n, p in crf_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01, \n",
    "     'lr': config.crf_learning_rate},\n",
    "    {'params': [p for n, p in crf_param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0,\n",
    "     'lr': config.crf_learning_rate},\n",
    "    \n",
    "    {'params': [p for n, p in linear_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.01,\n",
    "     'lr': config.crf_learning_rate},\n",
    "    {'params': [p for n, p in linear_param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0,\n",
    "     'lr': config.crf_learning_rate}\n",
    "]\n",
    "# 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存loss\n",
    "train_loss_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT training loop\n",
    "for _ in range(config.epochs): \n",
    "    ## 训练\n",
    "    print(f\"当前epoch： {_}\")\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # 把batch放入GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # 解包batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播loss计算\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  \n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"当前 epoch 的 Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证状态\n",
    "model.eval()\n",
    "\n",
    "# 建立变量\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集的读取也要batch\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        outputs = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    scores = outputs[0].detach().cpu().numpy()  # 每个字的标签的概率\n",
    "    pred_flat = np.argmax(scores[0], axis=1).flatten()\n",
    "    label_ids = b_labels.to('cpu').numpy()  # 真实labels\n",
    "    # print(logits, label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}