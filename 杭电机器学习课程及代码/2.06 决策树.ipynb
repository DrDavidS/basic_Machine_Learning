{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "\n",
    "作者：杨岱川\n",
    "\n",
    "时间：2019年11月\n",
    "\n",
    "github：https://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "开源协议：[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "## 决策树简介\n",
    "\n",
    "决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈现树形结构，在分类问题中，表示基于特征对实例进行分类的过程。\n",
    "\n",
    "决策树既可以认为是一个 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。\n",
    "\n",
    "<img src=\"http://www.itfly.net/uploads/allimg/20170223/1487800510698_1.jpg\" width=\"500\" alt=\"贷款决策树\" align=center>\n",
    "\n",
    "<center>决策树示例</center>\n",
    "\n",
    "决策树在学习时候，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，又利用决策树进行分类。\n",
    "\n",
    ">机器学习领域，决策树学习算法的思想主要来源于[John Ross Quinlan](https://en.wikipedia.org/wiki/Ross_Quinlan)在1986年的论文《Induction of Decision Trees》提出的 ID3 算法，而后他又在1993年提出了改进的 C4.5 算法。[Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman)也在1984年提出了分类与回归树（[CART](https://www.jstor.org/stable/2530946?seq=1#page_scan_tab_contents)）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的基本方法\n",
    "\n",
    "### 决策树模型\n",
    "\n",
    ">**定义**：决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型，内部结点（internal node）和叶节点（lead node）。内部结点表示一个属性或特征，叶结点表示一个类。\n",
    "\n",
    "如果我们使用决策树对某个实例进行分类，那么就从**根结点**开始，对实例的某个特征进行测试，根据测试的结果，将实例分配到其子结点。每一个子结点对应该特征的一个取值。如此递归地对实例进行测试分配，直到达到**叶结点**。最后我们把实例分到叶结点的类中。\n",
    "\n",
    "### 决策树和规则\n",
    "\n",
    "决策树可以看作是一个 if-then 规则的集合：从根结点到叶结点的每一条路径构建一条规则，**内部结点**的特征对应着规则的条件，而**叶结点**的类对应着规则的结论。这些规则的集合有一个重要的特征：互斥且完备。\n",
    "\n",
    ">**互斥且完备**：每个实例都被一条规则覆盖（只满足一条规则），而且只被一条规则所覆盖（满足）。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DrDavidS/basic_Machine_Learning/master/back_up_images/162042244705421.jpg\" width=\"500\" alt=\"决策树示例\" align=center>\n",
    "\n",
    "<center>送礼物决策树示例</center>\n",
    "\n",
    "以上图为例，如果我要送一个工艺模型给朋友，由于在平时比较了解朋友的喜好，那么我就可以绘制出上面这个决策树。根据“互斥且完备”的原则，各种备选的工艺模型（实例）均是可以被上面的决策树分类的。\n",
    "\n",
    "简单起见，叶子结点结果就是两种，“喜欢”和“不喜欢”。\n",
    "\n",
    ">如果送一个灰色的工艺模型，在根节点处应当如何分类？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树的学习\n",
    "\n",
    "#### 数据定义\n",
    "\n",
    "假设给定训练数据集\n",
    "\n",
    "$$\\large T=\\left \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N) \\right \\} $$\n",
    "\n",
    "其中：\n",
    "\n",
    "$i = 1,2,3,\\cdots,N$，表示样本的容量；\n",
    "\n",
    "$x_i \\in \\Bbb X \\subseteq \\mathbf R^n$ 为实例的特征向量，可以即数据的 features（特征），有多个维度；\n",
    "\n",
    "$ y_i \\in \\Bbb Y = \\left \\{ c_1,c_2,\\cdots,c_K \\right \\}$，可以类比为数据的 labels（标签），可以是多分类的。\n",
    "\n",
    "决策树的学习目标就是根据给定的训练数据集构建一个决策树模型，使他能够对实例进行正确的分类。\n",
    "\n",
    "#### 学习目标\n",
    "\n",
    "在实际数据集中，我们为归纳出一组分类规则，从而构建决策树。与训练集不相矛盾的决策树构建方法可能有多个，也可能一个都没有。由于后一种情况经常存在，因此我们的目标是构建一个矛盾较小的决策树，同时具有很好的泛化能力。\n",
    "\n",
    "> 泛化能力强，简单理解为在其他未知数据集上的表现也很好。\n",
    "\n",
    "在机器学习领域，决策树学习用损失函数来表示“矛盾较小”这一目标。决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。\n",
    "\n",
    "#### 方法简述\n",
    "\n",
    "决策树学习的算法通常就是递归地选择最优特征，然后根据这个最优特征对训练集进行分割，使得对各个子数据集（这时候最优特征已经被抽样，不放回了，剩下的其他特征构成了子数据集）有一个最好地分类过程。\n",
    "\n",
    "过程如下:\n",
    "\n",
    "- 一开始构建根节点，将所有训练数据放在根节点，选择一个最优特征，然后按照该最优特征将数据集分割成子集，使得各个子集在当前条件下有一个最好地分类。\n",
    "    - 如果这些子集已经能够被基本正确地分类，就把这些子集分到他们所对应地叶结点。\n",
    "    - 如果有的子集不能被基本正确分类，那么对这些子集重新选择最优特征进行分隔。\n",
    "\n",
    "如此递归进行，直到：\n",
    "\n",
    "- 所有训练数据子集都被基本正确地分类\n",
    "- 没有合适的特征\n",
    "\n",
    "最后每个子集都被分到一个叶结点，说明都有了明确的分类，这样就形成了一棵**决策树**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择\n",
    "\n",
    "前面有讲过，特征选择就是选取对训练数据具有分类能力的特征，可以提高决策树学习的效率。在数据集中，有的特征分类效果好，有的特征分类效果差。\n",
    "\n",
    "如果我们选择了一个特征进行分类，其结果和**随机分类**的结果没有很大的区别，则称这个特征是没有分类能力的，可以考虑丢弃这样的特征。\n",
    "\n",
    "通常我们应该如何进行特征的选择呢，答案是采用 **信息增益（information gain）** 或者 **信息增益比（information gain ratio）**。\n",
    "\n",
    "### 信息增益\n",
    "\n",
    "简单起见，这里只讨论信息增益。\n",
    "\n",
    "#### 熵的定义\n",
    "\n",
    "首先给出**熵（entropy）**的定义，假设 $X$ 是一个取有限个值的离散随机变量（比如等级变量 A/B/C/D），其概率分布为：\n",
    "\n",
    "$$\\large P(X=x_i)=p_i,\\quad i=1,2,\\cdots,n$$\n",
    "\n",
    "则随机变量 $X$ 的熵定义为：\n",
    "\n",
    "$$\\large H(X)=-\\sum^n_{i=1}p_i\\log p_i$$\n",
    "\n",
    "由定义可知，熵只依赖于 $X$ 的分布，而与 $X$ 的取值无关，所以也可以将 $X$ 的熵记作 $H(p)$，即：\n",
    "\n",
    "$$\\large H(p)=-\\sum^n_{i=1}p_i\\log p_i$$\n",
    "\n",
    "熵越大，随机变量的不确定性就越大。\n",
    "\n",
    ">例如，当随机变量取 $0$ 和 $1$ 的时候， $X$ 的分布为\n",
    ">\n",
    ">$ P(X=1)=p,\\quad P(X=0)=1-p,\\quad 0\\le p\\le1$\n",
    ">\n",
    ">熵为：\n",
    ">\n",
    ">$H(p)=-p\\log_2p-(1-p)\\log_2(1-p)$\n",
    ">\n",
    ">当 $p=0$ 或者 $p=1$ 的时候，熵 $H(p)=0$，说明随机变量完全没有不确定性。当 $p=0$ 时，$H(p)=1$，表明不确定性最大。\n",
    "\n",
    "**条件熵** $H(Y|X)$ 表示在已知随机变量 $X$ 的条件下随机变量 $Y$ 的不确定性：\n",
    "\n",
    "$$\\large H(Y|X)=\\sum^n_{i=1}p_iH(Y|X=x_i)$$\n",
    "\n",
    "其中，$p_i=P(X=x_i),\\quad i=1,2,\\cdots,n$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 信息增益的定义\n",
    "\n",
    "现在给出信息增益的定义：\n",
    "\n",
    ">特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$，定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即\n",
    ">$$\\large g(D,A)=H(D)-H(D|A)$$\n",
    "\n",
    "这里，条件经验熵 $H(D|A)$ 表示在特征 $A$ 给定的条件下，对数据集 $D$ 进行分类的不确定性。\n",
    "\n",
    "简单地说，$g(D,A)$ 就是将特征 $A$ 作为决策树的分类特征时，数据集分类不确定性减少的程度。在实际计算过程中，选择信息增益最大的那个特征（最优特征）作为决策树第一个分类特征。\n",
    "\n",
    "具体的计算步骤和公式可以参加李航《统计学习方法》中决策树一章的说明。\n",
    "\n",
    "### 缺点与改进\n",
    "\n",
    "以**信息增益**（ID3决策树）作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用**信息增益比**（C4.5决策树）可以对这个问题进行校正。\n",
    "\n",
    ">特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$，定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即\n",
    ">\n",
    ">$$\\large g_R(D,A)= \\frac{g(D,A)}{H_A(D)}$$\n",
    ">\n",
    ">其中，\n",
    ">\n",
    ">$H_A(D)=-\\sum^n_{i=1}\\frac{|D_i|}{|D|}\\log_2 \\frac{|D_i|}{|D|}$，$n$ 是特征的取值个数，\n",
    ">\n",
    ">$|D|$ 表示其样本容量，$|D_i|$ 表示根据特征 $A$ 的取值而将 $D$ 划分为 $n$ 个子集的其中第 $i$ 个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "\n",
    "### 剪枝简介\n",
    "\n",
    "决策树由算法递归而生成，会一直分裂下去直到不能继续为止。这样的决策树往往对训练集的分类很准确，但是对未知的测试集的分类难以准确判断，也就是产生了“过拟合”的现象。\n",
    "\n",
    "简而言之，决策树的过拟合体现在，算法构建了过于复杂的决策树。自然，解决这个问题的方法就是降低决策树的复杂度。\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DrDavidS/basic_Machine_Learning/master/back_up_images/timg.jpg\" width=\"500\" alt=\"决策树剪枝\" align=center>\n",
    "\n",
    "<center>决策树剪枝</center>\n",
    "\n",
    "在决策树的学习中，将已生成的决策树进行简化的过程叫做**剪枝**，具体来说，就是从已经生成的树上裁掉一些子树或者叶节点，然后将其根节点或者父节点作为新的叶节点，从而简化分类树模型。\n",
    "\n",
    "### 后剪枝\n",
    "\n",
    "常用的决策树剪枝方法有**前剪枝（预剪枝）**和**后剪枝**，前剪枝是边生成树边剪枝，训练速度快，但容易欠拟合；后剪枝是完全生成后再从底向上剪枝，得出的模型泛化性能好，但训练时间需要更久。\n",
    "\n",
    "《统计学习方法》一书中所展示的剪枝方法属于**后剪枝**，即先生成整个决策树，再进行剪枝。主要思想就是通过极小化决策树整体的损失函数来实现。简单解释一下，书上把决策树学习的损失函数定义为\n",
    "\n",
    "$$\\large C_\\alpha(T)=C(T)+\\alpha|T|$$\n",
    "\n",
    "其中 $C(T)$ 指的是模型对训练数据的预测误差，而 $\\alpha|T|$ 中的 $|T|$ 表示模型的复杂度，$\\alpha$ 控制其影响。整体的目标就是 $C_{\\alpha}(T)$ 最小。\n",
    "\n",
    "实际上，这就是我们在[2.01 统计学习及监督学习概论](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/%E6%9D%AD%E7%94%B5%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%8F%8A%E4%BB%A3%E7%A0%81/2.01%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA.ipynb)中提到过的**结构风险最小化（正则化）**，也就是既要保证训练的经验风险小，也要保证模型的结构风险小，也就是模型要越简单越好。$\\alpha$ 是正则化系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的编程应用\n",
    "\n",
    "在 SKlearn 中，已经有封装好的[决策树](https://scikit-learn.org/stable/modules/tree.html)方法了，使用方法和之前的一致。\n",
    "\n",
    "我们只需要\n",
    "\n",
    "```python\n",
    "from sklearn import tree\n",
    "```\n",
    "\n",
    "即可。\n",
    "\n",
    "这里暂时先借用 SKlearn 的官方例子作为代码，而详细的使用方法我们放在下一节中说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()  # 实例化\n",
    "clf = clf.fit(X, y)  # 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.]])  # 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
