{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升方法\n",
    "\n",
    "作者：杨岱川\n",
    "\n",
    "时间：2019年11月\n",
    "\n",
    "github：https://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "开源协议：[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "## 提升方法简介\n",
    "\n",
    "**提升方法（Boosting）**是一种常用的统计学习方法，应用十分广泛。在各种表格数据比赛和实验中，采用基于Boosting Tree的各种机器学习工具足以得到优秀的结果。在分类问题中，它通过改变训练样本的权重，学习多个弱分类器，并且将这些分类器进行线性组合，提高分类的性能。\n",
    "\n",
    "Boosting Tree及其衍生方法是实际应用的重点。AdaBoost 算法在1995年由以色列计算机科学家[Yoav Freund](https://en.wikipedia.org/wiki/Yoav_Freund)和美国计算机科学家[Robert Schapire](https://en.wikipedia.org/wiki/Robert_Schapire)提出。Boosting Tree最早由斯坦福大学统计学教授[Jerome H. Friedman](https://en.wikipedia.org/wiki/Jerome_H._Friedman)在2000年提出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost算法\n",
    "\n",
    "### Boosting方法的基本思路\n",
    "\n",
    "提升方法基于如下思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。也就是“三个臭皮匠，顶个诸葛亮”。这种思想在之前基于多颗决策树的构成随机森林算法中，已经有所体现。\n",
    "\n",
    "对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（**弱分类器**，比如决策树）要比求精确的分类规则（**强分类器**，比如随机森林）更容易。于是Boosting就从弱学习算法出发，反复学习，得到一系列弱分类器，然后将这些弱分类器组合起来构成一个强分类器。\n",
    "\n",
    "大多数提升方法都是改变训练数据的**概率分布**（或者叫**权值分布**），针对不同的训练数据分布调用弱学习算法，学习一系列弱分类器。\n",
    "\n",
    "问题在于：\n",
    "\n",
    "- 如何在每一轮训练中改变训练数据的权值或者概率分布？\n",
    "- 如何将弱分类器组合成一个强分类器？\n",
    "\n",
    "对于前一个问题，我们即将介绍的 AdaBoost 算法的思路是，提高被前一轮弱分类器错误分类的样本的权值，而降低那些被证券分类的样本的权值，如此以后，那部分没有被正确分类的数据由于权值被加大，而受到后一轮的弱分类器的更大关注。\n",
    "\n",
    "对于后一个问题，AdaBoost 采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大作用；同时见笑分类误差率大的弱分类器的权值，使其在表决中起较小的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost算法\n",
    "\n",
    "AdaBoost，是英文 \"Adaptive Boosting\" （自适应增强）的缩写，是一种机器学习方法。AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器。\n",
    "\n",
    "老样子，给定一个二分类的训练数据集\n",
    "\n",
    "$$\\large T=\\left \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N) \\right \\} $$\n",
    "\n",
    "输入空间 $\\Bbb X \\subseteq \\mathbf R^n$ 为 $n$ 维向量的集合，输入为特征向量 $x \\in \\Bbb X $。\n",
    "\n",
    "输出为类标记（class label） $ y \\in \\Bbb Y = \\{-1, +1\\}$。\n",
    "\n",
    "AdaBoost算法过程如下，内容摘抄自《统计学习方法（第二版）》：\n",
    "\n",
    "1. 初始化训练数据的权值分布：\n",
    "\n",
    "$$\\large D_1 = (w_{11}, \\cdots, w_{1i},\\cdots,w_{1N}),\\quad w_{1i}=\\frac{1}{N},\\quad i=1,2,\\cdots,N$$\n",
    "\n",
    "2. 对 $ m = 1,2,\\cdots,M$ \n",
    "  \n",
    "  1. 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器\n",
    "  \n",
    "  $$\\large G_m(x)=\\Bbb X \\to \\{-1,+1\\}$$\n",
    "  \n",
    "  2. 计算 $G_m(x)$ 在训练数据集上的分类误差率\n",
    "  \n",
    "  $$\\large e_m=\\sum^N_{i=1}P(G_m(x_i)\\ne y_i)=\\sum^N_{i=1}w_{mi}I(G_m(x_i)\\ne y_i)$$\n",
    "  \n",
    "  其中，$w_{mi}$ 表示第 $m$ 轮中第 $i$ 个实例的权值，$\\large \\sum^N_{i=1}w_{mi}=1$。$I$ 是信号函数。\n",
    "  \n",
    "  3. 计算 $G_m(x)$ 的系数，这个系数在构建基本分类器线性组合的时候会用上：\n",
    "  \n",
    "  $$\\large \\alpha_m = \\frac{1}{2}\\log\\frac{1-e_m}{e_m}$$\n",
    "  \n",
    "  可以看出，误差率 $e_m$ 越大，系数 $\\alpha_m$ 越小。\n",
    "  \n",
    "  4. 更新训练集的权值分布\n",
    "  \n",
    "  $$\\large D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$$\n",
    "  \n",
    "  $$\\large w_{m+1,i} = \\frac{w_{mi}}{Z_m}\\exp(-\\alpha_my_iG_m(x_i)),\\quad i=1,2,\\cdots,N$$\n",
    "  \n",
    "  其中，$Z_m$ 是规范化因子\n",
    "  \n",
    "  $$\\large Z_m= \\sum_{i=1}^N w_{mi}\\exp(-\\alpha_my_iG_m(x_i))$$\n",
    "  \n",
    "  它使 $D_{m+1}$ 成为一个概率分布。\n",
    "  \n",
    "3. 构建基本分类器的线性组合\n",
    "\n",
    "$$\\large f(x)=\\sum_{m=1}^M\\alpha_mG_m(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
