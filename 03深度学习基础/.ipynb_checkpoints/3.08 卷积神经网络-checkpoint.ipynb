{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络\n",
    "\n",
    "作者：杨岱川\n",
    "\n",
    "时间：2020年03月\n",
    "\n",
    "github：https://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "开源协议：[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "参考文献：\n",
    "\n",
    "- 《深度学习入门》，作者：斋藤康毅；\n",
    "- 《深度学习》，作者：Ian Goodfellow 、Yoshua Bengio、Aaron Courville。\n",
    "- [CS231n](http://cs231n.stanford.edu/)\n",
    "\n",
    "## 本节目的\n",
    "\n",
    "本节的主题就是大名鼎鼎的卷积神经网络（Convolutional Neural Network，CNN）。\n",
    "\n",
    "CNN被广泛用于图像识别、语音识别等各种场合。在图像识别的比赛中，基于深度学习的方法几乎都以 CNN 为基础。\n",
    "\n",
    "我们将会在本章学习 CNN 的原理和用法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整体结构\n",
    "\n",
    "首先了解 CNN 的网络结构。\n",
    "\n",
    "CNN和之前介绍的神经网络一样，可以像积木一样通过组装各个层来构建。不过，这里出现了卷积层（Convolution层）和池化层（Pooling层），我们会在后面介绍。\n",
    "\n",
    "这里我们先介绍如何组装层，以此来构建CNN。\n",
    "\n",
    "回顾之前介绍的神经网络，相邻层的所有神经元之间都有连接，这叫做**全连接（fully-connected）**。我们用 Affine 层实现了全连接层。\n",
    "\n",
    "![CNN-全连接](https://raw.githubusercontent.com/DrDavidS/basic_Machine_Learning/master/back_up_images/CNN-结构1.png)\n",
    "\n",
    "如果使用 Affine 层构建一个 5 层的全连接神经网络，可以通过下图中的结构来实现：\n",
    "\n",
    "![全连接结构](https://raw.githubusercontent.com/DrDavidS/basic_Machine_Learning/master/back_up_images/CNN-全连接结构.png)\n",
    "\n",
    "如上图所示，全连接神经网络中，Affine 层后面跟的是激活函数 `ReLU`层（或者`Sigmoid`层）。第五层 Affine 层由 Softmax 输出最终结果。\n",
    "\n",
    "那么CNN应该是什么样的呢？下图是 CNN 的一个例子：\n",
    "\n",
    "![CNN2](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/back_up_images/CNN%E7%BB%93%E6%9E%842.png?raw=true)\n",
    "\n",
    "如图所示，CNN 中新增了 Convolution 层和 Pooling 层。一般 CNN 的层连接顺序是 “Convolution - ReLU - Pooling”，其中 Pooling 可能会被省略。\n",
    "\n",
    "还需要注意的是，在上图的 CNN 中：\n",
    "\n",
    "- 靠近输出的层中使用了之前的 “Affine - ReLU” 组合。\n",
    "\n",
    "- 最后的输出层中使用了之前的 “Affine - Softmax”组合。\n",
    "\n",
    "这些都是 CNN 中比较常见的结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "\n",
    "CNN 中会有各种奇奇怪怪的术语，比如 “填充” 或 “步幅” 等等。此外各层中传递的数据还是有形状的数据，比如 “3维数据”，这和之前的全连接神经网络有些不一样。\n",
    "\n",
    "刚刚开始也许会有些难理解，不过我们会认真学习一下 CNN 中所使用的卷积层结构。\n",
    "\n",
    "### 全连接层存在的问题\n",
    "\n",
    "在最初的全连接神经网络中我们使用的是全连接层（Affine 层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以人为任意决定。\n",
    "\n",
    "全连接层会存在什么问题呢？那就是**数据的形状被忽视了**。\n",
    "\n",
    "比如，如果输入的数据是图像，那么图像通常是**高**、**长**、**通道**方向上的 3 维形状。但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。\n",
    "\n",
    "实际上在之前的 MNIST 数据集实验中，我们的输入图像就是 1 通道、高 28 像素、长 28 像素的 $(1,28,28)$ 形状，但是被我们排成了 1 列，以 784 个数据的形式输入到最开始的 Affine 层。\n",
    " \n",
    "试想，图像是三维形状，这个形状里面有重要的空间信息，比如上下左右相邻的像素多为比较相近的值，RGB 各通道之间有紧密的联系；相反，较远的像素可能关联就不那么大。\n",
    "\n",
    "但是我们采用了全连接层，会**忽略掉形状**，而将全部输入作为相同的神经元处理，所以无法利用与形状相关的信息。\n",
    "\n",
    "> 想象一下把“图片”拉成“长条”。\n",
    " \n",
    "而卷积层则可以保持形状不变。当输入数据是图像的时候，卷积层会以 3 维数据的形式接收输入数据，并以同样的 3 维数据的形式输出至下一层。因此有更高的可能性会正确理解 “图像” 等具有形状的数据。\n",
    " \n",
    "此外，在 CNN 中，我们将卷积层的输入输出数据称为 **特征图（feature map）**。其中卷积层的输入数据叫做**输入特征图**，输出数据称为**输出特征图**。\n",
    " \n",
    "### 卷积运算\n",
    "\n",
    "卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。首先我们看一个具体的例子，如下图：\n",
    "\n",
    "### 填充\n",
    "\n",
    "### 步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
