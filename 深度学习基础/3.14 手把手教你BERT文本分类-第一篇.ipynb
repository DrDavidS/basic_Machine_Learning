{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手把手教你BERT中文文本分类-第一篇\n",
    "\n",
    "作者：杨岱川\n",
    "\n",
    "时间：2019年12月\n",
    "\n",
    "github：https://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "开源协议：[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "## 写在开头\n",
    "\n",
    "### 感言\n",
    "\n",
    "BERT 模型自2018年发布以来，它和它的衍生品几乎在NLP圈一统天下。关于BERT模型的原理我就不系统介绍了，各位自行去读论文和看博客分析。\n",
    "\n",
    "这篇Notebook的起因，是我在使用 PyTorch 和 Keras 学习和实验 BERT 系列模型（包括RoBERTa、ALBERT等）的时候，看着 CSDN、Github、知乎 等论坛上五花八门的封装代码头疼——尤其是中文任务的代码，大多数代码作者估计都是知其然而不知其所以然，用得上用不上的统统封装，又臭又长、缺失注释、结构混乱，对初学者极其不友好，看得想吐。\n",
    "\n",
    "此外，这些代码还有一些版本问题，就是基于 PyTorch 的 BERT 框架已经进化为了 [transformers](https://github.com/huggingface/transformers)，而CSDN、知乎、Github上很多项目还是基于**旧版** `pytorch-pretrained-bert` 框架，参考价值有限。\n",
    "\n",
    "新框架支持 ALBERT、RoBERTa 等新模型的调用，模型功能上也有所更新，更何况我本人在技术上爱用新不爱用旧，所以对很多基于旧版本框架的中文任务参考代码很是头疼。\n",
    "\n",
    "基于这些原因，我在进行了一段时间的实验以后，决定基于 PyTorch 和 [transformers](https://github.com/huggingface/transformers) 框架写一篇**新手友好**的 BERT 实战教程。\n",
    "\n",
    "> 注意，这篇教程可以说是 Baseline，里面缺少了很多工程实用处理，比如：\n",
    ">\n",
    ">- BERT.Config封装；\n",
    ">- 文本预处理；\n",
    ">- 半精度处理；\n",
    ">- 数据读写改进；\n",
    ">- 显存监控；\n",
    ">- TorchSnooper；\n",
    ">- 自定义损失函数；\n",
    ">- 自定义网络结构；\n",
    ">\n",
    ">计划是以后在另一篇完善，本篇任务仅仅是简洁易懂地说明 BERT 和 [transformers](https://github.com/huggingface/transformers) 框架的使用。\n",
    "\n",
    "### 基础要求\n",
    "\n",
    "- Python基础；\n",
    "- 你有一定的 PyTorch 使用经验；\n",
    "- 你对 NLP 有一些经验；\n",
    "- 你对 BERT 和 Tranformer 的理论和结构有一定的了解；\n",
    "- 你想使用 BERT 系列模型执行一些 NLP 任务，比如 NER、文本分类等。\n",
    "\n",
    "### transformers框架简介\n",
    "\n",
    "🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "\n",
    "Github地址：https://github.com/huggingface/transformers\n",
    "\n",
    "文档地址：https://huggingface.co/transformers/index.html\n",
    "\n",
    "transformers 框架横跨 TF2.0 和 PyTorch ，是一个非常好用的高级语言模型框架。\n",
    "\n",
    "### 主要软件准备\n",
    "\n",
    "- **transformers**框架：\n",
    "\n",
    "    pip安装：\n",
    "\n",
    "    ```shell\n",
    "    pip install transformers\n",
    "    ```\n",
    "    \n",
    "- **PyTorch**：见[PyTorch官网](https://pytorch.org)\n",
    "\n",
    "- **Keras**：见[Keras官网](https://keras.io)\n",
    "\n",
    "### Pytorch-BERT中文预训练模型下载\n",
    "\n",
    "包含三个文件：\n",
    "\n",
    "|name | size \n",
    "|:------|:------\n",
    "|config.json | 1KB |\n",
    "|pytorch_model.bin | 392MB |\n",
    "|bocab.txt | 107KB |\n",
    "\n",
    ">其中 `pytorch_model.bin` 就是 `bert-base-chinese`:\n",
    ">\n",
    ">12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    ">Trained on cased Chinese Simplified and Traditional text.\n",
    ">\n",
    ">建议不要用transformers自带的命令下载，由于众所周知的原因是奇慢无比，而且容易断线。\n",
    ">\n",
    ">我传到了度盘上面，如果还嫌慢可以自己找地方下载。\n",
    ">\n",
    ">度盘下载地址：https://pan.baidu.com/s/1CCylS1nkL4ut8T3nr9cUNA 提取码：3ypf\n",
    "\n",
    "### 硬件准备\n",
    "\n",
    "支持CUDA运算的机器，最好给点力，要不然训练很久。\n",
    "\n",
    "### 数据准备\n",
    "\n",
    "我采用的是 THUCNews 数据集的**子集**，由清华NLP组提供。\n",
    "\n",
    "这个数据集是针对新闻标题进行分类的数据集。可以在[这里](https://github.com/DrDavidS/Pytorch_Basic/tree/master/datasets/THUCNews)下载。文件以txt格式保存，打开以后可以看看内容。\n",
    "\n",
    "简单介绍一下数据集，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档，划分出 14 个候选分类。\n",
    "\n",
    "我们只采用了其中 10 个子类，包括\n",
    "\n",
    "```\n",
    "finance\n",
    "realty\n",
    "stocks\n",
    "education\n",
    "science\n",
    "society\n",
    "politics\n",
    "sports\n",
    "game\n",
    "entertainment\n",
    "```\n",
    "\n",
    "训练数据共 180000 条，保存在 `train.txt` 中。测试数据保存在 `dev.txt` 和 `text.txt` 中，每个文件10000条。\n",
    "\n",
    "数据格式如下：\n",
    "\n",
    "```\n",
    "《非诚勿扰》“冯女郎”车晓带妈妈闯世界(图)\t9\n",
    "美弗吉尼亚大学访华太设计签实习基地协议（组图）\t1\n",
    "华中科技大学2010年考研成绩查询开通\t3\n",
    "陈小艺“激吻照”疑似炒作\t9\n",
    "90岁老太半世纪撮合200多对新人(图)\t5\n",
    "袁立挑选钻戒被疑婚期将近 男伴酷似梁朝伟(图)\t9\n",
    "国务院：严打拐卖操控未成年人违法犯罪\t6\n",
    "郎平不惧土耳其劲旅窘境：我就喜欢接这种烂摊子\t7\n",
    "...\n",
    "```\n",
    "\n",
    "其中末尾数字代表标签类型，数字和种类对照参见 `class.txt`。数字标签和文本中间用制表符隔开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正式代码\n",
    "\n",
    "### 导入必要包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences  # padding句子用\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch 版本： {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意我用的 PyTorch 版本是 1.3.1，用新不用旧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "检查GPU状态，我用的是一块 Tesla M40。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "GPU numbers:  2\n",
      "device_name:  Tesla M40 24GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPU numbers: \", n_gpu)\n",
    "print(\"device_name: \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理\n",
    "\n",
    "#### 读取数据\n",
    "\n",
    "放在 `./datasets/THUCNews/train.txt`中，有需要请自己改路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./datasets/THUCNews/train.txt\"\n",
    "\n",
    "with open(file, encoding=\"utf-8\") as f:\n",
    "    sentences_and_labels = [line for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中华女子学院：本科层次仅1专业招男生\\t3\\n',\n",
       " '两天价网站背后重重迷雾：做个网站究竟要多少钱\\t4\\n',\n",
       " '东5环海棠公社230-290平2居准现房98折优惠\\t1\\n',\n",
       " '卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球\\t7\\n',\n",
       " '82岁老太为学生做饭扫地44年获授港大荣誉院士\\t5\\n',\n",
       " '记者回访地震中可乐男孩：将受邀赴美国参观\\t5\\n',\n",
       " '冯德伦徐若瑄隔空传情 默认其是女友\\t9\\n',\n",
       " '传郭晶晶欲落户香港战伦敦奥运 装修别墅当婚房\\t1\\n',\n",
       " '《赤壁OL》攻城战诸侯战硝烟又起\\t8\\n',\n",
       " '“手机钱包”亮相科博会\\t4\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 前几句\n",
    "sentences_and_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据以 `table` 分割，所以用 `split('\\t')`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "东5环海棠公社230-290平2居准现房98折优惠\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq, label = sentences_and_labels[2].split('\\t')\n",
    "print(seq)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sentence_with_label in sentences_and_labels:\n",
    "    sentence, label = sentence_with_label.split('\\t')\n",
    "    sentences.append(sentence)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中华女子学院：本科层次仅1专业招男生', '两天价网站背后重重迷雾：做个网站究竟要多少钱', '东5环海棠公社230-290平2居准现房98折优惠', '卡佩罗：告诉你德国脚生猛的原因 不希望英德战踢点球', '82岁老太为学生做饭扫地44年获授港大荣誉院士']\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 按字拆分：\n",
    "    \n",
    "    \n",
    "    \n",
    "按照BERT的要求，我们需要使用输入为：\n",
    "```\n",
    "[CLS]<句子A>[SEP]<句子B>[SEP]\n",
    "```\n",
    "这样的形式，但是很明显我们的新闻标题是不适合这样分开的，所以我们的输入形式是：\n",
    "```\n",
    "[CLS]<句子A>[SEP]\n",
    "```\n",
    "\n",
    "BERT不需要分词，我们只要直接将他们转换为 `vocab.txt` 字典中对应的字索引即可，包括`[CLS]`和`[SEP]`。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x7f00f3675bd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-chinese/', do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "中华女子学院：本科层次仅1专业招男生\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n"
     ]
    }
   ],
   "source": [
    "# 这句话的input_ids\n",
    "print(f\"Tokenize 前的第一句话：\\n{sentences[0]}\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n{tokenized_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上面的例子说明：\n",
    "\n",
    "首先encode包含了两个动作，\n",
    "\n",
    "**第一**，对句子\n",
    "\n",
    "    ```中华女子学院：本科层次仅1专业招男生```\n",
    "\n",
    "的前后添加标签，即：\n",
    "\n",
    "    ```[CLS]中华女子学院：本科层次仅1专业招男生[SEP]```\n",
    "\n",
    "**第二**，将添加标签后的句子按字符（标点符号也单独算一个字符）分开，然后转换为 `input_ids`：\n",
    "\n",
    "    ```[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]```\n",
    "\n",
    "> 需要说明的是，其中 `101` 是 `[CLS]` 的索引，`102` 是 `[SEP]` 的索引。\n",
    "\n",
    "上述过程称为 `tokenized`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "print (len(tokenized_texts))  # 180000句话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "为了保证输入长度的统一，我们需要对句子进行padding。\n",
    "\n",
    "本例中采用的是新闻标题，所以标题不会太长，我们限定为 `32` 个字符。\n",
    "\n",
    "一旦标题长度超过32个字符，则会截断超过部分不用；如果不足 `32` 个字符，则执行 `pad_sequences` （即`padding`）操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子最长长度\n",
    "MAX_LEN = 32\n",
    "\n",
    "# 输入padding\n",
    "# 此函数在keras里面\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize 前的第一句话：\n",
      "\n",
      "中华女子学院：本科层次仅1专业招男生\n",
      "\n",
      "\n",
      "Tokenize 后的第一句话: \n",
      "\n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n",
      "\n",
      "\n",
      "Padding 后的第一句话： \n",
      "\n",
      "[ 101  704 1290 1957 2094 2110 7368 8038 3315 4906 2231 3613  788  122\n",
      "  683  689 2875 4511 4495  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize 前的第一句话：\\n\\n{sentences[0]}\\n\\n\")\n",
    "print(f\"Tokenize 后的第一句话: \\n\\n{tokenized_texts[0]}\\n\\n\")\n",
    "print(f\"Padding 后的第一句话： \\n\\n{input_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实 `padding` 之后还可转换回来，\n",
    "\n",
    "很容易看出每个字，包括`[PAD]`、`[CLS]`、`[SEP]`所在的位置:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 中 华 女 子 学 院 ： 本 科 层 次 仅 1 专 业 招 男 生 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 转换回来\n",
    "raw_texts = [tokenizer.decode(input_ids[0])]\n",
    "print(raw_texts)\n",
    "print(len(raw_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT的输入准备\n",
    "\n",
    "#### 注意力mask（attention masks）：\n",
    "\n",
    "BERT 模型的核心是 Transformer 结构，其中很重要的一点就是 self-attention 结构。\n",
    "\n",
    "BERT-Chinese 模型同 BERT-base 模型结构一致，每层有12个自注意头，为了不让这些 self-attention 结构注意到补全的`[PAD]`部分，我们需要输入一个 attention_masks 标签，告诉模型哪些内容是真实内容，哪些是无意义的[PAD]。\n",
    "\n",
    "刚刚说到被 `padding` 部分是不需要被 attention 到的。相当于这部分在  attention_masks 中的标签就是真实句子为1，padding部分为0。所以我们得到attention masks："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一句话的 attention_masks\n",
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  准备Labels\n",
    "\n",
    "首先准备Labels。这些标题的 Labels 在一开始就已经分离开来，保存在了 `labels` 里面\n",
    " \n",
    "这里可以用 `train_test_split` 来分。注意，由于多了一个 `attention_masks` 所以我们需要用两次 `train_test_split`，并且采用相同的随机种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n', '5\\n', '9\\n', '1\\n', '8\\n', '4\\n']\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于现在的labels里面并不是数字，而且有换行符`\\n`，我们需要一些处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 1, 7, 5, 5, 9, 1, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "clean_labels = []\n",
    "for label in labels:\n",
    "    clean_labels.append(int(label.strip('\\n')))\n",
    "\n",
    "print(clean_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, clean_labels, \n",
    "                                                            random_state=2019, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2019, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 4, 3, 8, 8, 1, 8, 7, 7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      标签总数： 180000\n",
      "训练集标签总数： 162000\n",
      "验证集标签总数： 18000\n"
     ]
    }
   ],
   "source": [
    "print(f\"      标签总数：\", len(labels))\n",
    "print(f\"训练集标签总数：\", len(train_labels))\n",
    "print(f\"验证集标签总数：\", len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在准备放入 PyTorch 中，准备 Tensor 化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor化\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_inputs))\n",
    "print(len(validation_labels))\n",
    "print(len(validation_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建迭代器\n",
    "\n",
    "我们采用\n",
    "\n",
    "`torch.utils.data.TensorDataset` 将他们封装为 `TensorDataset` 的形式，\n",
    "\n",
    "`torch.utils.data.RandomSampler` 采用随机采样的方法从中采样，\n",
    "\n",
    "`torch.utils.data.DataLoader` 自动形成迭代器。\n",
    "\n",
    "> 注意 batch_size 的设置大小和显存大小密切相关！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成训练数据集\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# 随机采样\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# 读取数据\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# 形成验证数据集\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# 随机采样\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# 读取数据\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT的微调\n",
    "\n",
    "在准备好输入以后，现在我们开始微调BERT模型。\n",
    "\n",
    "使用 `BertForSequenceClassification`，它就是一个普通BERT模型，在最后面加了一个线形层用于分类。\n",
    "\n",
    "#### 导入模型\n",
    "\n",
    "直接使用 `from_pretrained` 导入预训练好的中文 BERT 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# 统计标签种类\n",
    "label_count = len(set(labels))\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取 BertForSequenceClassification 模型，\n",
    "# 是一个预训练的BERT模型，在最后面加了一个线形层用于分类。\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./bert-chinese/\", \n",
    "                                                      num_labels=label_count)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备微调\n",
    "\n",
    "待补充。\n",
    "\n",
    "其中，`no_decay`见[issue#492](https://github.com/huggingface/transformers/issues/492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# 权重衰减\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率计算函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存loss\n",
    "train_loss_set = []\n",
    "# epochs \n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练\n",
    "\n",
    "4个epoch："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前epoch： 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:26,  7.52it/s]\n",
      "1it [00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.31841557952945615\n",
      "当前epoch： 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:32,  7.48it/s]\n",
      "1it [00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.2379983004839332\n",
      "当前epoch： 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:20,  7.55it/s]\n",
      "1it [00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.22909607106667979\n",
      "当前epoch： 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:20,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前 epoch 的 Train loss: 0.19303164174012197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in range(epochs): \n",
    "    ## 训练\n",
    "    print(f\"当前epoch： {_}\")\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # 把batch放入GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # 解包batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # 梯度归零\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播loss计算\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  # 有labels的时候，且labels>1就直接返回Cross-Entropy\n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # 更新模型参数\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"当前 epoch 的 Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证状态\n",
    "model.eval()\n",
    "\n",
    "# 建立变量\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1125/1125 [00:34<00:00, 32.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9240555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 验证集的读取也要batch\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # 元组打包放进GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # 解开元组\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # 预测\n",
    "    with torch.no_grad():\n",
    "        # segment embeddings，如果没有就是全0，表示单句\n",
    "        # position embeddings，[0,句子长度-1]\n",
    "        logits = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()  # 注意这里的logits是在softmax之前，所以和不为1\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    # print(logits, label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)  # 计算准确率\n",
    "    eval_accuracy += tmp_eval_accuracy  # 准确率积累\n",
    "    nb_eval_steps += 1  # 步数积累\n",
    "print(f\"Validation Accuracy: {eval_accuracy/nb_eval_steps}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存模型\n",
    "\n",
    "待补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能小结\n",
    "\n",
    "当前我采用的卡是 Tesla M40，单卡24GB显存。\n",
    "\n",
    "总共跑了4个 epoch，batch_size 是16。\n",
    "\n",
    "从结果上来看，一个 epoch 的时间大概是22分30秒左右，也就是说跑4遍整个训练集 16w2 的数据大概要 90 分钟的样子。\n",
    "\n",
    "测试集仅仅是前向传播，总共18000条数据只花了34秒就完成了。\n",
    "\n",
    "最后准确率92.4%。\n",
    "\n",
    "实际上这个准确率有很大的提升空间，包括调参，包括清洗什么的都没有做，直接就是BERT梭哈。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续改进方向\n",
    "\n",
    "还有很多工作没做：\n",
    "\n",
    "- 模型的保存\n",
    "- Apex.fp16改写\n",
    "- Scheduler设置\n",
    "- Early-Stop设置\n",
    "- Config封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
