{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ‰‹æŠŠæ‰‹æ•™ä½ BERTä¸­æ–‡æ–‡æœ¬åˆ†ç±»-ç¬¬ä¸€ç¯‡\n",
    "\n",
    "ä½œè€…ï¼šæ¨å²±å·\n",
    "\n",
    "æ—¶é—´ï¼š2019å¹´12æœˆ\n",
    "\n",
    "githubï¼šhttps://github.com/DrDavidS/basic_Machine_Learning\n",
    "\n",
    "å¼€æºåè®®ï¼š[MIT](https://github.com/DrDavidS/basic_Machine_Learning/blob/master/LICENSE)\n",
    "\n",
    "## å†™åœ¨å¼€å¤´\n",
    "\n",
    "### æ„Ÿè¨€\n",
    "\n",
    "BERT æ¨¡å‹è‡ª2018å¹´å‘å¸ƒä»¥æ¥ï¼Œå®ƒå’Œå®ƒçš„è¡ç”Ÿå“å‡ ä¹åœ¨NLPåœˆä¸€ç»Ÿå¤©ä¸‹ã€‚å…³äºBERTæ¨¡å‹çš„åŸç†æˆ‘å°±ä¸ç³»ç»Ÿä»‹ç»äº†ï¼Œå„ä½è‡ªè¡Œå»è¯»è®ºæ–‡å’Œçœ‹åšå®¢åˆ†æã€‚\n",
    "\n",
    "è¿™ç¯‡Notebookçš„èµ·å› ï¼Œæ˜¯æˆ‘åœ¨ä½¿ç”¨ PyTorch å’Œ Keras å­¦ä¹ å’Œå®éªŒ BERT ç³»åˆ—æ¨¡å‹ï¼ˆåŒ…æ‹¬RoBERTaã€ALBERTç­‰ï¼‰çš„æ—¶å€™ï¼Œçœ‹ç€ CSDNã€Githubã€çŸ¥ä¹ ç­‰è®ºå›ä¸Šäº”èŠ±å…«é—¨çš„å°è£…ä»£ç å¤´ç–¼â€”â€”å°¤å…¶æ˜¯ä¸­æ–‡ä»»åŠ¡çš„ä»£ç ï¼Œå¤§å¤šæ•°ä»£ç ä½œè€…ä¼°è®¡éƒ½æ˜¯çŸ¥å…¶ç„¶è€Œä¸çŸ¥å…¶æ‰€ä»¥ç„¶ï¼Œç”¨å¾—ä¸Šç”¨ä¸ä¸Šçš„ç»Ÿç»Ÿå°è£…ï¼Œåˆè‡­åˆé•¿ã€ç¼ºå¤±æ³¨é‡Šã€ç»“æ„æ··ä¹±ï¼Œå¯¹åˆå­¦è€…æå…¶ä¸å‹å¥½ï¼Œçœ‹å¾—æƒ³åã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œè¿™äº›ä»£ç è¿˜æœ‰ä¸€äº›ç‰ˆæœ¬é—®é¢˜ï¼Œå°±æ˜¯åŸºäº PyTorch çš„ BERT æ¡†æ¶å·²ç»è¿›åŒ–ä¸ºäº† [transformers](https://github.com/huggingface/transformers)ï¼Œè€ŒCSDNã€çŸ¥ä¹ã€Githubä¸Šå¾ˆå¤šé¡¹ç›®è¿˜æ˜¯åŸºäº**æ—§ç‰ˆ** `pytorch-pretrained-bert` æ¡†æ¶ï¼Œå‚è€ƒä»·å€¼æœ‰é™ã€‚\n",
    "\n",
    "æ–°æ¡†æ¶æ”¯æŒ ALBERTã€RoBERTa ç­‰æ–°æ¨¡å‹çš„è°ƒç”¨ï¼Œæ¨¡å‹åŠŸèƒ½ä¸Šä¹Ÿæœ‰æ‰€æ›´æ–°ï¼Œæ›´ä½•å†µæˆ‘æœ¬äººåœ¨æŠ€æœ¯ä¸Šçˆ±ç”¨æ–°ä¸çˆ±ç”¨æ—§ï¼Œæ‰€ä»¥å¯¹å¾ˆå¤šåŸºäºæ—§ç‰ˆæœ¬æ¡†æ¶çš„ä¸­æ–‡ä»»åŠ¡å‚è€ƒä»£ç å¾ˆæ˜¯å¤´ç–¼ã€‚\n",
    "\n",
    "åŸºäºè¿™äº›åŸå› ï¼Œæˆ‘åœ¨è¿›è¡Œäº†ä¸€æ®µæ—¶é—´çš„å®éªŒä»¥åï¼Œå†³å®šåŸºäº PyTorch å’Œ [transformers](https://github.com/huggingface/transformers) æ¡†æ¶å†™ä¸€ç¯‡**æ–°æ‰‹å‹å¥½**çš„ BERT å®æˆ˜æ•™ç¨‹ã€‚\n",
    "\n",
    "> æ³¨æ„ï¼Œè¿™ç¯‡æ•™ç¨‹å¯ä»¥è¯´æ˜¯ Baselineï¼Œé‡Œé¢ç¼ºå°‘äº†å¾ˆå¤šå·¥ç¨‹å®ç”¨å¤„ç†ï¼Œæ¯”å¦‚ï¼š\n",
    ">\n",
    ">- BERT.Configå°è£…ï¼›\n",
    ">- æ–‡æœ¬é¢„å¤„ç†ï¼›\n",
    ">- åŠç²¾åº¦å¤„ç†ï¼›\n",
    ">- æ•°æ®è¯»å†™æ”¹è¿›ï¼›\n",
    ">- æ˜¾å­˜ç›‘æ§ï¼›\n",
    ">- TorchSnooperï¼›\n",
    ">- è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼›\n",
    ">- è‡ªå®šä¹‰ç½‘ç»œç»“æ„ï¼›\n",
    ">\n",
    ">è®¡åˆ’æ˜¯ä»¥ååœ¨å¦ä¸€ç¯‡å®Œå–„ï¼Œæœ¬ç¯‡ä»»åŠ¡ä»…ä»…æ˜¯ç®€æ´æ˜“æ‡‚åœ°è¯´æ˜ BERT å’Œ [transformers](https://github.com/huggingface/transformers) æ¡†æ¶çš„ä½¿ç”¨ã€‚\n",
    "\n",
    "### åŸºç¡€è¦æ±‚\n",
    "\n",
    "- PythonåŸºç¡€ï¼›\n",
    "- ä½ æœ‰ä¸€å®šçš„ PyTorch ä½¿ç”¨ç»éªŒï¼›\n",
    "- ä½ å¯¹ NLP æœ‰ä¸€äº›ç»éªŒï¼›\n",
    "- ä½ å¯¹ BERT å’Œ Tranformer çš„ç†è®ºå’Œç»“æ„æœ‰ä¸€å®šçš„äº†è§£ï¼›\n",
    "- ä½ æƒ³ä½¿ç”¨ BERT ç³»åˆ—æ¨¡å‹æ‰§è¡Œä¸€äº› NLP ä»»åŠ¡ï¼Œæ¯”å¦‚ NERã€æ–‡æœ¬åˆ†ç±»ç­‰ã€‚\n",
    "\n",
    "### transformersæ¡†æ¶ç®€ä»‹\n",
    "\n",
    "ğŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.\n",
    "\n",
    "Githubåœ°å€ï¼šhttps://github.com/huggingface/transformers\n",
    "\n",
    "æ–‡æ¡£åœ°å€ï¼šhttps://huggingface.co/transformers/index.html\n",
    "\n",
    "transformers æ¡†æ¶æ¨ªè·¨ TF2.0 å’Œ PyTorch ï¼Œæ˜¯ä¸€ä¸ªéå¸¸å¥½ç”¨çš„é«˜çº§è¯­è¨€æ¨¡å‹æ¡†æ¶ã€‚\n",
    "\n",
    "### ä¸»è¦è½¯ä»¶å‡†å¤‡\n",
    "\n",
    "- **transformers**æ¡†æ¶ï¼š\n",
    "\n",
    "    pipå®‰è£…ï¼š\n",
    "\n",
    "    ```shell\n",
    "    pip install transformers\n",
    "    ```\n",
    "    \n",
    "- **PyTorch**ï¼šè§[PyTorchå®˜ç½‘](https://pytorch.org)\n",
    "\n",
    "- **Keras**ï¼šè§[Keraså®˜ç½‘](https://keras.io)\n",
    "\n",
    "### Pytorch-BERTä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½\n",
    "\n",
    "åŒ…å«ä¸‰ä¸ªæ–‡ä»¶ï¼š\n",
    "\n",
    "|name | size \n",
    "|:------|:------\n",
    "|config.json | 1KB |\n",
    "|pytorch_model.bin | 392MB |\n",
    "|bocab.txt | 107KB |\n",
    "\n",
    ">å…¶ä¸­ `pytorch_model.bin` å°±æ˜¯ `bert-base-chinese`:\n",
    ">\n",
    ">12-layer, 768-hidden, 12-heads, 110M parameters.\n",
    ">Trained on cased Chinese Simplified and Traditional text.\n",
    ">\n",
    ">å»ºè®®ä¸è¦ç”¨transformersè‡ªå¸¦çš„å‘½ä»¤ä¸‹è½½ï¼Œç”±äºä¼—æ‰€å‘¨çŸ¥çš„åŸå› æ˜¯å¥‡æ…¢æ— æ¯”ï¼Œè€Œä¸”å®¹æ˜“æ–­çº¿ã€‚\n",
    ">\n",
    ">æˆ‘ä¼ åˆ°äº†åº¦ç›˜ä¸Šé¢ï¼Œå¦‚æœè¿˜å«Œæ…¢å¯ä»¥è‡ªå·±æ‰¾åœ°æ–¹ä¸‹è½½ã€‚\n",
    ">\n",
    ">åº¦ç›˜ä¸‹è½½åœ°å€ï¼šhttps://pan.baidu.com/s/1CCylS1nkL4ut8T3nr9cUNA æå–ç ï¼š3ypf\n",
    "\n",
    "### ç¡¬ä»¶å‡†å¤‡\n",
    "\n",
    "æ”¯æŒCUDAè¿ç®—çš„æœºå™¨ï¼Œæœ€å¥½ç»™ç‚¹åŠ›ï¼Œè¦ä¸ç„¶è®­ç»ƒå¾ˆä¹…ã€‚\n",
    "\n",
    "### æ•°æ®å‡†å¤‡\n",
    "\n",
    "æˆ‘é‡‡ç”¨çš„æ˜¯ THUCNews æ•°æ®é›†çš„**å­é›†**ï¼Œç”±æ¸…åNLPç»„æä¾›ã€‚\n",
    "\n",
    "è¿™ä¸ªæ•°æ®é›†æ˜¯é’ˆå¯¹æ–°é—»æ ‡é¢˜è¿›è¡Œåˆ†ç±»çš„æ•°æ®é›†ã€‚å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/DrDavidS/Pytorch_Basic/tree/master/datasets/THUCNews)ä¸‹è½½ã€‚æ–‡ä»¶ä»¥txtæ ¼å¼ä¿å­˜ï¼Œæ‰“å¼€ä»¥åå¯ä»¥çœ‹çœ‹å†…å®¹ã€‚\n",
    "\n",
    "ç®€å•ä»‹ç»ä¸€ä¸‹æ•°æ®é›†ï¼ŒTHUCNewsæ˜¯æ ¹æ®æ–°æµªæ–°é—»RSSè®¢é˜…é¢‘é“2005~2011å¹´é—´çš„å†å²æ•°æ®ç­›é€‰è¿‡æ»¤ç”Ÿæˆï¼ŒåŒ…å«74ä¸‡ç¯‡æ–°é—»æ–‡æ¡£ï¼Œåˆ’åˆ†å‡º 14 ä¸ªå€™é€‰åˆ†ç±»ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªé‡‡ç”¨äº†å…¶ä¸­ 10 ä¸ªå­ç±»ï¼ŒåŒ…æ‹¬\n",
    "\n",
    "```\n",
    "finance\n",
    "realty\n",
    "stocks\n",
    "education\n",
    "science\n",
    "society\n",
    "politics\n",
    "sports\n",
    "game\n",
    "entertainment\n",
    "```\n",
    "\n",
    "è®­ç»ƒæ•°æ®å…± 180000 æ¡ï¼Œä¿å­˜åœ¨ `train.txt` ä¸­ã€‚æµ‹è¯•æ•°æ®ä¿å­˜åœ¨ `dev.txt` å’Œ `text.txt` ä¸­ï¼Œæ¯ä¸ªæ–‡ä»¶10000æ¡ã€‚\n",
    "\n",
    "æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "```\n",
    "ã€Šéè¯šå‹¿æ‰°ã€‹â€œå†¯å¥³éƒâ€è½¦æ™“å¸¦å¦ˆå¦ˆé—¯ä¸–ç•Œ(å›¾)\t9\n",
    "ç¾å¼—å‰å°¼äºšå¤§å­¦è®¿åå¤ªè®¾è®¡ç­¾å®ä¹ åŸºåœ°åè®®ï¼ˆç»„å›¾ï¼‰\t1\n",
    "åä¸­ç§‘æŠ€å¤§å­¦2010å¹´è€ƒç ”æˆç»©æŸ¥è¯¢å¼€é€š\t3\n",
    "é™ˆå°è‰ºâ€œæ¿€å»ç…§â€ç–‘ä¼¼ç‚’ä½œ\t9\n",
    "90å²è€å¤ªåŠä¸–çºªæ’®åˆ200å¤šå¯¹æ–°äºº(å›¾)\t5\n",
    "è¢ç«‹æŒ‘é€‰é’»æˆ’è¢«ç–‘å©šæœŸå°†è¿‘ ç”·ä¼´é…·ä¼¼æ¢æœä¼Ÿ(å›¾)\t9\n",
    "å›½åŠ¡é™¢ï¼šä¸¥æ‰“æ‹å–æ“æ§æœªæˆå¹´äººè¿æ³•çŠ¯ç½ª\t6\n",
    "éƒå¹³ä¸æƒ§åœŸè€³å…¶åŠ²æ—…çª˜å¢ƒï¼šæˆ‘å°±å–œæ¬¢æ¥è¿™ç§çƒ‚æ‘Šå­\t7\n",
    "...\n",
    "```\n",
    "\n",
    "å…¶ä¸­æœ«å°¾æ•°å­—ä»£è¡¨æ ‡ç­¾ç±»å‹ï¼Œæ•°å­—å’Œç§ç±»å¯¹ç…§å‚è§ `class.txt`ã€‚æ•°å­—æ ‡ç­¾å’Œæ–‡æœ¬ä¸­é—´ç”¨åˆ¶è¡¨ç¬¦éš”å¼€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­£å¼ä»£ç \n",
    "\n",
    "### å¯¼å…¥å¿…è¦åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT imports\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences  # paddingå¥å­ç”¨\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch ç‰ˆæœ¬ï¼š {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„æˆ‘ç”¨çš„ PyTorch ç‰ˆæœ¬æ˜¯ 1.3.1ï¼Œç”¨æ–°ä¸ç”¨æ—§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "æ£€æŸ¥GPUçŠ¶æ€ï¼Œæˆ‘ç”¨çš„æ˜¯ä¸€å— Tesla M40ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "GPU numbers:  2\n",
      "device_name:  Tesla M40 24GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"GPU numbers: \", n_gpu)\n",
    "print(\"device_name: \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®å¤„ç†\n",
    "\n",
    "#### è¯»å–æ•°æ®\n",
    "\n",
    "æ”¾åœ¨ `./datasets/THUCNews/train.txt`ä¸­ï¼Œæœ‰éœ€è¦è¯·è‡ªå·±æ”¹è·¯å¾„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./datasets/THUCNews/train.txt\"\n",
    "\n",
    "with open(file, encoding=\"utf-8\") as f:\n",
    "    sentences_and_labels = [line for line in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ\\t3\\n',\n",
       " 'ä¸¤å¤©ä»·ç½‘ç«™èƒŒåé‡é‡è¿·é›¾ï¼šåšä¸ªç½‘ç«™ç©¶ç«Ÿè¦å¤šå°‘é’±\\t4\\n',\n",
       " 'ä¸œ5ç¯æµ·æ£ å…¬ç¤¾230-290å¹³2å±…å‡†ç°æˆ¿98æŠ˜ä¼˜æƒ \\t1\\n',\n",
       " 'å¡ä½©ç½—ï¼šå‘Šè¯‰ä½ å¾·å›½è„šç”ŸçŒ›çš„åŸå›  ä¸å¸Œæœ›è‹±å¾·æˆ˜è¸¢ç‚¹çƒ\\t7\\n',\n",
       " '82å²è€å¤ªä¸ºå­¦ç”Ÿåšé¥­æ‰«åœ°44å¹´è·æˆæ¸¯å¤§è£èª‰é™¢å£«\\t5\\n',\n",
       " 'è®°è€…å›è®¿åœ°éœ‡ä¸­å¯ä¹ç”·å­©ï¼šå°†å—é‚€èµ´ç¾å›½å‚è§‚\\t5\\n',\n",
       " 'å†¯å¾·ä¼¦å¾è‹¥ç‘„éš”ç©ºä¼ æƒ… é»˜è®¤å…¶æ˜¯å¥³å‹\\t9\\n',\n",
       " 'ä¼ éƒ­æ™¶æ™¶æ¬²è½æˆ·é¦™æ¸¯æˆ˜ä¼¦æ•¦å¥¥è¿ è£…ä¿®åˆ«å¢…å½“å©šæˆ¿\\t1\\n',\n",
       " 'ã€Šèµ¤å£OLã€‹æ”»åŸæˆ˜è¯¸ä¾¯æˆ˜ç¡çƒŸåˆèµ·\\t8\\n',\n",
       " 'â€œæ‰‹æœºé’±åŒ…â€äº®ç›¸ç§‘åšä¼š\\t4\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å‰å‡ å¥\n",
    "sentences_and_labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°æ®ä»¥ `table` åˆ†å‰²ï¼Œæ‰€ä»¥ç”¨ `split('\\t')`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸œ5ç¯æµ·æ£ å…¬ç¤¾230-290å¹³2å±…å‡†ç°æˆ¿98æŠ˜ä¼˜æƒ \n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq, label = sentences_and_labels[2].split('\\t')\n",
    "print(seq)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sentence_with_label in sentences_and_labels:\n",
    "    sentence, label = sentence_with_label.split('\\t')\n",
    "    sentences.append(sentence)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ', 'ä¸¤å¤©ä»·ç½‘ç«™èƒŒåé‡é‡è¿·é›¾ï¼šåšä¸ªç½‘ç«™ç©¶ç«Ÿè¦å¤šå°‘é’±', 'ä¸œ5ç¯æµ·æ£ å…¬ç¤¾230-290å¹³2å±…å‡†ç°æˆ¿98æŠ˜ä¼˜æƒ ', 'å¡ä½©ç½—ï¼šå‘Šè¯‰ä½ å¾·å›½è„šç”ŸçŒ›çš„åŸå›  ä¸å¸Œæœ›è‹±å¾·æˆ˜è¸¢ç‚¹çƒ', '82å²è€å¤ªä¸ºå­¦ç”Ÿåšé¥­æ‰«åœ°44å¹´è·æˆæ¸¯å¤§è£èª‰é™¢å£«']\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0:5])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æŒ‰å­—æ‹†åˆ†ï¼š\n",
    "    \n",
    "    \n",
    "    \n",
    "æŒ‰ç…§BERTçš„è¦æ±‚ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è¾“å…¥ä¸ºï¼š\n",
    "```\n",
    "[CLS]<å¥å­A>[SEP]<å¥å­B>[SEP]\n",
    "```\n",
    "è¿™æ ·çš„å½¢å¼ï¼Œä½†æ˜¯å¾ˆæ˜æ˜¾æˆ‘ä»¬çš„æ–°é—»æ ‡é¢˜æ˜¯ä¸é€‚åˆè¿™æ ·åˆ†å¼€çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è¾“å…¥å½¢å¼æ˜¯ï¼š\n",
    "```\n",
    "[CLS]<å¥å­A>[SEP]\n",
    "```\n",
    "\n",
    "BERTä¸éœ€è¦åˆ†è¯ï¼Œæˆ‘ä»¬åªè¦ç›´æ¥å°†ä»–ä»¬è½¬æ¢ä¸º `vocab.txt` å­—å…¸ä¸­å¯¹åº”çš„å­—ç´¢å¼•å³å¯ï¼ŒåŒ…æ‹¬`[CLS]`å’Œ`[SEP]`ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_bert.BertTokenizer at 0x7f00f3675bd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./bert-chinese/', do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\n",
      "ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ\n",
      "\n",
      "Tokenize åçš„ç¬¬ä¸€å¥è¯: \n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n"
     ]
    }
   ],
   "source": [
    "# è¿™å¥è¯çš„input_ids\n",
    "print(f\"Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\\n{sentences[0]}\\n\")\n",
    "print(f\"Tokenize åçš„ç¬¬ä¸€å¥è¯: \\n{tokenized_texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šé¢çš„ä¾‹å­è¯´æ˜ï¼š\n",
    "\n",
    "é¦–å…ˆencodeåŒ…å«äº†ä¸¤ä¸ªåŠ¨ä½œï¼Œ\n",
    "\n",
    "**ç¬¬ä¸€**ï¼Œå¯¹å¥å­\n",
    "\n",
    "    ```ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ```\n",
    "\n",
    "çš„å‰åæ·»åŠ æ ‡ç­¾ï¼Œå³ï¼š\n",
    "\n",
    "    ```[CLS]ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ[SEP]```\n",
    "\n",
    "**ç¬¬äºŒ**ï¼Œå°†æ·»åŠ æ ‡ç­¾åçš„å¥å­æŒ‰å­—ç¬¦ï¼ˆæ ‡ç‚¹ç¬¦å·ä¹Ÿå•ç‹¬ç®—ä¸€ä¸ªå­—ç¬¦ï¼‰åˆ†å¼€ï¼Œç„¶åè½¬æ¢ä¸º `input_ids`ï¼š\n",
    "\n",
    "    ```[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]```\n",
    "\n",
    "> éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå…¶ä¸­ `101` æ˜¯ `[CLS]` çš„ç´¢å¼•ï¼Œ`102` æ˜¯ `[SEP]` çš„ç´¢å¼•ã€‚\n",
    "\n",
    "ä¸Šè¿°è¿‡ç¨‹ç§°ä¸º `tokenized`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n"
     ]
    }
   ],
   "source": [
    "print (len(tokenized_texts))  # 180000å¥è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding\n",
    "\n",
    "ä¸ºäº†ä¿è¯è¾“å…¥é•¿åº¦çš„ç»Ÿä¸€ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å¥å­è¿›è¡Œpaddingã€‚\n",
    "\n",
    "æœ¬ä¾‹ä¸­é‡‡ç”¨çš„æ˜¯æ–°é—»æ ‡é¢˜ï¼Œæ‰€ä»¥æ ‡é¢˜ä¸ä¼šå¤ªé•¿ï¼Œæˆ‘ä»¬é™å®šä¸º `32` ä¸ªå­—ç¬¦ã€‚\n",
    "\n",
    "ä¸€æ—¦æ ‡é¢˜é•¿åº¦è¶…è¿‡32ä¸ªå­—ç¬¦ï¼Œåˆ™ä¼šæˆªæ–­è¶…è¿‡éƒ¨åˆ†ä¸ç”¨ï¼›å¦‚æœä¸è¶³ `32` ä¸ªå­—ç¬¦ï¼Œåˆ™æ‰§è¡Œ `pad_sequences` ï¼ˆå³`padding`ï¼‰æ“ä½œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¥å­æœ€é•¿é•¿åº¦\n",
    "MAX_LEN = 32\n",
    "\n",
    "# è¾“å…¥padding\n",
    "# æ­¤å‡½æ•°åœ¨kerasé‡Œé¢\n",
    "input_ids = pad_sequences([txt for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", \n",
    "                          truncating=\"post\", \n",
    "                          padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\n",
      "\n",
      "ä¸­åå¥³å­å­¦é™¢ï¼šæœ¬ç§‘å±‚æ¬¡ä»…1ä¸“ä¸šæ‹›ç”·ç”Ÿ\n",
      "\n",
      "\n",
      "Tokenize åçš„ç¬¬ä¸€å¥è¯: \n",
      "\n",
      "[101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 102]\n",
      "\n",
      "\n",
      "Padding åçš„ç¬¬ä¸€å¥è¯ï¼š \n",
      "\n",
      "[ 101  704 1290 1957 2094 2110 7368 8038 3315 4906 2231 3613  788  122\n",
      "  683  689 2875 4511 4495  102    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenize å‰çš„ç¬¬ä¸€å¥è¯ï¼š\\n\\n{sentences[0]}\\n\\n\")\n",
    "print(f\"Tokenize åçš„ç¬¬ä¸€å¥è¯: \\n\\n{tokenized_texts[0]}\\n\\n\")\n",
    "print(f\"Padding åçš„ç¬¬ä¸€å¥è¯ï¼š \\n\\n{input_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶å® `padding` ä¹‹åè¿˜å¯è½¬æ¢å›æ¥ï¼Œ\n",
    "\n",
    "å¾ˆå®¹æ˜“çœ‹å‡ºæ¯ä¸ªå­—ï¼ŒåŒ…æ‹¬`[PAD]`ã€`[CLS]`ã€`[SEP]`æ‰€åœ¨çš„ä½ç½®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] ä¸­ å å¥³ å­ å­¦ é™¢ ï¼š æœ¬ ç§‘ å±‚ æ¬¡ ä»… 1 ä¸“ ä¸š æ‹› ç”· ç”Ÿ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# è½¬æ¢å›æ¥\n",
    "raw_texts = [tokenizer.decode(input_ids[0])]\n",
    "print(raw_texts)\n",
    "print(len(raw_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTçš„è¾“å…¥å‡†å¤‡\n",
    "\n",
    "#### æ³¨æ„åŠ›maskï¼ˆattention masksï¼‰ï¼š\n",
    "\n",
    "BERT æ¨¡å‹çš„æ ¸å¿ƒæ˜¯ Transformer ç»“æ„ï¼Œå…¶ä¸­å¾ˆé‡è¦çš„ä¸€ç‚¹å°±æ˜¯ self-attention ç»“æ„ã€‚\n",
    "\n",
    "BERT-Chinese æ¨¡å‹åŒ BERT-base æ¨¡å‹ç»“æ„ä¸€è‡´ï¼Œæ¯å±‚æœ‰12ä¸ªè‡ªæ³¨æ„å¤´ï¼Œä¸ºäº†ä¸è®©è¿™äº› self-attention ç»“æ„æ³¨æ„åˆ°è¡¥å…¨çš„`[PAD]`éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦è¾“å…¥ä¸€ä¸ª attention_masks æ ‡ç­¾ï¼Œå‘Šè¯‰æ¨¡å‹å“ªäº›å†…å®¹æ˜¯çœŸå®å†…å®¹ï¼Œå“ªäº›æ˜¯æ— æ„ä¹‰çš„[PAD]ã€‚\n",
    "\n",
    "åˆšåˆšè¯´åˆ°è¢« `padding` éƒ¨åˆ†æ˜¯ä¸éœ€è¦è¢« attention åˆ°çš„ã€‚ç›¸å½“äºè¿™éƒ¨åˆ†åœ¨  attention_masks ä¸­çš„æ ‡ç­¾å°±æ˜¯çœŸå®å¥å­ä¸º1ï¼Œpaddingéƒ¨åˆ†ä¸º0ã€‚æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°attention masksï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºattention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i > 0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç¬¬ä¸€å¥è¯çš„ attention_masks\n",
    "attention_masks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  å‡†å¤‡Labels\n",
    "\n",
    "é¦–å…ˆå‡†å¤‡Labelsã€‚è¿™äº›æ ‡é¢˜çš„ Labels åœ¨ä¸€å¼€å§‹å°±å·²ç»åˆ†ç¦»å¼€æ¥ï¼Œä¿å­˜åœ¨äº† `labels` é‡Œé¢\n",
    " \n",
    "è¿™é‡Œå¯ä»¥ç”¨ `train_test_split` æ¥åˆ†ã€‚æ³¨æ„ï¼Œç”±äºå¤šäº†ä¸€ä¸ª `attention_masks` æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨ä¸¤æ¬¡ `train_test_split`ï¼Œå¹¶ä¸”é‡‡ç”¨ç›¸åŒçš„éšæœºç§å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000\n",
      "['3\\n', '4\\n', '1\\n', '7\\n', '5\\n', '5\\n', '9\\n', '1\\n', '8\\n', '4\\n']\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºç°åœ¨çš„labelsé‡Œé¢å¹¶ä¸æ˜¯æ•°å­—ï¼Œè€Œä¸”æœ‰æ¢è¡Œç¬¦`\\n`ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 1, 7, 5, 5, 9, 1, 8, 4]\n"
     ]
    }
   ],
   "source": [
    "clean_labels = []\n",
    "for label in labels:\n",
    "    clean_labels.append(int(label.strip('\\n')))\n",
    "\n",
    "print(clean_labels[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, clean_labels, \n",
    "                                                            random_state=2019, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2019, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 4, 3, 8, 8, 1, 8, 7, 7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      æ ‡ç­¾æ€»æ•°ï¼š 180000\n",
      "è®­ç»ƒé›†æ ‡ç­¾æ€»æ•°ï¼š 162000\n",
      "éªŒè¯é›†æ ‡ç­¾æ€»æ•°ï¼š 18000\n"
     ]
    }
   ],
   "source": [
    "print(f\"      æ ‡ç­¾æ€»æ•°ï¼š\", len(labels))\n",
    "print(f\"è®­ç»ƒé›†æ ‡ç­¾æ€»æ•°ï¼š\", len(train_labels))\n",
    "print(f\"éªŒè¯é›†æ ‡ç­¾æ€»æ•°ï¼š\", len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨å‡†å¤‡æ”¾å…¥ PyTorch ä¸­ï¼Œå‡†å¤‡ Tensor åŒ–ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensoråŒ–\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_inputs))\n",
    "print(len(validation_labels))\n",
    "print(len(validation_masks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åˆ›å»ºè¿­ä»£å™¨\n",
    "\n",
    "æˆ‘ä»¬é‡‡ç”¨\n",
    "\n",
    "`torch.utils.data.TensorDataset` å°†ä»–ä»¬å°è£…ä¸º `TensorDataset` çš„å½¢å¼ï¼Œ\n",
    "\n",
    "`torch.utils.data.RandomSampler` é‡‡ç”¨éšæœºé‡‡æ ·çš„æ–¹æ³•ä»ä¸­é‡‡æ ·ï¼Œ\n",
    "\n",
    "`torch.utils.data.DataLoader` è‡ªåŠ¨å½¢æˆè¿­ä»£å™¨ã€‚\n",
    "\n",
    "> æ³¨æ„ batch_size çš„è®¾ç½®å¤§å°å’Œæ˜¾å­˜å¤§å°å¯†åˆ‡ç›¸å…³ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å½¢æˆè®­ç»ƒæ•°æ®é›†\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)  \n",
    "# éšæœºé‡‡æ ·\n",
    "train_sampler = RandomSampler(train_data) \n",
    "# è¯»å–æ•°æ®\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# å½¢æˆéªŒè¯æ•°æ®é›†\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# éšæœºé‡‡æ ·\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# è¯»å–æ•°æ®\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTçš„å¾®è°ƒ\n",
    "\n",
    "åœ¨å‡†å¤‡å¥½è¾“å…¥ä»¥åï¼Œç°åœ¨æˆ‘ä»¬å¼€å§‹å¾®è°ƒBERTæ¨¡å‹ã€‚\n",
    "\n",
    "ä½¿ç”¨ `BertForSequenceClassification`ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªæ™®é€šBERTæ¨¡å‹ï¼Œåœ¨æœ€åé¢åŠ äº†ä¸€ä¸ªçº¿å½¢å±‚ç”¨äºåˆ†ç±»ã€‚\n",
    "\n",
    "#### å¯¼å…¥æ¨¡å‹\n",
    "\n",
    "ç›´æ¥ä½¿ç”¨ `from_pretrained` å¯¼å…¥é¢„è®­ç»ƒå¥½çš„ä¸­æ–‡ BERT æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# ç»Ÿè®¡æ ‡ç­¾ç§ç±»\n",
    "label_count = len(set(labels))\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯»å– BertForSequenceClassification æ¨¡å‹ï¼Œ\n",
    "# æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„BERTæ¨¡å‹ï¼Œåœ¨æœ€åé¢åŠ äº†ä¸€ä¸ªçº¿å½¢å±‚ç”¨äºåˆ†ç±»ã€‚\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./bert-chinese/\", \n",
    "                                                      num_labels=label_count)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å‡†å¤‡å¾®è°ƒ\n",
    "\n",
    "å¾…è¡¥å……ã€‚\n",
    "\n",
    "å…¶ä¸­ï¼Œ`no_decay`è§[issue#492](https://github.com/huggingface/transformers/issues/492)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT fine-tuning parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# æƒé‡è¡°å‡\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "     'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¼˜åŒ–å™¨\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†ç¡®ç‡è®¡ç®—å‡½æ•°\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜loss\n",
    "train_loss_set = []\n",
    "# epochs \n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "4ä¸ªepochï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰epochï¼š 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:26,  7.52it/s]\n",
      "1it [00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰ epoch çš„ Train loss: 0.31841557952945615\n",
      "å½“å‰epochï¼š 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:32,  7.48it/s]\n",
      "1it [00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰ epoch çš„ Train loss: 0.2379983004839332\n",
      "å½“å‰epochï¼š 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:20,  7.55it/s]\n",
      "1it [00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰ epoch çš„ Train loss: 0.22909607106667979\n",
      "å½“å‰epochï¼š 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10125it [22:20,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“å‰ epoch çš„ Train loss: 0.19303164174012197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT training loop\n",
    "for _ in range(epochs): \n",
    "    ## è®­ç»ƒ\n",
    "    print(f\"å½“å‰epochï¼š {_}\")\n",
    "    # å¼€å¯è®­ç»ƒæ¨¡å¼\n",
    "    model.train()\n",
    "    tr_loss = 0  # train loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # æŠŠbatchæ”¾å…¥GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # è§£åŒ…batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # æ¢¯åº¦å½’é›¶\n",
    "        optimizer.zero_grad()\n",
    "        # å‰å‘ä¼ æ’­lossè®¡ç®—\n",
    "        output = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels)  # æœ‰labelsçš„æ—¶å€™ï¼Œä¸”labels>1å°±ç›´æ¥è¿”å›Cross-Entropy\n",
    "        loss = output[0]\n",
    "        # print(loss)\n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "    print(f\"å½“å‰ epoch çš„ Train loss: {tr_loss/nb_tr_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### éªŒè¯æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯çŠ¶æ€\n",
    "model.eval()\n",
    "\n",
    "# å»ºç«‹å˜é‡\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "# Evaluate data for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1125/1125 [00:34<00:00, 32.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9240555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# éªŒè¯é›†çš„è¯»å–ä¹Ÿè¦batch\n",
    "for batch in tqdm(validation_dataloader):\n",
    "    # å…ƒç»„æ‰“åŒ…æ”¾è¿›GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # è§£å¼€å…ƒç»„\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # é¢„æµ‹\n",
    "    with torch.no_grad():\n",
    "        # segment embeddingsï¼Œå¦‚æœæ²¡æœ‰å°±æ˜¯å…¨0ï¼Œè¡¨ç¤ºå•å¥\n",
    "        # position embeddingsï¼Œ[0,å¥å­é•¿åº¦-1]\n",
    "        logits = model(input_ids=b_input_ids, \n",
    "                       attention_mask=b_input_mask,\n",
    "                       token_type_ids=None,\n",
    "                       position_ids=None)  \n",
    "                       \n",
    "    # print(logits[0])\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].detach().cpu().numpy()  # æ³¨æ„è¿™é‡Œçš„logitsæ˜¯åœ¨softmaxä¹‹å‰ï¼Œæ‰€ä»¥å’Œä¸ä¸º1\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    # print(logits, label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)  # è®¡ç®—å‡†ç¡®ç‡\n",
    "    eval_accuracy += tmp_eval_accuracy  # å‡†ç¡®ç‡ç§¯ç´¯\n",
    "    nb_eval_steps += 1  # æ­¥æ•°ç§¯ç´¯\n",
    "print(f\"Validation Accuracy: {eval_accuracy/nb_eval_steps}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä¿å­˜æ¨¡å‹\n",
    "\n",
    "å¾…è¡¥å……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ€§èƒ½å°ç»“\n",
    "\n",
    "å½“å‰æˆ‘é‡‡ç”¨çš„å¡æ˜¯ Tesla M40ï¼Œå•å¡24GBæ˜¾å­˜ã€‚\n",
    "\n",
    "æ€»å…±è·‘äº†4ä¸ª epochï¼Œbatch_size æ˜¯16ã€‚\n",
    "\n",
    "ä»ç»“æœä¸Šæ¥çœ‹ï¼Œä¸€ä¸ª epoch çš„æ—¶é—´å¤§æ¦‚æ˜¯22åˆ†30ç§’å·¦å³ï¼Œä¹Ÿå°±æ˜¯è¯´è·‘4éæ•´ä¸ªè®­ç»ƒé›† 16w2 çš„æ•°æ®å¤§æ¦‚è¦ 90 åˆ†é’Ÿçš„æ ·å­ã€‚\n",
    "\n",
    "æµ‹è¯•é›†ä»…ä»…æ˜¯å‰å‘ä¼ æ’­ï¼Œæ€»å…±18000æ¡æ•°æ®åªèŠ±äº†34ç§’å°±å®Œæˆäº†ã€‚\n",
    "\n",
    "æœ€åå‡†ç¡®ç‡92.4%ã€‚\n",
    "\n",
    "å®é™…ä¸Šè¿™ä¸ªå‡†ç¡®ç‡æœ‰å¾ˆå¤§çš„æå‡ç©ºé—´ï¼ŒåŒ…æ‹¬è°ƒå‚ï¼ŒåŒ…æ‹¬æ¸…æ´—ä»€ä¹ˆçš„éƒ½æ²¡æœ‰åšï¼Œç›´æ¥å°±æ˜¯BERTæ¢­å“ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åç»­æ”¹è¿›æ–¹å‘\n",
    "\n",
    "è¿˜æœ‰å¾ˆå¤šå·¥ä½œæ²¡åšï¼š\n",
    "\n",
    "- æ¨¡å‹çš„ä¿å­˜\n",
    "- Apex.fp16æ”¹å†™\n",
    "- Schedulerè®¾ç½®\n",
    "- Early-Stopè®¾ç½®\n",
    "- Configå°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
